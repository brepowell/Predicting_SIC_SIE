{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.1\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a967ec-f4e5-4abf-a46b-f407687d9518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zarr version 2.18.3\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "print('Zarr version', zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9c1711-4d37-49fe-aa18-76a04aa25569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perlmutterpath import *  # Contains the data_dir and mesh_dir variables\n",
    "NUM_FEATURES = 2              # C: Number of features per cell (ex., Freeboard, Ice Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb8f648-272a-4897-bd50-fc2e9aa308c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "\n",
    "# Available patchify functions\n",
    "PATCHIFY_FUNCTIONS = {\n",
    "    \"agglomerative\": compute_agglomerative_patches,\n",
    "    \"breadth_first_bfs_basic\": build_patches_from_seeds_bfs_basic,\n",
    "    \"breadth_first_improved_padded\": build_patches_from_seeds_improved_padded,\n",
    "    \"dbscan\": get_clusters_dbscan,\n",
    "    \"kmeans\": cluster_patches_kmeans,\n",
    "    \"knn_basic\": compute_knn_patches,\n",
    "    \"knn_disjoint\": compute_disjoint_knn_patches,\n",
    "    \"latlon_spillover\": patchify_by_latlon_spillover,              # RELIABLE PERFORMANCE\n",
    "    \"latitude_neighbors\": patchify_by_latitude,\n",
    "    \"latitude_simple\": patchify_by_latitude_simple,\n",
    "    \"latitude_spillover_redo\": patchify_with_spillover,            # checking\n",
    "    \"lon_spilldown\": patchify_by_lon_spilldown,                    # TERRIBLE PERFORMANCE\n",
    "    \"rows\": get_rows_of_patches,                                   # RELIABLE PERFORMANCE\n",
    "    \"staggered_polar_descent\": patchify_staggered_polar_descent,\n",
    "}\n",
    "\n",
    "PATCHIFY_ABBREVIATIONS = {\n",
    "    \"agglomerative\": \"AGG\",\n",
    "    \"breadth_first_bfs_basic\": \"BFSB\",\n",
    "    \"breadth_first_improved_padded\": \"BPIP\",\n",
    "    \"dbscan\": \"DBSCAN\",\n",
    "    \"kmeans\": \"KM\",\n",
    "    \"knn_basic\": \"KNN\",\n",
    "    \"knn_disjoint\": \"DKNN\",\n",
    "    \"latlon_spillover\": \"LLSO\",               # RELIABLE PERFORMANCE\n",
    "    \"latitude_neighbors\": \"LAT\",\n",
    "    \"latitude_simple\": \"LSIM\",\n",
    "    \"latitude_spillover_redo\": \"PSO\",         # Uses PSO (Patchify SpillOver) - TRY THIS\n",
    "    \"lon_spilldown\": \"LSD\",                   # lOW PERFORMING\n",
    "    \"rows\": \"ROW\",                            # RELIABLE PERFORMANCE\n",
    "    \"staggered_polar_descent\": \"SPD\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0706a6-e559-4b22-9584-7e0911854542",
   "metadata": {},
   "source": [
    "# Variables for the Model\n",
    "\n",
    "Check over these CAREFULLY!\n",
    "\n",
    "Note that if you use the login node for training for the Jupyter notebook version (even for the trial dataset that is much smaller), you run the risk of getting the error: # OutOfMemoryError: CUDA out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166144bc-da7c-4a65-b45b-3c666d2be4a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Variables"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Run Settings:\n",
    "PATCHIFY_TO_USE = \"rows\"   # Change this to use other patching techniques\n",
    "\n",
    "MONTHLY =                      True\n",
    "TRIAL_RUN =                    False   # SET THIS TO USE THE PRACTICE SET (MUCH FASTER AND SMALLER, for debugging)\n",
    "NORMALIZE_ON =                 False   # SET THIS TO USE NORMALIZATION ON FREEBOARD (Results are independent of patchify used)\n",
    "TRAINING =                     False    # SET THIS TO RUN THE TRAINING LOOP (Use on full dataset for results)\n",
    "FAST_EVAL_ON =                 False    # SET THIS TO RUN THE METRICS AT THE BOTTOM (Use on full dataset for results)\n",
    "SLOW_EVAL_ON =                 True\n",
    "MAP_WITH_CARTOPY_ON =          False   # Make sure the Cartopy library is included in the kernel\n",
    "\n",
    "# Only run ONCE for daily and once for monthly!!\n",
    "# Run Settings (already performed, not needed now - KEEP FALSE!!!)\n",
    "PLOT_DATA_SPLIT_DISTRIBUTION = False   # Run the data split function to see the train, val, test distribution\n",
    "MAX_FREEBOARD_ON =            False   # To normalize with a pre-defined maximum for outlier handling\n",
    "\n",
    "# --- Time-Related Variables:\n",
    "CONTEXT_LENGTH = 12           # T: Number of historical time steps used for input\n",
    "FORECAST_HORIZON = 12          # Number of future time steps to predict (ex. 1 day for next time step)\n",
    "\n",
    "# --- Model Hyperparameters:\n",
    "D_MODEL = 128                 # d_model: Dimension of the transformer's internal representations (embedding dimension)\n",
    "N_HEAD = 8                    # nhead: Number of attention heads\n",
    "NUM_TRANSFORMER_LAYERS = 4    # num_layers: Number of TransformerEncoderLayers\n",
    "BATCH_SIZE = 4                # Monthly requires smaller batch size, because of fewer samples\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "# Note that when using multiple GPUs batch size will be multiplied by the number of GPUs available (x4).\n",
    "# Monthly: Use a batch size of 4 and num epochs of 40 for best results\n",
    "# Daily: Use a batch size of 16 for 3 day forecast; lower this for longer range; kernel may die with larger sizes\n",
    "\n",
    "# --- Performance-Related Variables:\n",
    "NUM_WORKERS = 64   # 64 worked fast for the 7 day forecast; too many workers causes it to stall out\n",
    "PREFETCH_FACTOR = 4 # 4 worked fast for the 7 day forecast (tried 16 for 4 GPUs, but ran out of shared memory; 8 works ok)\n",
    "\n",
    "# Daily: For 7 day forecast: num workers = 64; prefetch factor = 4\n",
    "\n",
    "# --- Feature-Related Variables:\n",
    "MAX_FREEBOARD_FOR_NORMALIZATION = 1    # Only works when you set MAX_FREEBOARD_ON too; Zarr files don't have this saved.\n",
    "\n",
    "# --- Space-Related Variables:\n",
    "LATITUDE_THRESHOLD = 40          # Determines number of cells and patches (could use -90 to use the entire dataset).\n",
    "CELLS_PER_PATCH = 256            # L: Number of cells within each patch (based on ViT paper 16 x 16 = 256)\n",
    "\n",
    "# SLURM - CONVERT THIS TO A .PY FILE FIRST\n",
    "#PATCHIFY_TO_USE = os.environ.get(\"SLURM_PATCHIFY_TO_USE\", \"rows\") # for SLURM\n",
    "#FORECAST_HORIZON = int(os.environ.get(\"SLURM_FORECAST_HORIZON\", 7)) # for SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38d745-255f-4af4-8869-521733fd3b72",
   "metadata": {},
   "source": [
    "## Other Variables Dependent on Those Above ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60335920-4778-48aa-9cf4-d8a19332bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nCells:        465044\n",
      "Mask size:           53973\n",
      "cells_per_patch:     256\n",
      "n_patches:           210\n"
     ]
    }
   ],
   "source": [
    "mesh = xr.open_dataset(mesh_dir)\n",
    "latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "mesh.close()\n",
    "print(\"Total nCells:       \", len(latCell))\n",
    "\n",
    "mask = latCell >= LATITUDE_THRESHOLD\n",
    "masked_ncells_size = np.count_nonzero(mask)\n",
    "print(\"Mask size:          \", masked_ncells_size)\n",
    "\n",
    "NUM_PATCHES = masked_ncells_size // CELLS_PER_PATCH    # P: Approximate number of spatial patches to expect\n",
    "\n",
    "print(\"cells_per_patch:    \", CELLS_PER_PATCH)\n",
    "print(\"n_patches:          \", NUM_PATCHES)\n",
    "\n",
    "# The input dimension for the patch embedding linear layer.\n",
    "# Each patch at a given time step has NUM_FEATURES * CELLS_PER_PATCH features.\n",
    "# This is the 'D' dimension used in the Transformer's input tensor (B, T, P, D).\n",
    "PATCH_EMBEDDING_INPUT_DIM = NUM_FEATURES * CELLS_PER_PATCH # 2 * 256 = 512\n",
    "\n",
    "DEFAULT_PATCHIFY_METHOD_FUNC = PATCHIFY_FUNCTIONS[PATCHIFY_TO_USE]\n",
    "\n",
    "# --- Common Parameters for all functions ---\n",
    "COMMON_PARAMS = {\n",
    "    \"latCell\": latCell,\n",
    "    \"lonCell\": lonCell,\n",
    "    \"cells_per_patch\": CELLS_PER_PATCH, \n",
    "    \"num_patches\": NUM_PATCHES,\n",
    "    \"latitude_threshold\": LATITUDE_THRESHOLD,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "cellsOnCell = np.load(f'cellsOnCell.npy')\n",
    "\n",
    "# --- Function-specific Parameters (if any) ---\n",
    "SPECIFIC_PARAMS = {\n",
    "    \"latitude_spillover_redo\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"latitude_simple\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"latitude_neighbors\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"breadth_first_improved_padded\": {\"cellsOnCell\": cellsOnCell, \"pad_to_exact_size\": True},\n",
    "    \"breadth_first_bfs_basic\": {\"cellsOnCell\": cellsOnCell},\n",
    "    \"agglomerative\": {\"n_neighbors\": 5},\n",
    "    \"kmeans\": {},\n",
    "    \"dbscan\": {},\n",
    "    \"rows\": {},\n",
    "    \"knn_basic\": {},\n",
    "    \"knn_disjoint\": {},\n",
    "    \"latlon_spillover\": {},\n",
    "    \"lon_spilldown\": {},\n",
    "    \"staggered_polar_descent\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a1e686-766a-49bd-b27c-d851e180a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Version: Mfd_nF_D128_B16_lt40_P210_L256_T12_Fh12_e40_ROW\n",
      "Dataset Name: Monthly_fd_nF_data.zarr\n"
     ]
    }
   ],
   "source": [
    "if NORMALIZE_ON:\n",
    "    if MAX_FREEBOARD_ON:\n",
    "        norm = f\"nT{MAX_FREEBOARD_FOR_NORMALIZATION}\" # Using the specified max value\n",
    "    else:\n",
    "        norm = \"nTM\" # Using the absolute max\n",
    "else:\n",
    "    norm = \"nF\" # Normalization is off\n",
    "    \n",
    "# Get the abbreviation, with a fallback for functions not yet mapped\n",
    "patching_strategy_abbr = PATCHIFY_ABBREVIATIONS.get(PATCHIFY_TO_USE, \"UNKNWN\")\n",
    "\n",
    "if patching_strategy_abbr == \"UNKNWN\":\n",
    "    raise ValueError(\"Check the name of the patchify function\")\n",
    "\n",
    "if TRAINING and not torch.cuda.is_available():\n",
    "    raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "else:\n",
    "    if torch.cuda.device_count() != 0:\n",
    "        BATCH_SIZE = BATCH_SIZE * torch.cuda.device_count()\n",
    "    # import psutil\n",
    "    # NUM_WORKERS = psutil.cpu_count() - 2 # not working!\n",
    "    # print(f\"num_workers is {NUM_WORKERS}\")\n",
    "\n",
    "# Model nome convention\n",
    "model_version = (\n",
    "    f\"{'M' if MONTHLY else 'D'}\" # MONTHLY OR DAILY\n",
    "    f\"{'tr' if TRIAL_RUN else 'fd'}_\" # TRIAL DATASET OR FULL DATASET\n",
    "    f\"{norm}_D{D_MODEL}_B{BATCH_SIZE}_lt{LATITUDE_THRESHOLD}_P{NUM_PATCHES}_L{CELLS_PER_PATCH}\"\n",
    "    f\"_T{CONTEXT_LENGTH}_Fh{FORECAST_HORIZON}_e{NUM_EPOCHS}_{patching_strategy_abbr}\"\n",
    ")\n",
    "\n",
    "# Place to save and load the data\n",
    "PROCESSED_DATA_DIR = (\n",
    "    f\"{'Monthly_' if MONTHLY else 'Daily_'}{'tr' if TRIAL_RUN else 'fd'}_{norm}_data.zarr\"\n",
    ")\n",
    "\n",
    "print(f\"Model Version: {model_version}\")\n",
    "print(f\"Dataset Name: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11cf9a-a867-4740-9d75-36f580e8d386",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "* TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "* TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "* TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "* TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "* Too few workers: GPUs might become idle waiting for data.\n",
    "* Too many workers: Can lead to increased CPU memory usage and context switching overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# More Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca026cf-05df-431c-9928-7284e71696c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn version 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print('Seaborn version', sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d15e35-d5b4-41e2-956d-95b6504edcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "if TRAINING and not torch.cuda.is_available():\n",
    "    raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "else:\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\") # check the number of available CUDA devices\n",
    "    # will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor Name: x86_64\n",
      "Physical Cores: 64\n",
      "Logical Cores: 128\n",
      "Current CPU Frequency: 2523.36 MHz\n",
      "Min CPU Frequency: 1500.00 MHz\n",
      "Max CPU Frequency: 2450.00 MHz\n",
      "Total CPU Usage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set level to logging.INFO to see the statements\n",
    "logging.basicConfig(filename=f'{model_version}_dataset.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "class MonthlyOrDailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transform : Callable | None\n",
    "        Optional - transform applied to the data tensor *only*.\n",
    "    processed_data_path\n",
    "        The Zarr directory from which to retrieve the preprocesed data  \n",
    "    context_length\n",
    "        The number of time steps to fetch for input in the prediction step (needed for __len__)\n",
    "    forecast_horizon\n",
    "        The number of time steps to predict in the future (needed for __len__)\n",
    "    load_normalized\n",
    "        Determines which version of the pre-processed data to load\n",
    "    num_patches\n",
    "        How many patches to use for the patchify function\n",
    "    cells_per_patch\n",
    "        How many cells to have in each patch for patchify\n",
    "    patchify_func : Callable\n",
    "        The patchify function to use (ex., patchify_by_latlon_spillover).\n",
    "    patchify_func_key : str\n",
    "        The string key identifying the patchify function (e.g., \"latlon_spillover\")\n",
    "        used to look up its specific parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Callable = None,\n",
    "\n",
    "        # File parameters\n",
    "        processed_data_path: str = PROCESSED_DATA_DIR,\n",
    "\n",
    "        # For __len__ function\n",
    "        context_length: int = CONTEXT_LENGTH,\n",
    "        forecast_horizon: int = FORECAST_HORIZON,\n",
    "\n",
    "        # Run Settings      \n",
    "        load_normalized: bool = NORMALIZE_ON, \n",
    "\n",
    "        # Patchification parameters\n",
    "        num_patches: int = NUM_PATCHES,\n",
    "        cells_per_patch: int = CELLS_PER_PATCH,\n",
    "        patchify_func: Callable = DEFAULT_PATCHIFY_METHOD_FUNC, # Default patchify function\n",
    "        patchify_func_key: str = PATCHIFY_TO_USE, # Key to look up specific params\n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "        1) Load the pre-processed data from Zarr\n",
    "        2) Apply the specified patchify and store patch_ids so the data loader can use them\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for performance\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Save parameters\n",
    "        self.transform = transform\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.load_normalized = load_normalized\n",
    "        self.num_patches = num_patches\n",
    "        self.cells_per_patch = cells_per_patch\n",
    "        self.patchify_func = patchify_func # Store the specified patchify function\n",
    "        self.patchify_func_key = patchify_func_key # Store the key for looking up specific params\n",
    "        self.processed_data_path = processed_data_path # Store the processed data path\n",
    "\n",
    "        # --- 1. Load pre-processed data from Zarr ---\n",
    "        if not os.path.exists(self.processed_data_path):\n",
    "            raise FileNotFoundError(f\"Pre-processed Zarr file not found. Please run `preprocess_data_variables.py` first.\")\n",
    "\n",
    "        try:\n",
    "            '''\n",
    "            Note that \"time\" will be 2100 for Monthly and 63875 for Daily\n",
    "            \n",
    "            Dimensions:         (nCells_full: 465044, time: 2100, nCells_masked: 53973)\n",
    "            Coordinates:\n",
    "              * nCells_full     (nCells_full) int64 4MB 0 1 2 3 ... 465041 465042 465043\n",
    "              * nCells_masked   (nCells_masked) int64 432kB 0 1 2 3 ... 53970 53971 53972\n",
    "              * time            (time) datetime64[ns] 511kB 1850-01-01T00:30:00 ... 2024-...\n",
    "            Data variables:\n",
    "                cell_mask       (nCells_full) bool 465kB dask.array<chunksize=(232522,), meta=np.ndarray>\n",
    "                freeboard       (time, nCells_masked) float32 14GB dask.array<chunksize=(999, 1687), meta=np.ndarray>\n",
    "                full_to_masked  <U1337572 5MB ...\n",
    "                ice_area        (time, nCells_masked) float32 14GB dask.array<chunksize=(999, 1687), meta=np.ndarray>\n",
    "                masked_to_full  <U1337572 5MB ...\n",
    "                num_raw_files   int64 8B ...\n",
    "                times           (time) datetime64[ns] 511kB dask.array<chunksize=(31938,), meta=np.ndarray>\n",
    "                '''\n",
    "            processed_ds = xr.open_zarr(self.processed_data_path)\n",
    "            print(processed_ds)\n",
    "            logging.info(processed_ds)\n",
    "\n",
    "            # Times\n",
    "            print(f\"processed_ds['times'].values.shape: {processed_ds['times'].values.shape}  # Expected: ({'2100' if MONTHLY else '63875'},)\")\n",
    "            logging.info(f\"times.shape = {processed_ds['times'].values.shape}\")\n",
    "            self.times = processed_ds[\"times\"].values\n",
    "            \n",
    "            # Ice Area\n",
    "            print(f\"processed_ds['ice_area'].load().values.shape: {processed_ds['ice_area'].load().values.shape}  # Expected: ({'2100' if MONTHLY else '63875'}, 53973)\")\n",
    "            logging.info(f\"ice_area.shape = {processed_ds['ice_area'].load().values.shape}\")\n",
    "            self.ice_area = processed_ds[\"ice_area\"].load().values\n",
    "            \n",
    "            # Freeboard\n",
    "            print(f\"processed_ds['freeboard'].load().values.shape: {processed_ds['freeboard'].load().values.shape}  # Expected: ({'2100' if MONTHLY else '63875'}, 53973)\")\n",
    "            logging.info(f\"freeboard.shape = {processed_ds['freeboard'].load().values.shape}\")\n",
    "            self.freeboard = processed_ds[\"freeboard\"].load().values\n",
    "            \n",
    "            # Cell Mask\n",
    "            print(f\"processed_ds['cell_mask'].values.shape: {processed_ds['cell_mask'].values.shape}  # Expected: (465044,)\")\n",
    "            logging.info(f\"cell_mask.shape = {processed_ds['cell_mask'].values.shape}\")\n",
    "            self.cell_mask = processed_ds[\"cell_mask\"].values\n",
    "            \n",
    "            # full_to_masked\n",
    "            self.full_to_masked = eval(processed_ds[\"full_to_masked\"].item())\n",
    "            print(f\"evaluated full_to_masked shape: {len(self.full_to_masked)}  # Expected: (53973,)\")\n",
    "            logging.info(f\"evaluated full_to_masked shape = {np.shape(self.full_to_masked)}\")\n",
    "            \n",
    "            # masked_to_full\n",
    "            self.masked_to_full = eval(processed_ds[\"masked_to_full\"].item())\n",
    "            print(f\"evaluated masked_to_full shape: {len(self.masked_to_full)}  # Expected: (53973,)\")\n",
    "            logging.info(f\"evaluated masked_to_full shape = {np.shape(self.masked_to_full)}\")\n",
    "            \n",
    "            # num_raw_files\n",
    "            print(f\"processed_ds['num_raw_files'].item(): {processed_ds['num_raw_files'].item()}  # Expected: 2100\")\n",
    "            logging.info(f\"num_raw_files = {processed_ds['num_raw_files'].item()}\")\n",
    "            self.num_raw_files = processed_ds[\"num_raw_files\"].item()\n",
    "                        \n",
    "            logging.info(f\"Loaded pre-processed data from Zarr in {time.perf_counter() - start_time:.2f} seconds.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading Zarr store: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load Zarr store. Check the file or rerun preprocessing: {e}\")\n",
    "\n",
    "        # --- 2. Patchify  ---\n",
    "        logging.info(f\"=== Patchifying using {self.patchify_func_key} algorithm ===\")\n",
    "        patchify_start_time = time.perf_counter()\n",
    "\n",
    "        patchify_call_params = COMMON_PARAMS.copy()\n",
    "        patchify_call_params.update(SPECIFIC_PARAMS.get(self.patchify_func_key, {}))\n",
    "\n",
    "        self.latCell = COMMON_PARAMS[\"latCell\"]\n",
    "        \n",
    "        # Use the dynamic patchify function\n",
    "        #     Returns \n",
    "        # full_nCells_patch_ids : np.ndarray\n",
    "        #     Array of shape (nCells,) giving patch ID or -1 if unassigned.\n",
    "        # indices_per_patch_id : List[np.ndarray]\n",
    "        #     List of patches, each a list of cell indices (np.ndarray of ints) that correspond with nCells array.\n",
    "        # patch_latlons : np.ndarray\n",
    "        #     Array of shape (n_patches, 2) containing (latitude, longitude) for one\n",
    "        #     representative cell per patch (the first cell added to the patch)\n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id, self.patch_latlons, self.algorithm = self.patchify_func(**patchify_call_params)\n",
    "\n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        # This ensures there's no out of bounds problem,\n",
    "        # like index 296237 is out of bounds for axis 1 with size 53973\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "        \n",
    "        logging.info(f\"Patchifying completed in {time.perf_counter() - patchify_start_time:.2f} seconds.\")\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        logging.info(f\"Total time steps collected (days or months): {len(self.times)}\")\n",
    "        logging.info(f\"Unique times: {len(np.unique(self.times))}\")\n",
    "        logging.info(f\"First 35 time values: {self.times[:35]}\") # TODO: MAYBE CHANGE FOR MONTHS?\n",
    "        logging.info(f\"Last 35 time values: {self.times[-35:]}\")\n",
    "\n",
    "        logging.info(f\"Shape of combined ice_area array: {self.ice_area.shape}\")\n",
    "        logging.info(f\"Shape of combined freeboard array: {self.freeboard.shape}\")\n",
    "\n",
    "        logging.info(f\"Elapsed time for MonthlyOrDailyNetCDFDataset __init__: {time.perf_counter() - start_time} seconds\")\n",
    "        print(f\"Elapsed time for MonthlyOrDailyNetCDFDataset __init__: {time.perf_counter() - start_time} seconds\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of possible starting indices (idx) for a valid sequence.\n",
    "        A valid sequence needs `self.context_length` for input and `self.forecast_horizon` for the target.\n",
    "        \n",
    "        ex) For daily data, if the total number of days is 365, \n",
    "        the context_length is 7 and the forecast_horizon is 3, then\n",
    "        \n",
    "        365 - (7 + 3) + 1 = 365 - 10 + 1 = 356 is the final valid starting index\n",
    "\n",
    "        ex) For monthly data, if the total number of months is 12, \n",
    "        the context_length is 6 and the forecast_horizon is 3, then\n",
    "        12 - (6 + 3) + 1 = 12 - 9 + 1 = 4 is the final valid starting index\n",
    "\n",
    "        return len(self.freeboard) - (self.context_length + self.forecast_horizon)\n",
    "                \n",
    "        \"\"\"\n",
    "        required_length = self.context_length + self.forecast_horizon\n",
    "\n",
    "        # Error check\n",
    "        if len(self.freeboard) < required_length:\n",
    "            return 0 # Not enough raw data to form even one sample\n",
    "\n",
    "        # The number of valid starting indices\n",
    "        return len(self.freeboard) - required_length + 1\n",
    "\n",
    "    def get_patch_tensor(self, time_step_idx: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Retrieves the feature data for a specific time step, organized into patches.\n",
    "\n",
    "        This method extracts 'freeboard' and 'ice_area' data for a given time step (month or day)\n",
    "        and then reshapes it according to the pre-defined patches. Each patch\n",
    "        will contain its own set of feature values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        time_step_idx : int\n",
    "            The integer index of the time step to retrieve data for, relative to the\n",
    "            concatenated dataset's time dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing the feature data organized by patches for the\n",
    "            specified month or day.\n",
    "            Shape: (context_length, num_patches, num_features, patch_size)\n",
    "            Where:\n",
    "            - num_patches: Total number of patches (ex., 210).\n",
    "            - num_features: The number of features per cell (currently 2: freeboard, ice_area).\n",
    "            - patch_size: The number of cells within each patch (ex., 256)\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        freeboard_now = self.freeboard[time_step_idx]  # (nCells,)\n",
    "        ice_area_now = self.ice_area[time_step_idx]    # (nCells,)\n",
    "        features = np.stack([freeboard_now, ice_area_now], axis=0)  # (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(patch_tensors)  # (context_length, num_patches, num_features, patch_size)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Given an input of a certain date id, get the input and the target tensors\n",
    "        2. Return all the patches for the input and the target\n",
    "           Features are: [freeboard, ice_area] over masked cells. \n",
    "           \n",
    "        \"\"\"\n",
    "        # Start with the id of the time step (month or day) in question\n",
    "        start_idx = idx\n",
    "\n",
    "        # end_idx is the exclusive end of the input sequence,\n",
    "        # and the inclusive start of the target sequence.\n",
    "        end_idx = idx + self.context_length    # ex. start is 0, context length is 3, end is 3, exclusive\n",
    "        target_start = end_idx                 # target starts indexing at 3\n",
    "\n",
    "        # the target sequence ends after forecast horizon\n",
    "        target_end = end_idx + self.forecast_horizon       # target ends at 3 + 7 = 10 exclusive\n",
    "\n",
    "        if target_end > len(self.freeboard):\n",
    "            raise IndexError(\n",
    "                f\"Requested time window exceeds dataset. \"\n",
    "                f\"Problematic idx: {idx}, \"\n",
    "                f\"Context Length: {self.context_length}, \"\n",
    "                f\"Forecast Horizon: {self.forecast_horizon}, \"\n",
    "                f\"Calculated target_end: {target_end}, \"\n",
    "                f\"Actual dataset length (len(self.freeboard)): {len(self.ice_area)}\"\n",
    "            )\n",
    "\n",
    "        # Build input tensor\n",
    "        input_seq = [self.get_patch_tensor(i) for i in range(start_idx, end_idx)]\n",
    "        input_tensor = torch.stack(input_seq)\n",
    "    \n",
    "        # Build target tensor: shape (forecast_horizon, num_patches)\n",
    "        target_seq = self.ice_area[end_idx:target_end]\n",
    "        target_patches = []\n",
    "        for time_step in target_seq:\n",
    "            patch_time_step = [\n",
    "                torch.tensor(time_step[patch_indices]) for patch_indices in self.indices_per_patch_id\n",
    "            ]\n",
    "            \n",
    "            # After stacking, patch_time_step_tensor will be (num_patches, CELLS_PER_PATCH)\n",
    "            patch_time_step_tensor = torch.stack(patch_time_step)  # (num_patches,)\n",
    "            target_patches.append(patch_time_step_tensor)\n",
    "\n",
    "        # Final target tensor shape: (forecast_horizon, num_patches, CELLS_PER_PATCH)\n",
    "        target_tensor = torch.stack(target_patches)  # (forecast_horizon, num_patches)\n",
    "        \n",
    "        return input_tensor, target_tensor, start_idx, end_idx, target_start, target_end\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<\"\n",
    "            f\"{self.processed_data_path}\"\n",
    "            f\"\\nInstance of MonthlyOrDailyNetCDFDataset\"\n",
    "            f\"\\n{len(self)} viable time steps (only includes up to the last possible input date)\"\n",
    "            f\"\\n{len(self.freeboard[0])} cells/time_step\"\n",
    "            f\"\\n{self.num_raw_files} files loaded \"\n",
    "            f\"\\n{len(self.ice_area)} ice_area length\"\n",
    "            f\"\\n{len(self.freeboard)} freeboard length\"\n",
    "            f\"\\nPatchify Algorithm: {self.algorithm}\" # What patchify algorithm was used\n",
    "            f\"\\n # The following should be ({'2100' if MONTHLY else '63875'}, 53973) for {'Monthly' if MONTHLY else 'Daily'} data at a latitude_threshold of 40\"\n",
    "            f\"\\n{self.ice_area.shape} shape of ice_area\"\n",
    "            f\"\\n{self.freeboard.shape} shape of freeboard\"\n",
    "            f\"\\n{len(self.indices_per_patch_id)} indices_per_patch_id # Should be {len(self.freeboard[0]) // CELLS_PER_PATCH}\"\n",
    "            f\">\" \n",
    "\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f515e6b-b655-4496-9ce7-adf5833540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "class YearlySubset(Dataset):\n",
    "    def __init__(self, raw_subset, context_length=CONTEXT_LENGTH, forecast_horizon=FORECAST_HORIZON):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_subset (Subset): A Subset of the main dataset containing raw day indices.\n",
    "            context_length (int): The length of the input sequence.\n",
    "            forecast_horizon (int): The length of the target sequence.\n",
    "        \"\"\"\n",
    "        self.raw_subset = raw_subset\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # The indices of the raw data that the subset has access to.\n",
    "        # These are the global indices of the original dataset.\n",
    "        self.raw_indices = self.raw_subset.indices\n",
    "\n",
    "        # Calculate the number of viable samples within this subset.\n",
    "        # A sample starts at index `i` of the raw_subset.\n",
    "        # A sample requires a total window of `context_length + forecast_horizon` time steps\n",
    "        required_window_size = self.context_length + self.forecast_horizon\n",
    "\n",
    "        # The last valid starting index in the raw_subset is its length minus the window size.\n",
    "        self.num_viable_samples = len(self.raw_subset) - required_window_size + 1\n",
    "        \n",
    "        # Check for invalid cases\n",
    "        if self.num_viable_samples <= 0:\n",
    "            print(f\"Warning: {type(self.raw_subset).__name__} is too small to form a single sample.\")\n",
    "            self.num_viable_samples = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_viable_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the raw_subset.\n",
    "        The idx here is for the viable samples within the subset, not the global dataset.\n",
    "        \"\"\"\n",
    "        # Get the global index of the starting day for this sample.\n",
    "        # This idx is relative to the start of the raw_subset.\n",
    "        global_start_idx = self.raw_indices[idx]\n",
    "        \n",
    "        # The main dataset's __getitem__ takes a global index.\n",
    "        # We need to call the main dataset's __getitem__ with the correct global index.\n",
    "        # The main dataset is `self.raw_subset.dataset`.\n",
    "        \n",
    "        # The main dataset's __getitem__ method already handles the windowing, so we\n",
    "        # just need to pass the correct starting index.\n",
    "        return self.raw_subset.dataset[global_start_idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"<{type(self.raw_subset).__name__} with {self.num_viable_samples} viable samples>\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class: TRIAL_RUN MODE IS False ===== \n",
      "<xarray.Dataset> Size: 922MB\n",
      "Dimensions:         (nCells_full: 465044, time: 2100, nCells_masked: 53973)\n",
      "Coordinates:\n",
      "  * nCells_full     (nCells_full) int64 4MB 0 1 2 3 ... 465041 465042 465043\n",
      "  * nCells_masked   (nCells_masked) int64 432kB 0 1 2 3 ... 53970 53971 53972\n",
      "  * time            (time) datetime64[ns] 17kB 1850-01-01T00:30:00 ... 2024-1...\n",
      "Data variables:\n",
      "    cell_mask       (nCells_full) bool 465kB dask.array<chunksize=(232522,), meta=np.ndarray>\n",
      "    freeboard       (time, nCells_masked) float32 453MB dask.array<chunksize=(132, 3374), meta=np.ndarray>\n",
      "    full_to_masked  <U1337572 5MB ...\n",
      "    ice_area        (time, nCells_masked) float32 453MB dask.array<chunksize=(132, 3374), meta=np.ndarray>\n",
      "    masked_to_full  <U1337572 5MB ...\n",
      "    num_raw_files   int64 8B ...\n",
      "    times           (time) datetime64[ns] 17kB dask.array<chunksize=(2100,), meta=np.ndarray>\n",
      "processed_ds['times'].values.shape: (2100,)  # Expected: (2100,)\n",
      "processed_ds['ice_area'].load().values.shape: (2100, 53973)  # Expected: (2100, 53973)\n",
      "processed_ds['freeboard'].load().values.shape: (2100, 53973)  # Expected: (2100, 53973)\n",
      "processed_ds['cell_mask'].values.shape: (465044,)  # Expected: (465044,)\n",
      "evaluated full_to_masked shape: 53973  # Expected: (53973,)\n",
      "evaluated masked_to_full shape: 53973  # Expected: (53973,)\n",
      "processed_ds['num_raw_files'].item(): 2100  # Expected: 2100\n",
      "Number of cells considered for patching: 53973\n",
      "Cluster sizes:\n",
      "min size 256\n",
      "max size 256\n",
      "smallest count (np.int64(209), 256)\n",
      "max count (np.int64(0), 256)\n",
      "number of patches: 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/b/brelypo/Predicting_SIC_SIE/NC_FILE_PROCESSING/patchify_utils.py:34: UserWarning: Warning for 'row_by_row': 213 cells were unassigned (patch ID -1) and will be excluded from the patch distribution plot.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST PATCH SIZE:  256\n",
      "Contains a -1 index  False\n",
      "Elapsed time for MonthlyOrDailyNetCDFDataset __init__: 1.7145132299046963 seconds\n",
      "========== SPLITTING THE DATASET ===================\n",
      "Training data length:    1933\n",
      "Validation data length:  49\n",
      "Testing data length:     49\n",
      "Total Time Steps =  2031\n",
      "Number of training batches 120\n",
      "Number of validation batches 3\n",
      "Number of test batches after drop_last incomplete batch 3\n",
      "===== Printing Dataset ===== \n",
      "<Monthly_fd_nF_data.zarr\n",
      "Instance of MonthlyOrDailyNetCDFDataset\n",
      "2077 viable time steps (only includes up to the last possible input date)\n",
      "53973 cells/time_step\n",
      "2100 files loaded \n",
      "2100 ice_area length\n",
      "2100 freeboard length\n",
      "Patchify Algorithm: row_by_row\n",
      " # The following should be (2100, 53973) for Monthly data at a latitude_threshold of 40\n",
      "(2100, 53973) shape of ice_area\n",
      "(2100, 53973) shape of freeboard\n",
      "210 indices_per_patch_id # Should be 210>\n",
      "===== Sample at dataset[0] ===== \n",
      "Fetched start index 0: Time=1850-01-01T00:30:00.000000000\n",
      "Fetched end   index 12: Time=1851-01-01T00:30:00.000000000\n",
      "Fetched target start index 12: Time=1851-01-01T00:30:00.000000000\n",
      "Fetched target end   index 24: Time=1852-01-01T00:30:00.000000000\n",
      "===== Start and End Dates for Each Set =====\n",
      "Training set start date: 1850-01-01T00:30:00.000000000\n",
      "Training set end date (cosmetic): 2012-12-01T00:30:00.000000000\n",
      "(actual last viable target day): 2012-12-01T00:30:00.000000000\n",
      "Validation set start date: 2013-01-01T00:30:00.000000000\n",
      "Validation set end date (cosmetic): 2018-12-01T00:30:00.000000000\n",
      "(actual last viable target day): 2018-12-01T00:30:00.000000000\n",
      "Testing set start date: 2019-01-01T00:30:00.000000000\n",
      "Testing set end date (cosmetic): 2024-12-01T00:30:00.000000000\n",
      "(actual last viable target day): 2024-12-01T00:30:00.000000000\n",
      "===== Starting DataLoader ====\n",
      "train_loader length: 120\n",
      "val_loader length: 3\n",
      "test_loader length: 3\n",
      "input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\n",
      "actual input_tensor.shape = torch.Size([12, 210, 2, 256])\n",
      "target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\n",
      "actual target_tensor.shape = torch.Size([12, 210, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(f\"===== Making the Dataset Class: TRIAL_RUN MODE IS {TRIAL_RUN} ===== \")\n",
    "\n",
    "# load all the data from one folder\n",
    "dataset = MonthlyOrDailyNetCDFDataset()\n",
    "\n",
    "# Patch locations for positional embedding\n",
    "PATCH_LATLONS_TENSOR = torch.tensor(dataset.patch_latlons, dtype=torch.float32)\n",
    "\n",
    "print(\"========== SPLITTING THE DATASET ===================\")\n",
    "# DIFFERENT SUBSET OPTIONS FOR TRAINING / VALIDATION / TESTING for the trial data vs. full dataset\n",
    "if TRIAL_RUN:\n",
    "    total_time_steps = len(dataset)\n",
    "    train_end = int(total_time_steps * 0.7)\n",
    "    val_end = int(total_time_steps * 0.85)\n",
    "    \n",
    "    train_set = Subset(dataset, range(0, train_end))\n",
    "    val_set   = Subset(dataset, range(train_end, val_end))\n",
    "    test_set  = Subset(dataset, range(val_end, total_time_steps))\n",
    "    \n",
    "else:\n",
    "\n",
    "    # --- Splitting by Valid Years (considering the context_length and forecast_horizon) ---\n",
    "\n",
    "    # Convert dataset.times to pandas DatetimeIndex for easier year-based filtering\n",
    "    all_times_pd = pd.to_datetime(dataset.times)\n",
    "\n",
    "    # Define the start and end years for each set - keep this for the full dataset\n",
    "    train_start_year = 1850\n",
    "    train_end_year = 2012   \n",
    "    val_start_year = 2013\n",
    "    val_end_year = 2018\n",
    "    test_start_year = 2019\n",
    "    test_end_year = 2024\n",
    "\n",
    "    # Now use these times for year-based splitting\n",
    "    all_times_pd = pd.to_datetime(dataset.times)\n",
    "    train_mask = (all_times_pd.year >= train_start_year) & (all_times_pd.year <= train_end_year)\n",
    "    val_mask = (all_times_pd.year >= val_start_year) & (all_times_pd.year <= val_end_year)\n",
    "    test_mask = (all_times_pd.year >= test_start_year) & (all_times_pd.year <= test_end_year)\n",
    "    \n",
    "    # Get the integer indices where the masks are True\n",
    "    train_indices = np.where(train_mask)[0].tolist()\n",
    "    val_indices = np.where(val_mask)[0].tolist()\n",
    "    test_indices = np.where(test_mask)[0].tolist()\n",
    "    \n",
    "    # Create the raw subsets first\n",
    "    raw_train_set = Subset(dataset, train_indices)\n",
    "    raw_val_set = Subset(dataset, val_indices)\n",
    "    raw_test_set = Subset(dataset, test_indices)\n",
    "\n",
    "    # The Subset now gets indices that are valid for the *sliding window*\n",
    "    # Now, wrap the raw subsets in our new YearlySubset class to get valid windows\n",
    "    train_set = YearlySubset(raw_train_set, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
    "    val_set = YearlySubset(raw_val_set, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
    "    test_set = YearlySubset(raw_test_set, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
    "        \n",
    "    train_end = train_indices[-1]\n",
    "    val_end = val_indices[-1]\n",
    "\n",
    "print(\"Training data length:   \", len(train_set))\n",
    "print(\"Validation data length: \", len(val_set))\n",
    "print(\"Testing data length:    \", len(test_set))\n",
    "\n",
    "total_time_steps = len(train_set) + len(val_set) + len(test_set)\n",
    "print(\"Total Time Steps = \", total_time_steps)\n",
    "\n",
    "print(\"Number of training batches\", len(train_set)//BATCH_SIZE)\n",
    "print(\"Number of validation batches\", len(val_set)//BATCH_SIZE)\n",
    "print(\"Number of test batches after drop_last incomplete batch\", len(test_set)//BATCH_SIZE)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__  see how many files & time steps loaded\n",
    "\n",
    "print(\"===== Sample at dataset[0] ===== \")\n",
    "input_tensor, target_tensor, start_idx, end_idx, target_start, target_end = dataset[0]\n",
    "\n",
    "print(f\"Fetched start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched end   index {end_idx}: Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched target start index {target_start}: Time={dataset.times[target_start]}\")\n",
    "print(f\"Fetched target end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "def print_set_dates(dataset_subset, set_name):\n",
    "    \"\"\" Print start and end dates for each set (Training, Validation, Testing)\"\"\"\n",
    "    if len(dataset_subset) == 0:\n",
    "        print(f\"{set_name} set: No data available.\")\n",
    "        return\n",
    "\n",
    "    # Get the global index of the first viable sample's start day.\n",
    "    first_global_idx = dataset_subset.raw_subset.indices[0]\n",
    "\n",
    "    # The cosmetic end date is simply the last day in the raw subset's indices.\n",
    "    raw_end_date_idx = dataset_subset.raw_subset.indices[-1]\n",
    "\n",
    "    # Get the global index of the last viable sample's start day.\n",
    "    # The last viable sample's start index is relative to the subset, so it's `len(dataset_subset) - 1`.\n",
    "    # We must map this back to the global index using the raw_indices.\n",
    "    last_viable_start_idx = len(dataset_subset) - 1\n",
    "    \n",
    "    # Ensure there's at least one viable sample before trying to access its index\n",
    "    if last_viable_start_idx < 0:\n",
    "        print(f\"{set_name} set: Not enough data to form a single viable sample.\")\n",
    "        return\n",
    "\n",
    "    last_global_idx_of_viable_start = dataset_subset.raw_subset.indices[last_viable_start_idx]\n",
    "\n",
    "    # The 'actual last viable target day' is the end date of the last full sample's target sequence.\n",
    "    # This requires adding the context and forecast lengths to the last global start index.\n",
    "    valid_end_date_idx = last_global_idx_of_viable_start + dataset_subset.context_length + dataset_subset.forecast_horizon - 1\n",
    "    \n",
    "    # Fetch the actual datetime objects\n",
    "\n",
    "    # Note: For the training, validation, and testing sets, each item (idx) represents the *start*\n",
    "    # of a `context_length + forecast_horizon` window.\n",
    "    # So, the start date of a set is the `dataset.times` value at the global index of its first item.\n",
    "    start_date = dataset.times[first_global_idx]\n",
    "    raw_end_date = dataset.times[raw_end_date_idx]\n",
    "    valid_end_date = dataset.times[valid_end_date_idx]\n",
    "\n",
    "    print(f\"{set_name} set start date: {start_date}\")\n",
    "    print(f\"{set_name} set end date (cosmetic): {raw_end_date}\")\n",
    "    print(f\"(actual last viable target day): {valid_end_date}\")\n",
    "    \n",
    "    logging.info(f\"{set_name} set start date: {start_date}\")\n",
    "    logging.info(f\"{set_name} set end date (cosmetic): {raw_end_date})\")\n",
    "    logging.info(f\"(actual last viable target day): {valid_end_date}\")\n",
    "    \n",
    "    return str(start_date), str(raw_end_date) # Returning raw_end_date for consistency with previous calls\n",
    "\n",
    "print(\"===== Start and End Dates for Each Set =====\")\n",
    "train_set_start_year, train_set_end_year = print_set_dates(train_set, \"Training\")\n",
    "val_set_start_year, val_set_end_year = print_set_dates(val_set, \"Validation\")\n",
    "test_set_start_year, test_set_end_year = print_set_dates(test_set, \"Testing\")\n",
    "\n",
    "train_set_start_year, train_set_end_year = train_set_start_year[:4], train_set_end_year[:4]\n",
    "val_set_start_year, val_set_end_year = val_set_start_year[:4], val_set_end_year[:4]\n",
    "test_set_start_year, test_set_end_year = test_set_start_year[:4], test_set_end_year[:4]\n",
    "\n",
    "print(\"===== Starting DataLoader ====\")\n",
    "# wrap in a DataLoader\n",
    "# 1. Use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. Use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "# 3. Use shuffle=False to preserve time order (especially for forecasting)\n",
    "# 4. Use drop_last=True to prevent it from testing on incomplete batches\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR, drop_last=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR, drop_last=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=PREFETCH_FACTOR, drop_last=True)\n",
    "\n",
    "print(f\"train_loader length: {len(train_loader)}\")\n",
    "print(f\"val_loader length: {len(val_loader)}\")\n",
    "print(f\"test_loader length: {len(test_loader)}\")\n",
    "\n",
    "if len(val_loader) < 1 or len(test_loader) < 1:\n",
    "    raise ValueError(\"Make BATCH_SIZE smaller to avoid zero-length loaders\")\n",
    "\n",
    "print(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "print(f\"actual input_tensor.shape = {input_tensor.shape}\")\n",
    "print(\"target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\")\n",
    "print(f\"actual target_tensor.shape = {target_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d66bc7-a29a-419f-a146-df434ddcc759",
   "metadata": {},
   "source": [
    "## Checking the distribution of train, validation, and testing sets\n",
    "This takes a long time. Only do this once for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4278c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conditional Data collection for Training and Validation Sets ---\n",
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "\n",
    "if PLOT_DATA_SPLIT_DISTRIBUTION:\n",
    "    print(\"\\n--- Collecting ground truth data directly from subsets ---\")\n",
    "    start_time_collect_data_new = time.perf_counter()\n",
    "    all_train_actual_values = []\n",
    "    all_val_actual_values = []\n",
    "    all_test_actual_values = []\n",
    "\n",
    "    # Iterate directly over the subsets\n",
    "    for i in range(len(test_set)):\n",
    "        _, sample_y, *_ = test_set[i]\n",
    "        all_test_actual_values.append(sample_y.cpu().numpy().flatten())\n",
    "    final_test_values = np.concatenate(all_test_actual_values)\n",
    "    print(\"Finished iterating over test_set\")\n",
    "\n",
    "    for i in range(len(val_set)):\n",
    "        _, sample_y, *_ = val_set[i]\n",
    "        all_val_actual_values.append(sample_y.cpu().numpy().flatten())\n",
    "    final_val_values = np.concatenate(all_val_actual_values)\n",
    "    print(\"Finished iterating over val_set\")\n",
    "    \n",
    "    for i in range(len(train_set)):\n",
    "        _, sample_y, *_ = train_set[i]\n",
    "        all_train_actual_values.append(sample_y.cpu().numpy().flatten())\n",
    "    final_train_values = np.concatenate(all_train_actual_values)\n",
    "    print(\"Finished iterating over train_set\")\n",
    "    \n",
    "    print(\"--- Finished collecting ground truth data from subsets ---\")\n",
    "    print(f\"Elapsed time for collecting ground truth method: {time.perf_counter() - start_time_collect_data_new:.2f} seconds\")\n",
    "\n",
    "else:\n",
    "    final_train_values = np.array([])\n",
    "    final_val_values = np.array([])\n",
    "    final_test_values = np.array([])\n",
    "\n",
    "# 9. Conditional Plotting All Three SIC Distributions (Train, Val, Test Sets)\n",
    "if PLOT_DATA_SPLIT_DISTRIBUTION:\n",
    "    print(\"\\n--- Plotting data distributions (Train, Val, Test) ---\")\n",
    "    start_time_plot_data_dist = time.perf_counter()\n",
    "    plot_sic_distribution_bars(\n",
    "        train_data=final_train_values,\n",
    "        val_data=final_val_values,\n",
    "        test_data=final_test_values,\n",
    "        start_date=train_set_start_year,\n",
    "        end_date=test_set_end_year,\n",
    "        num_bins=10\n",
    "    )\n",
    "    print(f\"Elapsed time for plotting data distribution comparison: {time.perf_counter() - start_time_plot_data_dist:.2f} seconds\")\n",
    "\n",
    "    # Calculate Pairwise Jensen-Shannon Distances for Data Splits\n",
    "    print(\"\\n--- Pairwise Jensen-Shannon Distances for Data Splits ---\")\n",
    "    start_time_jsd_pairwise = time.perf_counter()\n",
    "    distributions_for_jsd = {\n",
    "        'train': final_train_values,\n",
    "        'validation': final_val_values,\n",
    "        'test': final_test_values\n",
    "    }\n",
    "    \n",
    "    jsd_bins = np.linspace(0, 1, 10 + 1)\n",
    "    pairwise_jsd_results = jensen_shannon_distance_pairwise(distributions_for_jsd, jsd_bins)\n",
    "    \n",
    "    for pair, jsd_val in pairwise_jsd_results.items():\n",
    "        print(f\"JSD ({pair}): {jsd_val:.4f}\")\n",
    "    end_time_jsd_pairwise = time.perf_counter()\n",
    "    print(f\"Elapsed time for Jensen Shannon Pairwise Calculation: {end_time_jsd_pairwise - start_time_jsd_pairwise:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d9baa0-427e-49d9-a50a-97c3d60a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class IceForecastTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Transformer-based model for forecasting ice conditions based on sequences of\n",
    "    historical patch data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_patch_features_dim : int\n",
    "        The dimensionality of the feature vector for each individual patch (ex. 2 features).\n",
    "        This is the input dimension for the patch embedding layer.\n",
    "    num_patches : int\n",
    "        The total number of geographical patches that the `nCells` data was divided into.\n",
    "        (ex., 256 patches).\n",
    "    context_length : int, optional\n",
    "        The number of historical time steps (days or months) to use as input for the transformer.\n",
    "    forecast_horizon : int, optional\n",
    "        The number of future time steps (days or months) to predict for each patch.\n",
    "    d_model : int, optional\n",
    "        The dimension of the model's hidden states (embedding dimension).\n",
    "        This is the size of the vectors that flow through the Transformer encoder.\n",
    "    nhead : int, optional\n",
    "        The number of attention heads in the multi-head attention mechanism within\n",
    "        each Transformer encoder layer. Defaults to 8.\n",
    "    num_layers : int, optional\n",
    "        The number of Transformer encoder layers in the model. Defaults to 4.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : nn.Linear\n",
    "        Linear layer to project input patch features into the `d_model` hidden space.\n",
    "    encoder : nn.TransformerEncoder\n",
    "        The Transformer encoder module composed of `num_layers` encoder layers.\n",
    "    mlp_head : nn.Sequential\n",
    "        A multi-layer perceptron head for outputting predictions for each patch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_patch_features_dim: int = PATCH_EMBEDDING_INPUT_DIM, # D: The flat feature dimension of a single patch (ex., 512)\n",
    "                 num_patches: int = NUM_PATCHES,  # P: Number of spatial patches\n",
    "                 context_length: int = CONTEXT_LENGTH, # T: Number of historical time steps\n",
    "                 forecast_horizon: int = FORECAST_HORIZON, # Number of future time steps to predict (usually 1)\n",
    "                 d_model: int = D_MODEL,        # d_model: Transformer's embedding dimension\n",
    "                 nhead: int = N_HEAD,           # nhead: Number of attention heads\n",
    "                 num_layers: int = NUM_TRANSFORMER_LAYERS, # num_layers: Number of TransformerEncoderLayers\n",
    "                 dropout: float = 0.1,  \n",
    "                 dim_feedforward: int = PATCH_EMBEDDING_INPUT_DIM, \n",
    "                 activation: str = \"gelu\" \n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The transformer should\n",
    "        1. Accept a sequence of data (ex. 3 months of patches). \n",
    "           The context_length parameter says how many time steps to use for input.\n",
    "        2. Encode each patch with the transformer.\n",
    "        3. Output the patches for regression (ex. predict the 4th month).\n",
    "           The forecast_horizon parameter says how many time steps to use for the output prediction.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.input_patch_features_dim = input_patch_features_dim\n",
    "   \n",
    "        print(\"Calling IceForecastTransformer __init__\")\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Patch embedding layer: projects the raw patch features (512)\n",
    "        # into d_model (128) hidden space dimension\n",
    "        self.patch_embed = nn.Linear(input_patch_features_dim, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output tensors are (batch, sequence, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,               \n",
    "            activation=activation,          \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP head: (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        # Make a prediction for every cell per patch. \n",
    "        # The Sigmoid is CRITICAL. It ensures there are no out of bounds predictions.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, CELLS_PER_PATCH * forecast_horizon),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"End of IceForecastTransformer __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B = Batch size\n",
    "        T = Time (context_length)\n",
    "        P = Patch count\n",
    "        D = Patch Dimension (cells per patch * feature count)\n",
    "        x: Tensor of shape (B, T, P, D)\n",
    "        Output: Tensor of shape (batch_size, forecast_horizon, num_patches)\n",
    "        Output: (B, forecast_horizon, P)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initial input x shape from DataLoader / pre-processing:\n",
    "        # (B, T, P, D) i.e., (Batch_Size, Context_Length, Num_Patches, Input_Patch_Features_Dim)\n",
    "        # Example: (16, 7, 140, 512)\n",
    "        \n",
    "        B, T, P, D = x.shape\n",
    "\n",
    "        # Flatten time and patches for the Transformer Encoder:\n",
    "        # Each (Time, Patch) combination becomes a single token in the sequence.\n",
    "        # Output shape: (B, T * P, D)\n",
    "        # Example: (16, 7 * 140 = 980, 512)\n",
    "        \n",
    "        # Flatten time and patches for the Transformer Encoder: (B, T * P, D)\n",
    "        # This treats each patch at each time step as a distinct token\n",
    "        x = x.view(B, T * P, D)\n",
    "\n",
    "        # Project patch features to the transformer's d_model dimension\n",
    "        x = self.patch_embed(x)  # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        x = self.encoder(x)      # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "\n",
    "        # Reshape back to separate time and patches: (B, T, P, d_model) ex., (16, 7, 140, 128)\n",
    "        x = x.view(B, T, P, self.d_model) \n",
    "\n",
    "        # Mean pooling over the time (context_length) dimension for each patch.\n",
    "        # This aggregates information from all historical time steps for each patch's final prediction.        \n",
    "        x = x.mean(dim=1)  # Output: (B, P, d_model) ex., (16, 140, 128)\n",
    "\n",
    "        # Apply MLP head to predict values for each cell in each patch\n",
    "        # The MLP head outputs (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        x = self.mlp_head(x) # ex. (16, 140, 256 * 3) = (16, 140, 768)\n",
    "\n",
    "        # Reshape the output to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "        # Explicitly reshape the last dimension to seperate the forecast horizon out\n",
    "        x = x.view(B, P, self.forecast_horizon, CELLS_PER_PATCH) # Reshape into forecast_horizon and CELLS_PER_PATCH\n",
    "        x = x.permute(0, 2, 1, 3) # Permute to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200db32-3fbb-4f8f-b841-bbcdc9117c11",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7383efb0-3e8d-468d-bf66-0bb91f943ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED TRAINING\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch import Tensor\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = IceForecastTransformer()\n",
    "    \n",
    "    # Wrap the model with DataParallel if there's more than one GPU available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    print(model)\n",
    "    print(\"--------------------------\\n\")\n",
    "    \n",
    "    logging.info(\"\\n--- Model Architecture ---\")\n",
    "    logging.info(str(model)) # Log the full model structure\n",
    "    logging.info(f\"Total model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    logging.info(\"--------------------------\\n\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    logging.info(\"== TIMER IS STARTING FOR TRAINING ==\")\n",
    "    start_time = time.perf_counter()\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(\"       STARTING EPOCHS       \")\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "    logging.info(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        for batch_idx, (input_tensor, target_tensor, start_idx, end_idx, target_start, target_end) in enumerate(train_loader):  \n",
    "    \n",
    "            # Move input and target to the device\n",
    "            # x: (B, context_length, num_patches, input_patch_features_dim), y: (B, forecast_horizon, num_patches)\n",
    "            x = input_tensor.to(device)  # Shape: (B, T, P, C, L)\n",
    "            y = target_tensor.to(device)  # Shape: (B, forecast_horizon, P, L)\n",
    "    \n",
    "            # Reshape x for transformer input\n",
    "            B, T, P, C, L = x.shape\n",
    "            x_reshaped_for_transformer_D = x.view(B, T, P, C * L)\n",
    "    \n",
    "            # Run through transformer\n",
    "            y_pred = model(x_reshaped_for_transformer_D) # y_pred is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, y) # DIRECTLY compare y_pred and y\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "        # --- Validation loop ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Unpack the full tuple\n",
    "                x_val, y_val, start_idx, end_idx, target_start, target_end = batch\n",
    "        \n",
    "                # Move to GPU if available\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "    \n",
    "                # Extract dimensions from x_val for reshaping\n",
    "                # x_val before reshaping: (B_val, T_val, P_val, C_val, L_val)\n",
    "                B_val, T_val, P_val, C_val, L_val = x_val.shape\n",
    "                \n",
    "                # Reshape x_val for transformer input\n",
    "                x_val_reshaped_for_transformer_input = x_val.view(B_val, T_val, P_val, C_val * L_val)\n",
    "    \n",
    "                # Model output is (B, forecast_horizon, P, L)\n",
    "                y_val_pred = model(x_val_reshaped_for_transformer_input) \n",
    "    \n",
    "                # Compute validation loss (y_val_pred and y_val should have identical shapes)\n",
    "                val_loss += criterion(y_val_pred, y_val).item() # y_val is (B, forecast_horizon, P, L)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    logging.info(\"===============================================\")\n",
    "    logging.info(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    logging.info(\"===============================================\")\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "else:\n",
    "    print(\"SKIPPED TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {},
   "source": [
    "TODO OPTION: Try temporal attention only (ex., Informer, Time Series Transformer).\n",
    "\n",
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ecdd478-4bc1-4a3f-b91d-b40f03db7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED SAVING THE MODEL\n"
     ]
    }
   ],
   "source": [
    "# Define the path where to save or load the model\n",
    "PATH = f\"{model_version}_model.pth\"\n",
    "\n",
    "if TRAINING:\n",
    "    # Save the model's state_dict\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(f\"Saved model at {PATH}\")\n",
    "\n",
    "else:\n",
    "    print(\"SKIPPED SAVING THE MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a5f4e-ea4d-4cb1-88f4-6e800dc92fe4",
   "metadata": {},
   "source": [
    "# === BELOW - CAN BE USED ANY TIME FROM A .PTH FILE\n",
    "\n",
    "Make sure and run the cells that contain constants or run all, but comment out the \"save\" and the training loop cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278114f-96bb-4bbc-8e7f-f87c52e2a946",
   "metadata": {},
   "source": [
    "# Re-Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dc4b5-7cca-4a6d-99fd-6754ca833879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "\n",
    "PATH = \"peNONE_results/\" + PATH\n",
    "print(PATH)\n",
    "\n",
    "# Instantiate the model (must have the same architecture as when it was saved)\n",
    "# Create an identical instance of the original __init__ parameters\n",
    "loaded_model = IceForecastTransformer()\n",
    "\n",
    "# Load the saved state_dict\n",
    "# weights_only=True is good practice for safety\n",
    "state_dict = torch.load(PATH, weights_only=True)\n",
    "\n",
    "# Create an ordered dictionary to store the keys\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "# Loop through the state_dict and remove the \"module.\" prefix because of the multiple GPUs used\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # Remove the \"module.\" prefix\n",
    "        new_state_dict[name] = v\n",
    "    else:\n",
    "        # In case some keys don't have the prefix (though unlikely for a DataParallel model)\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "loaded_model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e362e-fc7b-4692-93aa-96dc68546d91",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630a7cf-9771-4b4d-b266-459305b95876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "if FAST_EVAL_ON:    \n",
    "\n",
    "    # Accumulators for errors\n",
    "    all_abs_errors = [] # To store absolute errors for each cell in each patch\n",
    "    all_mse_errors = [] # To store MSE for each cell in each patch\n",
    "    \n",
    "    # Accumulators for histogram data\n",
    "    all_predicted_values_flat = []\n",
    "    all_actual_values_flat = []\n",
    "    \n",
    "    print(\"\\nStarting evaluation and metric calculation...\")\n",
    "    logging.info(\"\\nStarting evaluation and metric calculation...\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"DEBUG: Context Length: {CONTEXT_LENGTH}\")\n",
    "    print(f\"DEBUG: Forecast Horizon: {FORECAST_HORIZON}\")\n",
    "    print(f\"DEBUG: Number of batches in test_loader (with drop_last=True): {len(test_loader)} Batches\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: len(test_set): {len(test_set)} time steps\")\n",
    "    print(f\"DEBUG: len(dataset) for splitting: {len(dataset)} time steps\")\n",
    "    print(f\"DEBUG: train_end: {train_end}\")\n",
    "    print(f\"DEBUG: val_end: {val_end}\")\n",
    "    print(f\"DEBUG: range for test_set: {range(val_end, total_time_steps)}\") \n",
    "    # Should be range(302, 356) for daily and range(2027, 2058) for monthly\n",
    "    print(\"==================\")\n",
    "\n",
    "    start_time_metrics = time.perf_counter()\n",
    "    \n",
    "    for batch_idx, (sample_x, sample_y, \n",
    "                    start_idx, end_idx, target_start, target_end) in enumerate(test_loader):\n",
    "        print(f\"Processing batch {batch_idx+1}/{len(test_loader)}\")\n",
    "    \n",
    "        # Move to device and apply initial reshape as done in training\n",
    "        sample_x = sample_x.to(device)\n",
    "        sample_y = sample_y.to(device) # Actual target values\n",
    "\n",
    "        # Initial reshape of x for the Transformer model\n",
    "        B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "        sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "            predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "\n",
    "        # Ensure predicted_y_patches and sample_y have the same shape for comparison\n",
    "        # Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "        if predicted_y_patches.shape != sample_y.shape:\n",
    "            print(f\"Shape mismatch: Predicted {predicted_y_patches.shape}, Actual {sample_y.shape}\")\n",
    "            continue # Skip this batch if shapes are incompatible\n",
    "\n",
    "        # Calculate errors for each cell in each patch, across the forecast horizon and batch\n",
    "        # The errors will implicitly be averaged over the batch when we take the mean later\n",
    "        diff = predicted_y_patches - sample_y\n",
    "        abs_error_batch = torch.abs(diff)\n",
    "        mse_error_batch = diff ** 2\n",
    "\n",
    "        # Accumulate errors (move to CPU for storage if memory is a concern)\n",
    "        all_abs_errors.append(abs_error_batch.cpu())\n",
    "        all_mse_errors.append(mse_error_batch.cpu())\n",
    "\n",
    "        # Collect data for histograms (flatten all values)\n",
    "        all_predicted_values_flat.append(predicted_y_patches.cpu().numpy().flatten())\n",
    "        all_actual_values_flat.append(sample_y.cpu().numpy().flatten())\n",
    "\n",
    "    # --- Final Data Concatenation ---\n",
    "    if all_abs_errors and all_mse_errors:\n",
    "        combined_abs_errors = torch.cat(all_abs_errors, dim=0)\n",
    "        combined_mse_errors = torch.cat(all_mse_errors, dim=0)\n",
    "        mean_abs_error_per_cell_patch = combined_abs_errors.mean(dim=(0, 1)).numpy()\n",
    "        mean_mse_per_cell_patch = combined_mse_errors.mean(dim=(0, 1)).numpy()\n",
    "\n",
    "    if all_predicted_values_flat and all_actual_values_flat:\n",
    "        final_predicted_values = np.concatenate(all_predicted_values_flat)\n",
    "        final_actual_values = np.concatenate(all_actual_values_flat)\n",
    "\n",
    "    #######\n",
    "    # SIC #\n",
    "    #######\n",
    "    \n",
    "    print(\"\\n--- Error Metrics (Averaged per Cell per Patch) ---\")\n",
    "    print(f\"Mean Absolute Error (shape {mean_abs_error_per_cell_patch.shape}):\")\n",
    "    # print(mean_abs_error_per_cell_patch) # Uncomment to see the full tensor\n",
    "    print(f\"Overall Mean Absolute Error:            {mean_abs_error_per_cell_patch.mean().item():.4f}\")\n",
    "\n",
    "    print(f\"\\nMean Squared Error (shape {mean_mse_per_cell_patch.shape}):\")\n",
    "    # print(mean_mse_per_cell_patch) # Uncomment to see the full tensor\n",
    "\n",
    "    mse = mean_mse_per_cell_patch.mean().item()\n",
    "    print(f\"Overall Mean Squared Error:             {mse:.4f}\")\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Overall Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "    # --- Save error arrays for later use with Cartopy ---\n",
    "    print(\"\\n--- Saving Error Arrays ---\")\n",
    "    np.save(f\"{model_version}_MAE_per_cell_patch.npy\", mean_abs_error_per_cell_patch)\n",
    "    np.save(f\"{model_version}_MSE_per_cell_patch.npy\", mean_mse_per_cell_patch)\n",
    "    print(f\"Mean ABS Error array saved as {model_version}_MAE_per_cell_patch.npy\")\n",
    "    print(f\"Mean MSE Error array saved as {model_version}_MSE_per_cell_patch.npy\")\n",
    "\n",
    "    #######\n",
    "    # SIE #\n",
    "    #######\n",
    "    \n",
    "    # --- Sea Ice Extent (SIE) Prediction with Sklearn/Seaborn ---\n",
    "    print(\"\\n--- Sea Ice Extent (SIE) Metrics (Threshold > 0.15) ---\")\n",
    "\n",
    "    # Apply the threshold for sea ice extent (SIE)\n",
    "    threshold_cm = 0.15 \n",
    "    sie_actual_flat = np.where(final_actual_values > threshold_cm, 1, 0)\n",
    "    sie_predicted_flat = np.where(final_predicted_values > threshold_cm, 1, 0)\n",
    "\n",
    "    # Calculate and print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(sie_actual_flat, sie_predicted_flat, target_names=['No Ice', 'Ice'], zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # --- Plotting Confusion Matrix with Percentages ---\n",
    "    cm = confusion_matrix(sie_actual_flat, sie_predicted_flat)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    annot_labels = np.array([\n",
    "        [f\"{cm[0, 0]:,}\\n({cm_percent[0, 0]:.2%})\", f\"{cm[0, 1]:,}\\n({cm_percent[0, 1]:.2%})\"],\n",
    "        [f\"{cm[1, 0]:,}\\n({cm_percent[1, 0]:.2%})\", f\"{cm[1, 1]:,}\\n({cm_percent[1, 1]:.2%})\"]\n",
    "    ])\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=annot_labels, fmt='', cmap='Blues', xticklabels=['No Ice', 'Ice'], yticklabels=['No Ice', 'Ice'], ax=ax)\n",
    "    ax.set_title(f'Sea Ice Extent Confusion Matrix (Threshold > {threshold_cm}) for {patching_strategy_abbr}', fontsize=16)\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_version}_SIE_Confusion_Matrix.png\")\n",
    "    plt.close()\n",
    "    print(f\"\\nConfusion matrix plot saved as {model_version}_SIE_Confusion_Matrix.png\")\n",
    "\n",
    "    # --- ROC Curve and AUC Calculation with Sklearn ---\n",
    "    print(\"\\n--- ROC Curve and AUC Metrics ---\")\n",
    "\n",
    "    # Use the continuous predicted values and binary actual values\n",
    "    y_true_binary = np.where(final_actual_values > 0.15, 1, 0)\n",
    "    y_scores = final_predicted_values\n",
    "\n",
    "    # Calculate ROC curve data\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_true_binary, y_scores)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve {patching_strategy_abbr}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_version}_ROC_Curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nArea Under the Curve (AUC): {auc:.4f}\")\n",
    "    print(f\"ROC curve plot saved as {model_version}_ROC_Curve.png\")\n",
    "    # --- End of Sklearn ROC Curve Section ---\n",
    "\n",
    "    print(f\"Elapsed time for metrics: {time.perf_counter() - start_time_metrics} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46292248-0099-4509-acd6-50e7fc60b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118394/620406350.py:143: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=months, y=rmse_values, ax=ax, palette=\"rocket\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly RMSE plot saved as Mfd_nF_D128_B16_lt40_P210_L256_T12_Fh12_e40_ROW_monthly_RMSE.png\n",
      "\n",
      "--- Calculating Temporal Degradation Metrics ---\n",
      "Plot saved: Mfd_nF_D128_B16_lt40_P210_L256_T12_Fh12_e40_ROW_SIC_temporal_degradation.png\n",
      "Elapsed time for full evaluation: 12.085208751959726 seconds\n",
      "Monthly and step RMSE degradation heatmap saved.\n",
      "\n",
      "==================================================\n",
      "       TOP 30 WORST RMSE DATES AND FORECAST STEPS\n",
      "==================================================\n",
      "RMSE       Date            Forecast Step  \n",
      "----------------------------------------\n",
      "0.3687   2023-09         11             \n",
      "0.3684   2023-09         12             \n",
      "0.3679   2023-09         10             \n",
      "0.3637   2020-09         10             \n",
      "0.3631   2020-09         8              \n",
      "0.3626   2020-09         9              \n",
      "0.3625   2020-09         7              \n",
      "0.3623   2020-09         6              \n",
      "0.3621   2020-09         5              \n",
      "0.3619   2020-09         4              \n",
      "0.3613   2020-09         3              \n",
      "0.3612   2020-09         11             \n",
      "0.3600   2020-09         2              \n",
      "0.3592   2020-09         1              \n",
      "0.3588   2020-09         12             \n",
      "0.3542   2021-09         1              \n",
      "0.3530   2021-09         12             \n",
      "0.3526   2021-09         2              \n",
      "0.3505   2021-09         11             \n",
      "0.3503   2021-09         3              \n",
      "0.3501   2021-09         4              \n",
      "0.3501   2021-09         5              \n",
      "0.3499   2021-09         6              \n",
      "0.3493   2021-09         7              \n",
      "0.3492   2021-09         8              \n",
      "0.3489   2021-09         10             \n",
      "0.3487   2021-09         9              \n",
      "0.3342   2022-09         8              \n",
      "0.3342   2022-09         4              \n",
      "0.3339   2022-09         5              \n",
      "==================================================\n",
      "\n",
      "--- Calculating Overall Distribution Metrics (JSD) ---\n",
      "\n",
      "Jensen-Shannon Distance between Actual vs. Predicted SIC for ROW patchify  (Overall Distribution): 0.3836\n",
      "Actual vs. Predicted SIC histogram saved as Mfd_nF_D128_B16_lt40_P210_L256_T12_Fh12_e40_ROW_SIC_xy_Overall.png\n"
     ]
    }
   ],
   "source": [
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "if SLOW_EVAL_ON:\n",
    "\n",
    "    start_full_evaluation = time.perf_counter()\n",
    "    \n",
    "    # Create lists to store errors with month metadata\n",
    "    all_predicted_values = []\n",
    "    all_actual_values = []\n",
    "    all_abs_errors_with_month = []\n",
    "    all_mse_errors_with_month = []\n",
    "    all_predicted_values_with_month = []\n",
    "    all_actual_values_with_month = []\n",
    "    mse_per_step = {step: [] for step in range(1, FORECAST_HORIZON + 1)}\n",
    "    monthly_step_mse_data = {}\n",
    "    worst_rmse_list = [] # Format: (RMSE_value, 'YYYY-MM-DD', forecast_step)\n",
    "    \n",
    "    for batch_idx, (sample_x, sample_y, start_idx, end_idx, target_start, target_end) in enumerate(test_loader):\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{len(test_loader)}\")\n",
    "    \n",
    "        # Move to device and apply initial reshape as done in training\n",
    "        sample_x = sample_x.to(device)\n",
    "        sample_y = sample_y.to(device) # Actual target values\n",
    "\n",
    "        # Initial reshape of x for the Transformer model\n",
    "        B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "        sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "            predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "\n",
    "        # Ensure predicted_y_patches and sample_y have the same shape for comparison\n",
    "        # Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "        if predicted_y_patches.shape != sample_y.shape:\n",
    "            print(f\"Shape mismatch: Predicted {predicted_y_patches.shape}, Actual {sample_y.shape}\")\n",
    "            continue # Skip this batch if shapes are incompatible\n",
    "\n",
    "        # Calculate errors for each cell in each patch, across the forecast horizon and batch\n",
    "        # The errors will implicitly be averaged over the batch when we take the mean later\n",
    "        diff = predicted_y_patches - sample_y\n",
    "        abs_error_batch = torch.abs(diff)\n",
    "        mse_error_batch = diff ** 2\n",
    "\n",
    "        # Loop through each item in the batch\n",
    "        for i in range(B_sample):\n",
    "            \n",
    "            # Get the start date for this specific sample\n",
    "            start_time_np = dataset.times[start_idx[i]]\n",
    "        \n",
    "            # Cast start_time_np to Month precision to allow addition with np.timedelta64(step, 'M')\n",
    "            start_time_month_np = start_time_np.astype('datetime64[M]') \n",
    "        \n",
    "            start_year  = start_time_np.astype(\"datetime64[Y]\").astype(int) + 1970\n",
    "            start_month = start_time_np.astype(\"datetime64[M]\").astype(int) % 12 + 1\n",
    "            start_day   = (start_time_np.astype(\"datetime64[D]\") - start_time_np.astype(\"datetime64[M]\")).astype(int) + 1\n",
    "            \n",
    "\n",
    "            # Loop through the forecast horizon to get the month for each forecast step\n",
    "            for step in range(FORECAST_HORIZON):\n",
    "\n",
    "                # The forecast date is the start date plus the forecast step (in months)\n",
    "                # Use the month-precision date object for addition\n",
    "                forecast_date_np = start_time_month_np + np.timedelta64(step, 'M') \n",
    "                forecast_date_str = str(forecast_date_np).split('T')[0] # Get 'YYYY-MM-DD'\n",
    "                \n",
    "                # NOTE: start_month is a NumPy scalar, ensure it's converted to a Python int if used outside NumPy math\n",
    "                forecast_month = (start_month.item() + step - 1) % 12 + 1\n",
    "                forecast_step = step + 1\n",
    "                \n",
    "                # Select the squared errors for this specific sample and forecast step\n",
    "                mse_error_step = mse_error_batch[i, step, :, :].cpu().numpy()\n",
    "                \n",
    "                # --- Calculate RMSE for this single prediction ---\n",
    "                # RMSE for this sample/step is the sqrt of the mean of all cell-wise squared errors\n",
    "                current_rmse = np.sqrt(np.mean(mse_error_step))\n",
    "                \n",
    "                # --- Store the RMSE and metadata in the list ---\n",
    "                worst_rmse_list.append({\n",
    "                    'rmse': current_rmse,\n",
    "                    'date': forecast_date_str,\n",
    "                    'step': forecast_step\n",
    "                })\n",
    "\n",
    "                # Select the data for this specific sample and forecast step\n",
    "                predicted_step = predicted_y_patches[i, step, :, :].cpu().numpy()\n",
    "                actual_step = sample_y[i, step, :, :].cpu().numpy()\n",
    "                # abs_error_step = abs_error_batch[i, step, :, :].cpu().numpy()\n",
    "\n",
    "                # Collect flattened arrays for overall JSD calculation\n",
    "                all_predicted_values.append(predicted_step.flatten())\n",
    "                all_actual_values.append(actual_step.flatten())\n",
    "\n",
    "                # Create a unique key for the dictionary\n",
    "                data_key = (forecast_month, forecast_step)\n",
    "    \n",
    "                # Initialize a list if the key doesn't exist\n",
    "                if data_key not in monthly_step_mse_data:\n",
    "                    monthly_step_mse_data[data_key] = []\n",
    "\n",
    "                # Append the flattened errors\n",
    "                monthly_step_mse_data[data_key].extend(mse_error_step.flatten())\n",
    "\n",
    "                # Append the flattened errors to the list for the current forecast step\n",
    "                # Note: The forecast step is `step + 1` since `step` starts from 0.\n",
    "                mse_per_step[step + 1].extend(mse_error_step.flatten())\n",
    "                \n",
    "                # # Flatten the data for this step and store it with the month\n",
    "                # all_abs_errors_with_month.append({\n",
    "                #     'month': forecast_month,\n",
    "                #     'errors': abs_error_step.flatten()\n",
    "                # })\n",
    "                # all_predicted_values_with_month.append({\n",
    "                #     'month': forecast_month,\n",
    "                #     'values': predicted_step.flatten()\n",
    "                # })\n",
    "                # all_actual_values_with_month.append({\n",
    "                #     'month': forecast_month,\n",
    "                #     'values': actual_step.flatten()\n",
    "                # })\n",
    "    \n",
    "                # Flatten the squared errors for this step and store it with the month\n",
    "                all_mse_errors_with_month.append({\n",
    "                    'month': forecast_month,\n",
    "                    'errors': mse_error_step.flatten()\n",
    "                })\n",
    "\n",
    "    ###########\n",
    "    #   SIC   #\n",
    "    ###########\n",
    "\n",
    "    # --- Group errors by month ---\n",
    "    monthly_mse = {month: [] for month in range(1, 13)}\n",
    "    for item in all_mse_errors_with_month:\n",
    "        monthly_mse[item['month']].extend(item['errors'])\n",
    "    \n",
    "    # --- Calculate and Plot Monthly Degradation (RMSE) ---\n",
    "    monthly_rmse = {month: np.sqrt(np.mean(errors)) for month, errors in monthly_mse.items() if errors}\n",
    "    months = sorted(monthly_rmse.keys())\n",
    "    rmse_values = [monthly_rmse[m] for m in months]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(x=months, y=rmse_values, ax=ax, palette=\"rocket\")\n",
    "    ax.set_title(f\"{model_version} \\n Root Mean Squared Error by Forecasted Month\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_version}_monthly_RMSE.png\")\n",
    "    plt.close()\n",
    "    print(f\"Monthly RMSE plot saved as {model_version}_monthly_RMSE.png\")\n",
    "    \n",
    "    ## Temporal Degradation Plot (RMSE vs. Forecast Horizon)\n",
    "    \n",
    "    print(\"\\n--- Calculating Temporal Degradation Metrics ---\")\n",
    "    \n",
    "    # Calculate the RMSE for each forecast step by taking the mean of all squared errors\n",
    "    rmse_per_step = []\n",
    "    forecast_steps = sorted(mse_per_step.keys())\n",
    "    \n",
    "    for step in forecast_steps:\n",
    "        if mse_per_step[step]:\n",
    "            avg_mse = np.mean(mse_per_step[step])\n",
    "            rmse_per_step.append(np.sqrt(avg_mse))\n",
    "        else:\n",
    "            rmse_per_step.append(np.nan) # Handle cases with no data\n",
    "\n",
    "    # --- Save the plot data ---\n",
    "    plot_data = {\n",
    "        'forecast_steps': forecast_steps,\n",
    "        'rmse_per_step': rmse_per_step\n",
    "    }\n",
    "    np.save(f'{model_version}_temporal_degradation_data.npy', plot_data)\n",
    "    \n",
    "    # --- Plot Degradation Curve (RMSE vs. Forecast Step) ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.set_xlabel('Forecast Horizon (Months)')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.plot(forecast_steps, rmse_per_step, color='tab:blue', marker='o', linestyle='-', label='RMSE')\n",
    "    ax.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax.set_xticks(forecast_steps)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f'{model_version} \\n SIC Temporal Degradation Per Month')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_version}_SIC_temporal_degradation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Plot saved: {model_version}_SIC_temporal_degradation.png\")\n",
    "    print(f\"Elapsed time for full evaluation: {time.perf_counter() - start_full_evaluation} seconds\")\n",
    "\n",
    "    # --- Calculate average RMSE for each month and forecast step ---\n",
    "    avg_monthly_rmse = {}\n",
    "    for key, mse_values in monthly_step_mse_data.items():\n",
    "        if mse_values:\n",
    "            avg_mse = np.mean(mse_values)\n",
    "            avg_monthly_rmse[key] = np.sqrt(avg_mse)\n",
    "    \n",
    "    # --- Prepare data for heatmap ---\n",
    "    all_months = sorted(list(set(k[0] for k in avg_monthly_rmse.keys())))\n",
    "    all_forecast_steps = sorted(list(set(k[1] for k in avg_monthly_rmse.keys())))\n",
    "    \n",
    "    # Create a 2D array for the heatmap\n",
    "    heatmap_data = np.zeros((len(all_months), len(all_forecast_steps)))\n",
    "    \n",
    "    for (month, step), rmse in avg_monthly_rmse.items():\n",
    "        row_idx = all_months.index(month)\n",
    "        col_idx = all_forecast_steps.index(step)\n",
    "        heatmap_data[row_idx, col_idx] = rmse\n",
    "    \n",
    "    # --- Plot the heatmap with the color bar/legend ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # The cbar_kws argument adds the color bar to the side of the plot and sets its label.\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        cmap='viridis', # Choose a suitable colormap\n",
    "        ax=ax, \n",
    "        xticklabels=all_forecast_steps, \n",
    "        yticklabels=all_months,\n",
    "        cbar_kws={'label': 'Root Mean Squared Error (RMSE)'} # The key addition for the label\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"RMSE Degradation by Month and Forecast Horizon\")\n",
    "    ax.set_xlabel(\"Forecast Horizon (Months)\")\n",
    "    ax.set_ylabel(\"Month\")\n",
    "    \n",
    "    # Set the y-axis labels to use month names for better readability\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    ax.set_yticklabels([month_names[m-1] for m in all_months], rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_version}_heatmap_RMSE_degradation.png\")\n",
    "    plt.close()\n",
    "    print(f\"Monthly and step RMSE degradation heatmap saved.\")\n",
    "\n",
    "    # WORST RSME SCORES\n",
    "    sorted_worst_rmse = sorted(worst_rmse_list, key=lambda x: x['rmse'], reverse=True)\n",
    "    \n",
    "    # Select the top N worst dates\n",
    "    TOP_N = 30\n",
    "    top_worst_dates = sorted_worst_rmse[:TOP_N]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"       TOP {TOP_N} WORST RMSE DATES AND FORECAST STEPS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'RMSE':<10} {'Date':<15} {'Forecast Step':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    for entry in top_worst_dates:\n",
    "        print(f\"{entry['rmse']:.4f}   {entry['date']:<15} {entry['step']:<15}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- JENSEN-SHANNON DIFFERENCE (JSD) CALCULATION ---\n",
    "    \n",
    "    print(\"\\n--- Calculating Overall Distribution Metrics (JSD) ---\")\n",
    "    \n",
    "    # Concatenate all predicted and actual arrays into two massive 1D arrays\n",
    "    final_predicted_flat = np.concatenate(all_predicted_values)\n",
    "    final_actual_flat = np.concatenate(all_actual_values)\n",
    "    \n",
    "    plot_actual_vs_predicted_sic_distribution(\n",
    "        predicted_flat=final_predicted_flat,\n",
    "        actual_flat=final_actual_flat,\n",
    "        model_version=model_version,\n",
    "        patching_strategy_abbr=patching_strategy_abbr,\n",
    "        title_suffix=\" (Overall Distribution)\"\n",
    "    )\n",
    "\n",
    "    ###########\n",
    "    #   SIE   #\n",
    "    ###########\n",
    "\n",
    "    # # --- Group SIE data by month and plot ---\n",
    "    # monthly_sie_data = {month: {'predicted': [], 'actual': []} for month in range(1, 13)}\n",
    "    # for i in range(len(all_predicted_values_with_month)):\n",
    "    #     month = all_predicted_values_with_month[i]['month']\n",
    "    #     monthly_sie_data[month]['predicted'].extend(all_predicted_values_with_month[i]['values'])\n",
    "    #     monthly_sie_data[month]['actual'].extend(all_actual_values_with_month[i]['values'])\n",
    "    \n",
    "    # # --- Loop through months to generate per-month SIE metrics ---\n",
    "    # for month, data in monthly_sie_data.items():\n",
    "    #     if not data['actual']:\n",
    "    #         continue\n",
    "        \n",
    "    #     # Convert to NumPy arrays for calculation\n",
    "    #     actual_values = np.array(data['actual'])\n",
    "    #     predicted_values = np.array(data['predicted'])\n",
    "    \n",
    "    #     # Apply threshold for SIE\n",
    "    #     sie_actual = (actual_values > 0.15).astype(int)\n",
    "    #     sie_predicted = (predicted_values > 0.15).astype(int)\n",
    "    \n",
    "    #     # Print Classification Report\n",
    "    #     print(f\"\\n--- SIE Classification Report for Month {month} ---\")\n",
    "    #     print(classification_report(sie_actual, sie_predicted, target_names=['No Ice', 'Ice'], zero_division=0))\n",
    "    \n",
    "    #     # Plot Confusion Matrix\n",
    "    #     cm = confusion_matrix(sie_actual, sie_predicted)\n",
    "    #     plt.figure(figsize=(8, 6))\n",
    "    #     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Ice', 'Ice'], yticklabels=['No Ice', 'Ice'])\n",
    "    #     plt.title(f'Confusion Matrix - Month {month}')\n",
    "    #     plt.xlabel('Predicted Label')\n",
    "    #     plt.ylabel('True Label')\n",
    "    #     plt.savefig(f\"{model_version}_cmatrix_{month}.png\")\n",
    "    #     plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7955-7de9-4ac1-ae71-ef1f7bedd950",
   "metadata": {},
   "source": [
    "# Make a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e76ed2c4-bee4-4fd0-81b6-79a3e79ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAP_WITH_CARTOPY_ON:\n",
    "    \n",
    "    # Load one batch\n",
    "    data_iter = iter(test_loader)\n",
    "    sample_x, sample_y, start_idx, end_idx, target_start, target_end = next(data_iter)\n",
    "    \n",
    "    print(f\"Shape of sample_x {sample_x.shape}\")\n",
    "    print(f\"Shape of sample_y {sample_y.shape}\")   \n",
    "    \n",
    "    print(f\"Fetched sample_x start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "    print(f\"Fetched sample_x end   index {end_idx}:   Time={dataset.times[end_idx]}\")\n",
    "    \n",
    "    print(f\"Fetched sample_y (target) start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "    print(f\"Fetched sample_y (target) end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "    \n",
    "    # Move to device and apply initial reshape as done in training\n",
    "    sample_x = sample_x.to(device)\n",
    "    sample_y = sample_y.to(device) # Keep sample_y for actual comparison\n",
    "    \n",
    "    # Initial reshape of x for the Transformer model\n",
    "    B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "    sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "    \n",
    "    print(f\"Sample x for inference shape (reshaped): {sample_x_reshaped.shape}\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "        predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "    \n",
    "    print(f\"Predicted y patches shape: {predicted_y_patches.shape}\")\n",
    "    print(f\"Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH) ex., (16, {loaded_model.forecast_horizon}, 140, 256)\")\n",
    "                     \n",
    "    # Option 1: Select a specific day from the forecast horizon (ex., the first day)\n",
    "    # This is the shape (B, NUM_PATCHES, CELLS_PER_PATCH) for that specific day.\n",
    "    predicted_for_day_0 = predicted_y_patches[:, 0, :, :].cpu()\n",
    "    print(f\"Predicted ice area for Day 0 (specific day) shape: {predicted_for_day_0.shape}\")\n",
    "    \n",
    "    # Ensure sample_y has the same structure\n",
    "    actual_for_day_0 = sample_y[:, 0, :, :].cpu()\n",
    "    print(f\"Actual ice area for Day 0 (specific day) shape: {actual_for_day_0.shape}\")\n",
    "    \n",
    "    # Save predictions so that I can use cartopy by switching kernels for the next jupyter cell\n",
    "    np.save(f'patches/ice_area_patches_predicted_{PATH}_day0.npy', predicted_for_day_0)\n",
    "    np.save(f'patches/ice_area_patches_actual_{PATH}_day0.npy', actual_for_day_0)\n",
    "\n",
    "    # Option 2: Iterate through all forecast days\n",
    "    all_predicted_ice_areas = []\n",
    "    all_actual_ice_areas = []\n",
    "    \n",
    "    for time_step_idx in range(loaded_model.forecast_horizon):\n",
    "        predicted_day = predicted_y_patches[:, time_step_idx, :, :].cpu()\n",
    "        all_predicted_ice_areas.append(predicted_day)\n",
    "    \n",
    "        actual_day = sample_y[:, time_step_idx, :, :].cpu()\n",
    "        all_actual_ice_areas.append(actual_day)\n",
    "    \n",
    "        print(f\"Processing forecast day {time_step_idx}: Predicted shape {predicted_day.shape}, Actual shape {actual_day.shape}\")\n",
    "    \n",
    "        # Save each day's prediction/actual data if needed\n",
    "        # np.save(f'patches/ice_area_patches_predicted_day{time_step_idx}.npy', predicted_day)\n",
    "        # np.save(f'patches/ice_area_patches_actual_day{time_step_idx}.npy', actual_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Recover nCells from Patches for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAP_WITH_CARTOPY_ON:\n",
    "\n",
    "    ########################################\n",
    "    # SWAP KERNELS IN THE JUPYTER NOTEBOOK #\n",
    "    ########################################\n",
    "    \n",
    "    from MAP_ANIMATION_GENERATION.map_gen_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.nc_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.patchify_utils import *\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    predicted_ice_area_patches = np.load(f'patches/SIC_predicted_{model_version}_day0.npy')\n",
    "    actual_y_ice_area_patches = np.load(f'patches/SIC_actual_{model_version}_day0.npy')\n",
    "    \n",
    "    NUM_PATCHES = len(predicted_ice_area_patches[0])\n",
    "    print(\"NUM_PATCHES is\", NUM_PATCHES)\n",
    "    \n",
    "    latCell, lonCell = load_mesh(perlmutterpathMesh)\n",
    "    TOTAL_GRID_CELLS = len(lonCell) \n",
    "    cell_mask = latCell >= LATITUDE_THRESHOLD\n",
    "    \n",
    "    # Extract Freeboard (index 0) and Ice Area (index 1) for predicted and actual\n",
    "    # Predicted output is (B, 1, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "    # The model predicts ice area, which is the second feature (index 1)\n",
    "    # if the output of the model aligns with the order of features *within* the original patch_dim.\n",
    "    \n",
    "    # Load the original patch-to-cell mapping\n",
    "    # indices_per_patch_id = [\n",
    "    #     [idx_cell_0_0, ..., idx_cell_0_255],\n",
    "    #     [idx_cell_1_0, ..., idx_cell_1_255],\n",
    "    #     ...\n",
    "    # ]\n",
    "    \n",
    "    full_nCells_patch_ids, indices_per_patch_id, patch_latlons = patchify_by_latlon_spillover(\n",
    "                latCell, lonCell, k=256, max_patches=NUM_PATCHES, LATITUDE_THRESHOLD=LATITUDE_THRESHOLD)\n",
    "    \n",
    "    # Select one sample from the batch for visualization (ex., the first one)\n",
    "    # Output is (NUM_PATCHES, CELLS_PER_PATCH) for this single sample\n",
    "    sample_predicted_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    sample_actual_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    \n",
    "    # Initialize empty arrays for the full grid (nCells)\n",
    "    recovered_predicted_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    recovered_actual_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    \n",
    "    # Populate the full grid using the patch data and mapping\n",
    "    for patch_idx in range(NUM_PATCHES):\n",
    "        cell_indices_in_patch = indices_per_patch_id[patch_idx]\n",
    "        \n",
    "        # For predicted values\n",
    "        recovered_predicted_grid[cell_indices_in_patch] = sample_predicted_cells_per_patch[patch_idx]\n",
    "        nan_mask = np.isnan(recovered_predicted_grid)\n",
    "        nan_count = np.sum(nan_mask)\n",
    "    \n",
    "        # For actual values\n",
    "        recovered_actual_grid[cell_indices_in_patch] = sample_actual_cells_per_patch[patch_idx]\n",
    "    \n",
    "    print(f\"Recovered predicted grid shape: {recovered_predicted_grid.shape}\")\n",
    "    print(f\"Recovered actual grid shape: {recovered_actual_grid.shape}\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_predicted_grid, f\"model {model_version} ice area recovered\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_actual_grid, f\"model {model_version} ice area actual\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIC SIE Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
