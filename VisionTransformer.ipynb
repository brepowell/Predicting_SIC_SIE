{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0706a6-e559-4b22-9584-7e0911854542",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Constants / Defaults\n",
    "\n",
    "Check over these CAREFULLY!\n",
    "\n",
    "Note that if you use the login node for training (even for the trial dataset that is much smaller), you run the risk of getting the error: # OutOfMemoryError: CUDA out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.1\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166144bc-da7c-4a65-b45b-3c666d2be4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nCells:        465044\n",
      "Mask size:           53973\n",
      "cells_per_patch:     256\n",
      "n_patches:           210\n"
     ]
    }
   ],
   "source": [
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "# --- Space Constants:\n",
    "NUM_FEATURES = 2              # C: Number of features per cell (ex., Freeboard, Ice Area)\n",
    "LATITUDE_THRESHOLD = 40       # Determines number of cells and patches\n",
    "\n",
    "# Load the mesh and data to plot.\n",
    "mesh = xr.open_dataset(mesh_dir)\n",
    "latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "mesh.close()\n",
    "print(\"Total nCells:       \", len(latCell))\n",
    "\n",
    "mask = latCell >= LATITUDE_THRESHOLD\n",
    "masked_ncells_size = np.count_nonzero(mask)\n",
    "print(\"Mask size:          \", masked_ncells_size)\n",
    "\n",
    "CELLS_PER_PATCH = 256                                  # L: Number of cells within each patch\n",
    "NUM_PATCHES = masked_ncells_size // CELLS_PER_PATCH    # P: Number of spatial patches\n",
    "\n",
    "print(\"cells_per_patch:    \", CELLS_PER_PATCH)\n",
    "print(\"n_patches:          \", NUM_PATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd6b153-3646-4b50-b1e7-2807eac982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Time Constants:\n",
    "CONTEXT_LENGTH = 7            # T: Number of historical time steps used for input\n",
    "FORECAST_HORIZON = 3          # Number of future time steps to predict (ex. 1 day for next time step)\n",
    "\n",
    "# Model Constants\n",
    "D_MODEL = 128                 # d_model: Dimension of the transformer's internal representations (embedding dimension)\n",
    "N_HEAD = 8                    # nhead: Number of attention heads\n",
    "NUM_TRANSFORMER_LAYERS = 4    # num_layers: Number of TransformerEncoderLayers\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# The input dimension for the patch embedding linear layer.\n",
    "# Each patch at a given time step has NUM_FEATURES * CELLS_PER_PATCH features.\n",
    "# This is the 'D' dimension used in the Transformer's input tensor (B, T, P, D).\n",
    "PATCH_EMBEDDING_INPUT_DIM = NUM_FEATURES * CELLS_PER_PATCH # 2 * 256 = 512\n",
    "\n",
    "# Performance-related\n",
    "NUM_WORKERS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a1e686-766a-49bd-b27c-d851e180a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd_nT_D128_B16_lt40_P210_L256_T7_Fh3_e10\n"
     ]
    }
   ],
   "source": [
    "TRIAL_RUN =              False # TODO - SET THIS TO USE THE PRACTICE SET (MUCH FASTER AND SMALLER)\n",
    "TRAINING =               True  # TODO - SET THIS TO RUN THE TRAINING LOOP\n",
    "PLOT_DATA_DISTRIBUTION = True  # TODO - SET THIS TO PLOT THE OUTLIERS (use on the full dataset)\n",
    "NORMALIZE_ON =           True  # TODO - SET THIS TO USE NORMALIZATION ON FREEBOARD\n",
    "EVALUATING_ON =          True  # TODO - SET THIS TO RUN THE METRICS AT THE BOTTOM\n",
    "\n",
    "if TRIAL_RUN:\n",
    "    model_mode = \"tr\" # Training Dataset\n",
    "else:\n",
    "    model_mode = \"fd\" # Full Dataset\n",
    "\n",
    "if NORMALIZE_ON:\n",
    "    norm = \"nT\"\n",
    "else:\n",
    "    norm = \"nF\"\n",
    "\n",
    "# Model nome convention - fd:full data, etc.\n",
    "model_version = f\"{model_mode}_{norm}_D{D_MODEL}_B{BATCH_SIZE}_lt{LATITUDE_THRESHOLD}_P{NUM_PATCHES}_L{CELLS_PER_PATCH}_T{CONTEXT_LENGTH}_Fh{FORECAST_HORIZON}_e{NUM_EPOCHS}\"\n",
    "print(model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11cf9a-a867-4740-9d75-36f580e8d386",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "\n",
    "TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "\n",
    "TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "\n",
    "TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "Too few workers: GPUs might become idle waiting for data.\n",
    "Too many workers: Can lead to increased CPU memory usage and context switching overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# More Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d15e35-d5b4-41e2-956d-95b6504edcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "if TRAINING and not torch.cuda.is_available():\n",
    "    raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "else:\n",
    "    print(torch.cuda.device_count()) # check the number of available CUDA devices\n",
    "    # will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor Name: x86_64\n",
      "Physical Cores: 64\n",
      "Logical Cores: 128\n",
      "Current CPU Frequency: 2498.48 MHz\n",
      "Min CPU Frequency: 1500.00 MHz\n",
      "Max CPU Frequency: 2450.00 MHz\n",
      "Total CPU Usage: 0.3%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd50d6-18a5-4f14-94c6-56d69814e0e6",
   "metadata": {},
   "source": [
    "# Example of one netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")\n",
    "\n",
    "from perlmutterpath import * # has the path to the data on Perlmutter\n",
    "ds = xr.open_dataset(full_data_dir_sample, decode_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    timeDaily_counter             (Time) int32 124B ...\n",
       "    xtime_startDaily              (Time) |S64 2kB ...\n",
       "    xtime_endDaily                (Time) |S64 2kB ...\n",
       "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
       "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"].shape[0]\n",
    "print(day_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'xtime_startDaily' (Time: 31)> Size: 2kB\n",
      "[31 values with dtype=|S64]\n",
      "Dimensions without coordinates: Time\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'2024-12-01_00:00:00' b'2024-12-02_00:00:00' b'2024-12-03_00:00:00'\n",
      " b'2024-12-04_00:00:00' b'2024-12-05_00:00:00' b'2024-12-06_00:00:00'\n",
      " b'2024-12-07_00:00:00' b'2024-12-08_00:00:00' b'2024-12-09_00:00:00'\n",
      " b'2024-12-10_00:00:00' b'2024-12-11_00:00:00' b'2024-12-12_00:00:00'\n",
      " b'2024-12-13_00:00:00' b'2024-12-14_00:00:00' b'2024-12-15_00:00:00'\n",
      " b'2024-12-16_00:00:00' b'2024-12-17_00:00:00' b'2024-12-18_00:00:00'\n",
      " b'2024-12-19_00:00:00' b'2024-12-20_00:00:00' b'2024-12-21_00:00:00'\n",
      " b'2024-12-22_00:00:00' b'2024-12-23_00:00:00' b'2024-12-24_00:00:00'\n",
      " b'2024-12-25_00:00:00' b'2024-12-26_00:00:00' b'2024-12-27_00:00:00'\n",
      " b'2024-12-28_00:00:00' b'2024-12-29_00:00:00' b'2024-12-30_00:00:00'\n",
      " b'2024-12-31_00:00:00']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 465044)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9d6581-da44-4d6f-9582-908d0a86581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(31, 465044), dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'Time': 31, 'nCells': 465044, 'nVertices': 942873})\n"
     ]
    }
   ],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 407MB\n",
      "Dimensions:                       (Time: 31, nCells: 465044, nVertices: 942873)\n",
      "Dimensions without coordinates: Time, nCells, nVertices\n",
      "Data variables:\n",
      "    timeDaily_counter             (Time) int32 124B ...\n",
      "    xtime_startDaily              (Time) |S64 2kB b'2024-12-01_00:00:00' ... ...\n",
      "    xtime_endDaily                (Time) |S64 2kB ...\n",
      "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB 0.0 0.0 ... 0.0\n",
      "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "Attributes: (12/490)\n",
      "    case:                                                         v3.LR.histo...\n",
      "    source_id:                                                    399d430113\n",
      "    realm:                                                        seaIce\n",
      "    product:                                                      model-output\n",
      "    title:                                                        MPAS-Seaice...\n",
      "    source:                                                       E3SM Sea Ic...\n",
      "    ...                                                           ...\n",
      "    config_AM_timeSeriesStatsCustom_reference_times:              initial_time\n",
      "    config_AM_timeSeriesStatsCustom_duration_intervals:           repeat_inte...\n",
      "    config_AM_timeSeriesStatsCustom_repeat_intervals:             reset_interval\n",
      "    config_AM_timeSeriesStatsCustom_reset_intervals:              00-00-07_00...\n",
      "    config_AM_timeSeriesStatsCustom_backward_output_offset:       00-00-01_00...\n",
      "    file_id:                                                      w89d6aw0xo\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e89844-e98f-4b42-b3cd-e3593ea3f151",
   "metadata": {},
   "source": [
    "# Example of Mesh File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a8b6f4-6969-45c9-8859-709234040954",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(\"NC_FILE_PROCESSING/mpassi.IcoswISC30E3r5.20231120.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80b4df70-fa5a-4c94-8be1-f93caae8209c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
       "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
       "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
       "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
       "    angleEdge          (nEdges) float64 11MB ...\n",
       "    dcEdge             (nEdges) float64 11MB ...\n",
       "    dvEdge             (nEdges) float64 11MB ...\n",
       "    indexToEdgeID      (nEdges) int32 6MB ...\n",
       "    latEdge            (nEdges) float64 11MB ...\n",
       "    lonEdge            (nEdges) float64 11MB ...\n",
       "    nEdgesOnEdge       (nEdges) int32 6MB ...\n",
       "    xEdge              (nEdges) float64 11MB ...\n",
       "    yEdge              (nEdges) float64 11MB ...\n",
       "    zEdge              (nEdges) float64 11MB ...\n",
       "    fEdge              (nEdges) float64 11MB ...\n",
       "    cellsOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    edgesOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    kiteAreasOnVertex  (nVertices, vertexDegree) float64 23MB ...\n",
       "    areaTriangle       (nVertices) float64 8MB ...\n",
       "    indexToVertexID    (nVertices) int32 4MB ...\n",
       "    latVertex          (nVertices) float64 8MB ...\n",
       "    lonVertex          (nVertices) float64 8MB ...\n",
       "    xVertex            (nVertices) float64 8MB ...\n",
       "    yVertex            (nVertices) float64 8MB ...\n",
       "    zVertex            (nVertices) float64 8MB ...\n",
       "    fVertex            (nVertices) float64 8MB ...\n",
       "    cellsOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    edgesOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    verticesOnCell     (nCells, maxEdges) int32 11MB ...\n",
       "    areaCell           (nCells) float64 4MB ...\n",
       "    indexToCellID      (nCells) int32 2MB ...\n",
       "    latCell            (nCells) float64 4MB ...\n",
       "    lonCell            (nCells) float64 4MB ...\n",
       "    meshDensity        (nCells) float64 4MB ...\n",
       "    nEdgesOnCell       (nCells) int32 2MB ...\n",
       "    xCell              (nCells) float64 4MB ...\n",
       "    yCell              (nCells) float64 4MB ...\n",
       "    zCell              (nCells) float64 4MB ...\n",
       "    fCell              (nCells) float64 4MB ...\n",
       "    landIceMask        (Time, nCells) int32 2MB ..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40476985-dfb0-4da2-b48e-eb2c97f74f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     5      4      0      0      0      0]\n",
      " [    12     11      9      8      0      3]\n",
      " [     4     13     12      2      0      0]\n",
      " ...\n",
      " [465043      0 465040 465041      0      0]\n",
      " [     0 465042      0 465044      0      0]\n",
      " [     0      0      0      0 465043      0]]\n"
     ]
    }
   ],
   "source": [
    "cellsOnCell = mesh[\"cellsOnCell\"].values\n",
    "print(mesh[\"cellsOnCell\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "652cc980-b665-49dd-bd32-0a4640eece63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(cellsOnCell.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af6cbd2b-ae4f-41d4-b8c1-213fb92b3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465044\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mesh[\"cellsOnCell\"].max().values)\n",
    "print(mesh[\"cellsOnCell\"].min().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21202b02-d87e-4354-aea7-586babbfb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('cellsOnCell.npy', cellsOnCell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022d9a2a-123e-4278-a583-0e87a5d3f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#landIceMask = mesh[\"landIceMask\"].values\n",
    "#np.save('landIceMask.npy', landIceMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e0aab3b-733a-4f9f-ad63-e0a510b05cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'nEdges': 1408196, 'maxEdges2': 12, 'TWO': 2, 'nVertices': 942873, 'vertexDegree': 3, 'nCells': 465044, 'maxEdges': 6, 'Time': 1})\n"
     ]
    }
   ],
   "source": [
    "print(mesh.coords)\n",
    "print(mesh.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a816fde5-ec19-4430-9875-f4e8cadc10d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 509MB\n",
      "Dimensions:            (nEdges: 1408196, maxEdges2: 12, TWO: 2,\n",
      "                        nVertices: 942873, vertexDegree: 3, nCells: 465044,\n",
      "                        maxEdges: 6, Time: 1)\n",
      "Dimensions without coordinates: nEdges, maxEdges2, TWO, nVertices,\n",
      "                                vertexDegree, nCells, maxEdges, Time\n",
      "Data variables: (12/40)\n",
      "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
      "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
      "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
      "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
      "    angleEdge          (nEdges) float64 11MB ...\n",
      "    dcEdge             (nEdges) float64 11MB ...\n",
      "    ...                 ...\n",
      "    nEdgesOnCell       (nCells) int32 2MB ...\n",
      "    xCell              (nCells) float64 4MB ...\n",
      "    yCell              (nCells) float64 4MB ...\n",
      "    zCell              (nCells) float64 4MB ...\n",
      "    fCell              (nCells) float64 4MB ...\n",
      "    landIceMask        (Time, nCells) int32 2MB ...\n",
      "Attributes: (12/1313)\n",
      "    model_name:                                                      mpas\n",
      "    core_name:                                                       ocean\n",
      "    source:                                                          MPAS\n",
      "    Conventions:                                                     MPAS\n",
      "    git_version:                                                     v2.1.0-1...\n",
      "    on_a_sphere:                                                     YES\n",
      "    ...                                                              ...\n",
      "    MPAS_Mesh_NCO_Version:                                           5.1.9\n",
      "    MPAS_Mesh_ESMF_Version:                                          8.4.2\n",
      "    MPAS_Mesh_geometric_features_Version:                            1.3.0\n",
      "    MPAS_Mesh_Metis_Version:                                         5.1.1\n",
      "    MPAS_Mesh_pyremap_Version:                                       1.2.0\n",
      "    NCO:                                                             netCDF O...\n"
     ]
    }
   ],
   "source": [
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fff3ae1-5dc1-4ac7-af6d-eaa97bee17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Pre-processing + Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_AREA = 1e-6\n",
    "\n",
    "def compute_freeboard(area: np.ndarray, \n",
    "                      ice_volume: np.ndarray, \n",
    "                      snow_volume: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow volume and area.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    area : np.ndarray\n",
    "        Sea ice concentration / area (same shape as ice_volume and snow_volume).\n",
    "    ice_volume : np.ndarray\n",
    "        Sea ice volume per grid cell.\n",
    "    snow_volume : np.ndarray\n",
    "        Snow volume per grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : np.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize arrays\n",
    "    height_ice = np.zeros_like(ice_volume)\n",
    "    height_snow = np.zeros_like(snow_volume)\n",
    "\n",
    "    # Valid mask: avoid dividing by very small or zero area\n",
    "    valid = area > MIN_AREA\n",
    "\n",
    "    # Safely compute heights where valid\n",
    "    height_ice[valid] = ice_volume[valid] / area[valid]\n",
    "    height_snow[valid] = snow_volume[valid] / area[valid]\n",
    "\n",
    "    # Compute freeboard using the physical formula\n",
    "    freeboard = (\n",
    "        height_ice * (D_WATER - D_ICE) / D_WATER +\n",
    "        height_snow * (D_WATER - D_SNOW) / D_WATER\n",
    "    )\n",
    "\n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14a2b9bd-719b-46cf-b247-89f393fd37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_freeboard_outliers(freeboard_data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Checks for bad outliers in the freeboard data using the IQR method.\n",
    "    Logs the findings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    freeboard_data : np.ndarray\n",
    "        The flattened NumPy array of freeboard values to check.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Checking for Freeboard Outliers ---\")\n",
    "    \n",
    "    flat_freeboard = freeboard_data.flatten()\n",
    "    total_elements = len(flat_freeboard)\n",
    "\n",
    "    count_zero = np.sum(flat_freeboard == 0)\n",
    "    percent_zero = (count_zero / total_elements) * 100\n",
    "    logging.info(f\"Percentage of Freeboard values exactly 0: {percent_zero:.2f}% ({count_zero} points)\")\n",
    "\n",
    "    Q1 = np.percentile(flat_freeboard, 25)\n",
    "    Q3 = np.percentile(flat_freeboard, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_low = flat_freeboard[flat_freeboard < lower_bound]\n",
    "    outliers_high = flat_freeboard[flat_freeboard > upper_bound]\n",
    "\n",
    "    num_outliers = len(outliers_low) + len(outliers_high)\n",
    "    \n",
    "    logging.info(f\"Freeboard Q1: {Q1:.4f}\")\n",
    "    logging.info(f\"Freeboard Q3: {Q3:.4f}\")\n",
    "    logging.info(f\"Freeboard IQR: {IQR:.4f}\")\n",
    "    logging.info(f\"Freeboard Lower Bound (Q1 - 1.5*IQR): {lower_bound:.4f}\")\n",
    "    logging.info(f\"Freeboard Upper Bound (Q3 + 1.5*IQR): {upper_bound:.4f}\")\n",
    "    logging.info(f\"Number of low outliers: {len(outliers_low)}\")\n",
    "    logging.info(f\"Number of high outliers: {len(outliers_high)}\")\n",
    "    logging.info(f\"Total outliers: {num_outliers} ({num_outliers / total_elements * 100:.2f}% of total elements)\")\n",
    "\n",
    "    if num_outliers > 0:\n",
    "        logging.warning(\"Potential outliers detected in Freeboard data!\")\n",
    "        logging.info(f\"Sample low outliers (first 10): {outliers_low[:10]}\")\n",
    "        logging.info(f\"Sample high outliers (first 10): {outliers_high[:10]}\")\n",
    "        logging.info(f\"Sample high outliers (last 10): {outliers_high[:-10]}\")\n",
    "    else:\n",
    "        logging.info(\"No significant outliers detected in Freeboard data based on IQR method.\")\n",
    "\n",
    "def plot_freeboard_distribution(freeboard_data: np.ndarray, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the freeboard variable using a histogram and a boxplot,\n",
    "    and saves the plot as a PNG file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    freeboard_data : np.ndarray\n",
    "        The flattened NumPy array of freeboard values to plot.\n",
    "    save_path : str\n",
    "        The directory where the plot PNG file will be saved.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Plotting Freeboard Distribution ({prefix}) ---\")\n",
    "    \n",
    "    flat_freeboard = freeboard_data.flatten()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # --- Histogram ---\n",
    "    axes[0].hist(flat_freeboard, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title('Distribution of Freeboard (Histogram)')\n",
    "    axes[0].set_xlabel('Freeboard Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    axes[0].set_xlim(0, 1.8)\n",
    "\n",
    "    # --- Boxplot ---\n",
    "    axes[1].boxplot(flat_freeboard, vert=True, patch_artist=True, boxprops=dict(facecolor='lightcoral'),\n",
    "                    medianprops=dict(color='black'), whiskerprops=dict(color='gray'),\n",
    "                    capprops=dict(color='gray'), flierprops=dict(marker='o', markersize=5, markerfacecolor='red', alpha=0.5))\n",
    "    axes[1].set_title('Distribution of Freeboard (Boxplot)')\n",
    "    axes[1].set_ylabel('Freeboard Value')\n",
    "    axes[1].set_ylim(0, 0.3)\n",
    "    axes[1].set_xticks([])\n",
    "\n",
    "    plt.suptitle(f'Freeboard Data Distribution and Outlier Visualization {prefix}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect to leave space for suptitle\n",
    "    \n",
    "    # Save to the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(current_directory, f\"{prefix}_{timestamp}.png\")\n",
    "    \n",
    "    plt.savefig(filename, dpi=300) # dpi=300 for high-quality image\n",
    "    plt.close(fig) \n",
    "    \n",
    "    logging.info(f\"Freeboard distribution plot saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e92945c4-4945-4b6d-8d85-09c475f5117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ice_area_imbalance(ice_area_data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Measures and logs the percentage of ice_area data points that are 0, 1, or between 0 and 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ice_area_data : np.ndarray\n",
    "        The NumPy array of ice_area values (can be multi-dimensional).\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Analyzing Ice Area Imbalance ---\")\n",
    "\n",
    "    flat_ice_area = ice_area_data.flatten()\n",
    "    total_elements = len(flat_ice_area)\n",
    "\n",
    "    if total_elements == 0:\n",
    "        logging.warning(\"Ice Area data is empty, cannot analyze imbalance.\")\n",
    "        return\n",
    "\n",
    "    count_zero = np.sum(flat_ice_area == 0)\n",
    "    count_one = np.sum(flat_ice_area == 1)\n",
    "    count_between = np.sum((flat_ice_area > 0) & (flat_ice_area < 1))\n",
    "\n",
    "    percent_zero = (count_zero / total_elements) * 100\n",
    "    percent_one = (count_one / total_elements) * 100\n",
    "    percent_between = (count_between / total_elements) * 100\n",
    "\n",
    "    logging.info(f\"Total Ice Area data points: {total_elements}\")\n",
    "    logging.info(f\"Percentage of values == 0: {percent_zero:.2f}% ({count_zero} points)\")\n",
    "    logging.info(f\"Percentage of values == 1: {percent_one:.2f}% ({count_one} points)\")\n",
    "    logging.info(f\"Percentage of values between 0 and 1 (exclusive): {percent_between:.2f}% ({count_between} points)\")\n",
    "    \n",
    "    # Optional check for values outside [0, 1] range, if any\n",
    "    count_invalid = np.sum((flat_ice_area < 0) | (flat_ice_area > 1))\n",
    "    if count_invalid > 0:\n",
    "        logging.warning(f\"Found {count_invalid} ice_area values outside the [0, 1] range!\")\n",
    "\n",
    "\n",
    "def plot_ice_area_imbalance(ice_area_data: np.ndarray, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Creates a bar chart to visualize the imbalance of ice_area values (0, 1, or between 0-1).\n",
    "    Saves the chart as a PNG file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ice_area_data : np.ndarray\n",
    "        The NumPy array of ice_area values to plot (can be multi-dimensional).\n",
    "    save_path : str\n",
    "        The directory where the plot PNG file will be saved.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Plotting Ice Area Imbalance Chart ---\")\n",
    "\n",
    "    flat_ice_area = ice_area_data.flatten()\n",
    "    total_elements = len(flat_ice_area)\n",
    "\n",
    "    if total_elements == 0:\n",
    "        logging.warning(\"Ice Area data is empty, cannot plot imbalance.\")\n",
    "        return\n",
    "\n",
    "    count_zero = np.sum(flat_ice_area == 0)\n",
    "    count_00_to_25_percent = np.sum((flat_ice_area > 0) & (flat_ice_area < 0.25))\n",
    "    count_25_to_50_percent = np.sum((flat_ice_area > 0.25) & (flat_ice_area < 0.5))\n",
    "    count_59_to_75_percent = np.sum((flat_ice_area > 0.5) & (flat_ice_area < 0.75))\n",
    "    count_75_to_99_percent = np.sum((flat_ice_area > 0.75) & (flat_ice_area < 1))\n",
    "    count_one = np.sum(flat_ice_area == 1)\n",
    "    \n",
    "    categories = ['Exactly 0', '>0 - 0.25','0.25 - 0.5','0.5 - 0.75','0.75 - <1', 'Exactly 1']\n",
    "    percentages = [\n",
    "        (count_zero / total_elements) * 100,\n",
    "        (count_00_to_25_percent / total_elements) * 100,\n",
    "        (count_25_to_50_percent / total_elements) * 100,\n",
    "        (count_59_to_75_percent / total_elements) * 100,\n",
    "        (count_75_to_99_percent / total_elements) * 100,\n",
    "        (count_one / total_elements) * 100,\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    bars = ax.bar(categories, percentages, color=['black','gray','silver','lightgrey','whitesmoke','white','red'], edgecolor='black')\n",
    "     \n",
    "    ax.set_title('Distribution of Ice Area Values', fontsize=16)\n",
    "    ax.set_xlabel('Value Category', fontsize=12)\n",
    "    ax.set_ylabel('Percentage of Data (%)', fontsize=12)\n",
    "    ax.set_ylim(0, 80)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add percentage labels on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save to the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(current_directory, f\"{prefix}_SIC_imbalance_{timestamp}.png\")\n",
    "    \n",
    "    plt.savefig(filename, dpi=300) # dpi=300 for high-quality image\n",
    "    plt.close(fig) \n",
    "    logging.info(f\"Ice Area imbalance chart saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a739e27-622f-4a24-b031-270622c5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freeboard(freeboard, min_val=-0.2, max_val=1.2):\n",
    "    return np.clip((freeboard - min_val) / (max_val - min_val), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple\n",
    "from NC_FILE_PROCESSING.patchify_utils import patchify_by_latlon_spillover\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set level to logging.INFO to see the statements\n",
    "logging.basicConfig(filename='DailyNetCDFDataset.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional - transform applied to the data tensor *only*.\n",
    "    latitude_threshold\n",
    "        The minimum latitude to use for Artic data\n",
    "    context_length\n",
    "        The number of days to fetch for input in the prediction step\n",
    "    forecast_horizon\n",
    "        The number of days to predict in the future\n",
    "    plot_outliers_and_imbalance\n",
    "        Optional - check outliers and imbalance on the variables Ice Area and Freeboard\n",
    "    trial_run\n",
    "        Optional - use the data in the trial directory instead of the full dataset\n",
    "        Specify the name of the trial director in perlmutterpath.py\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        mesh_dir: str = mesh_dir,\n",
    "        transform: Callable = None,\n",
    "        latitude_threshold: int = LATITUDE_THRESHOLD,\n",
    "        context_length: int = CONTEXT_LENGTH,\n",
    "        forecast_horizon: int = FORECAST_HORIZON,\n",
    "        normalize_on: bool = NORMALIZE_ON,\n",
    "        plot_outliers_and_imbalance: bool = PLOT_DATA_DISTRIBUTION, # set FALSE FOR FINAL\n",
    "        trial_run: bool = TRIAL_RUN, # Use the trial data directory\n",
    "        num_patches: int = NUM_PATCHES,\n",
    "        cells_per_patch: int = CELLS_PER_PATCH\n",
    "        \n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "\n",
    "        Handle the raw data:\n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature (ice area, ice volume, etc.)\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Load the mesh and initialize the cell mask\n",
    "        3) Store a list of datetimes from each file \n",
    "        4) Extract raw data\n",
    "        \n",
    "        Perform pre-processing:\n",
    "        5) Apply a mask to nCells to look just at regions in certain latitudes\n",
    "            nCells >= 40 degrees is 53973 cells\n",
    "            nCells >= 50 degrees is 35623 cells\n",
    "        6) Derive Freeboard from ice area, snow volume, and ice volume\n",
    "        7) Custom patchify and store patch_ids so the data loader can use them\n",
    "        8) Optional: Plot the outliers and data imbalance for Ice Area and Freeboard\n",
    "        9) Optional: Normalize the data (Ice area is already between 0 and 1; Freeboard is not) \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.transform = transform\n",
    "        self.latitude_threshold = latitude_threshold\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.normalize_on = normalize_on\n",
    "        self.plot_outliers_and_imbalance = plot_outliers_and_imbalance\n",
    "        self.trial_run = trial_run\n",
    "        self.num_patches = num_patches\n",
    "        self.cells_per_patch = cells_per_patch\n",
    "\n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        if self.trial_run:\n",
    "            # USE THIS FOR PRACTICE (SMALLER CHUNK OF DATA)\n",
    "            self.data_dir = trial_data_dir\n",
    "            self.file_paths = sorted(\n",
    "                [\n",
    "                    os.path.join(trial_data_dir, f)\n",
    "                    for f in os.listdir(trial_data_dir)\n",
    "                    if f.endswith(\".nc\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # USE THE FULL DATASET (OR JUST A CERTAIN CENTURY, LIKE timeSeriesStatsDaily.20--)\n",
    "            self.data_dir = data_dir\n",
    "            self.file_paths = sorted(\n",
    "                [\n",
    "                    os.path.join(data_dir, f)\n",
    "                    for f in os.listdir(data_dir)\n",
    "                    if f.startswith(\"v3.LR.historical_0051.mpassi.hist.am.timeSeriesStatsDaily.\") and f.endswith(\".nc\")\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "        \n",
    "        # --- 2. Load the mesh file. Latitudes and Longitudes are in radians. ---\n",
    "        mesh = xr.open_dataset(mesh_dir)\n",
    "        latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "        lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "        mesh.close()\n",
    "        \n",
    "        # Initialize the cell mask\n",
    "        self.cell_mask = latCell >= latitude_threshold        \n",
    "        masked_ncells_size = np.count_nonzero(self.cell_mask)\n",
    "        logging.info(f\"Mask size: {masked_ncells_size}\")\n",
    "\n",
    "        self.full_to_masked = {\n",
    "            full_idx: new_idx\n",
    "            for new_idx, full_idx in enumerate(np.where(self.cell_mask)[0])\n",
    "        }\n",
    "\n",
    "        # Also store reverse mapping: masked -> full for recovery of data later\n",
    "        self.masked_to_full = {\n",
    "            v: k for k, v in self.full_to_masked.items()\n",
    "        }\n",
    "\n",
    "        logging.info(f\"=== Extracting raw data and times in a single loop === \")\n",
    "\n",
    "        all_times_list = []\n",
    "        ice_area_all_list = []\n",
    "        ice_volume_all_list = []\n",
    "        snow_volume_all_list = []\n",
    "        \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # --- 3. Store a list of datetimes from each file -> helps with retrieving 1 day's data later\n",
    "            # Extract times from byte string format\n",
    "            xtime_byte_array = ds[\"xtime_startDaily\"].values\n",
    "            xtime_unicode_array = xtime_byte_array.astype(str)\n",
    "            xtime_cleaned_array = np.char.replace(xtime_unicode_array, \"_\", \" \")\n",
    "            times_array = np.asarray(xtime_cleaned_array, dtype='datetime64[s]')\n",
    "            all_times_list.append(times_array)\n",
    "\n",
    "            # --- 4. Extract raw data\n",
    "            ice_area = ds[\"timeDaily_avg_iceAreaCell\"].values\n",
    "            ice_volume = ds[\"timeDaily_avg_iceVolumeCell\"].values\n",
    "            snow_volume = ds[\"timeDaily_avg_snowVolumeCell\"].values\n",
    "\n",
    "            # --- 5. Apply a mask to the nCells\n",
    "            ice_area = ice_area[:, self.cell_mask]\n",
    "            ice_volume = ice_volume[:, self.cell_mask]\n",
    "            snow_volume = snow_volume[:, self.cell_mask]\n",
    "\n",
    "            # Append masked data to lists\n",
    "            ice_area_all_list.append(ice_area)\n",
    "            ice_volume_all_list.append(ice_volume)\n",
    "            snow_volume_all_list.append(snow_volume)\n",
    "\n",
    "            ds.close() # Close dataset after processing\n",
    "\n",
    "        # --- Concatenate all collected data into single NumPy arrays after the loop\n",
    "        self.times = np.concatenate(all_times_list, axis=0)\n",
    "        self.ice_area = np.concatenate(ice_area_all_list, axis=0)\n",
    "        ice_volume_combined = np.concatenate(ice_volume_all_list, axis=0)\n",
    "        snow_volume_combined = np.concatenate(snow_volume_all_list, axis=0)\n",
    "\n",
    "        # Checking the dates\n",
    "        logging.info(f\"Parsed {len(self.times)} total dates\")\n",
    "        logging.info(f\"First few: {str(self.times[:5])}\")\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        logging.info(f\"Total days collected: {len(self.times)}\")\n",
    "        logging.info(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        logging.info(f\"First 35 days: {self.times[:35]}\")\n",
    "        logging.info(f\"Last 35 days: {self.times[-35:]}\")\n",
    "\n",
    "        logging.info(f\"Shape of combined ice_area array: {self.ice_area.shape}\")\n",
    "        logging.info(f\"Elapsed time for combined data/time loading: {time.time() - start_time} seconds\")\n",
    "        \n",
    "        # --- 6. Derive Freeboard from ice area, snow volume and ice volume\n",
    "        logging.info(f\"=== Calculating Freeboard === \")\n",
    "        self.freeboard = compute_freeboard(self.ice_area, ice_volume_combined, snow_volume_combined)\n",
    "        logging.info(f\"Elapsed time for freeboard calculation: {time.time() - start_time} seconds\")\n",
    "        \n",
    "        logging.info(f\"=== Patchifying === \")\n",
    "        \n",
    "        # --- 7. Custom patchify function\n",
    "        #     Returns \n",
    "        # full_nCells_patch_ids : np.ndarray\n",
    "        #     Array of shape (nCells,) giving patch ID or -1 if unassigned.\n",
    "        # indices_per_patch_id : List[np.ndarray]\n",
    "        #     List of patches, each a list of cell indices (np.ndarray of ints) that correspond with nCells array.\n",
    "        # patch_latlons : np.ndarray\n",
    "        #     Array of shape (n_patches, 2) containing (latitude, longitude) for one\n",
    "        #     representative cell per patch (the first cell added to the patch)\n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id, self.patch_latlons, self.algorithm = patchify_by_latlon_spillover(\n",
    "            latCell, lonCell, k=self.cells_per_patch, max_patches=self.num_patches, latitude_threshold=self.latitude_threshold)\n",
    "        \n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        # This ensures there's no out of bounds problem,\n",
    "        # like index 296237 is out of bounds for axis 1 with size 53973\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "        logging.info(f\"Elapsed time for patchifying with the {self.algorithm} algorithm: {time.time() - start_time} seconds\")\n",
    "\n",
    "        # --- 8. Optional --- OUTLIER DETECTION AND DATA IMBALANCE CHECK ---\n",
    "        prefix = \"\"\n",
    "        if self.trial_run:\n",
    "            prefix = \"trial\"\n",
    "            \n",
    "        if self.plot_outliers_and_imbalance:\n",
    "            logging.info(f\"=== Plotting Outliers and Imbalance === \")\n",
    "            check_freeboard_outliers(self.freeboard)\n",
    "            plot_freeboard_distribution(self.freeboard, f\"{prefix}_fb_pre_norm\")\n",
    "            analyze_ice_area_imbalance(self.ice_area)\n",
    "            plot_ice_area_imbalance(self.ice_area, prefix)\n",
    "\n",
    "        # --- 9. Optional --- Normalize the data (Area is already between 0 and 1; Freeboard is not)\n",
    "        if self.normalize_on:\n",
    "            logging.info(f\"=== Normalizing Freeboard === \")\n",
    "    \n",
    "            self.freeboard_min = self.freeboard.min()\n",
    "            self.freeboard_max = self.freeboard.max()\n",
    "    \n",
    "            logging.info(f\"Freeboard min (pre-norm): {self.freeboard_min} meters\" )\n",
    "            logging.info(f\"Freeboard max (pre-norm): {self.freeboard_max} meters\")\n",
    "    \n",
    "            self.freeboard = normalize_freeboard(\n",
    "                self.freeboard, min_val=self.freeboard_min, max_val=self.freeboard_max)\n",
    "    \n",
    "            logging.info(f\"Freeboard Shape: {self.freeboard.shape}\")\n",
    "            logging.info(f\"Ice Area Shape:  {self.ice_area.shape}\")\n",
    "    \n",
    "            logging.info(\"=== Normalized Freeboard ===\")\n",
    "            freeboard_min_after_norm = self.freeboard.min()\n",
    "            freeboard_max_after_norm  = self.freeboard.max()\n",
    "    \n",
    "            logging.info(f\"Freeboard min (post-norm): {freeboard_min_after_norm}\" )\n",
    "            logging.info(f\"Freeboard max (post-norm): {freeboard_max_after_norm}\")\n",
    "\n",
    "            if self.plot_outliers_and_imbalance:\n",
    "                check_freeboard_outliers(self.freeboard)\n",
    "                plot_freeboard_distribution(self.freeboard, f\"{prefix}_fb_post_norm\")\n",
    "\n",
    "        logging.info(\"End of __init__\")\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "        print(f\"Elapsed time for __init__: {end_time - start_time} seconds\")\n",
    "        print(f\"In minutes:                {(end_time - start_time)//60} minutes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of possible starting indices (idx) for a valid sequence.\n",
    "        A valid sequence needs `self.context_length` days for input and `self.forecast_horizon` days for target.\n",
    "        \n",
    "        ex) If the total number of days is 365, the context_length is 7 and the forecast_horizon is 3, then\n",
    "        \n",
    "        365 - (7 + 3) + 1 = 365 - 10 + 1 = 356 valid starting indices\n",
    "        \"\"\"\n",
    "        required_length = self.context_length + self.forecast_horizon\n",
    "        if len(self.freeboard) < required_length:\n",
    "            return 0 # Not enough raw data to form even one sample\n",
    "\n",
    "        # The number of valid starting indices\n",
    "        return len(self.freeboard) - required_length + 1\n",
    "\n",
    "    def get_patch_tensor(self, day_idx: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Retrieves the feature data for a specific day, organized into patches.\n",
    "\n",
    "        This method extracts 'freeboard' and 'ice_area' data for a given day\n",
    "        and then reshapes it according to the pre-defined patches. Each patch\n",
    "        will contain its own set of feature values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        day_idx : int\n",
    "            The integer index of the day to retrieve data for, relative to the\n",
    "            concatenated dataset's time dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing the feature data organized by patches for the\n",
    "            specified day.\n",
    "            Shape: (num_patches, num_features, patch_size)\n",
    "            Where:\n",
    "            - num_patches: Total number of patches (ex., 140).\n",
    "            - num_features: The number of features per cell (currently 2: freeboard, ice_area).\n",
    "            - patch_size: The number of cells within each patch.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        freeboard_day = self.freeboard[day_idx]  # (nCells,)\n",
    "        ice_area_day = self.ice_area[day_idx]    # (nCells,)\n",
    "        features = np.stack([freeboard_day, ice_area_day], axis=0)  # (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(patch_tensors)  # (context_length, num_patches, num_features, patch_size)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Given an input of a certain date id, get the input and the target tensors\n",
    "        2. Return all the patches for the input and the target\n",
    "           Features are: [freeboard, ice_area] over masked cells. \n",
    "           \n",
    "        \"\"\"\n",
    "        # Start with the id of the day in question\n",
    "        start_idx = idx\n",
    "\n",
    "        # end_idx is the exclusive end of the input sequence,\n",
    "        # and the inclusive start of the target sequence.\n",
    "        end_idx = idx + self.context_length\n",
    "        target_start = end_idx\n",
    "\n",
    "        # the target sequence ends after forecast horizon\n",
    "        target_end = end_idx + self.forecast_horizon\n",
    "\n",
    "        if target_end > len(self.freeboard):\n",
    "            raise IndexError(\n",
    "                f\"Requested time window exceeds dataset. \"\n",
    "                f\"Problematic idx: {idx}, \"\n",
    "                f\"Context Length: {self.context_length}, \"\n",
    "                f\"Forecast Horizon: {self.forecast_horizon}, \"\n",
    "                f\"Calculated target_end: {target_end}, \"\n",
    "                f\"Actual dataset length (len(self.freeboard)): {len(self.freeboard)}\"\n",
    "            )\n",
    "\n",
    "        # Build input tensor\n",
    "        input_seq = [self.get_patch_tensor(i) for i in range(start_idx, end_idx)]\n",
    "        input_tensor = torch.stack(input_seq)\n",
    "    \n",
    "        # Build target tensor: shape (forecast_horizon, num_patches)\n",
    "        target_seq = self.ice_area[end_idx:target_end]\n",
    "        target_patches = []\n",
    "        for day in target_seq:\n",
    "            patch_day = [\n",
    "                torch.tensor(day[patch_indices]) for patch_indices in self.indices_per_patch_id\n",
    "            ]\n",
    "            \n",
    "            # After stacking, patch_day_tensor will be (num_patches, CELLS_PER_PATCH)\n",
    "            patch_day_tensor = torch.stack(patch_day)  # (num_patches,)\n",
    "            target_patches.append(patch_day_tensor)\n",
    "\n",
    "        # Final target tensor shape: (forecast_horizon, num_patches, CELLS_PER_PATCH)\n",
    "        target_tensor = torch.stack(target_patches)  # (forecast_horizon, num_patches)\n",
    "        \n",
    "        return input_tensor, target_tensor, start_idx, end_idx, target_start, target_end\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} days, \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{len(self.file_paths)} files loaded>\"\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "480b5288-899b-49a0-919c-e559d8354c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID            ST USER      NAME          NODES TIME_LIMIT       TIME  SUBMIT_TIME          QOS             START_TIME           FEATURES       NODELIST(REASON\n",
      "41060061         R  brelypo   jupyter       1        6:00:00    1:11:02  2025-07-25T22:09:34  gpu_jupyter     2025-07-25T22:09:36  gpu&a100       nid001013      \n"
     ]
    }
   ],
   "source": [
    "!sqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class: TRIAL_RUN MODE IS False ===== \n",
      "Built 210 patches of size ~256\n",
      "Cluster sizes:\n",
      "min size 256\n",
      "max size 411284\n",
      "smallest count (np.int64(0), 256)\n",
      "max count (np.int64(-1), 411284)\n",
      "number of patches: 211\n",
      "Elapsed time for __init__: 3281.9779698848724 seconds\n",
      "In minutes:                54.0 minutes\n",
      "Training data length:    44706\n",
      "Validation data length:  9580\n",
      "Testing data length:     9580\n",
      "Total days =  63866\n",
      "Number of training batches 2794.125\n",
      "Number of training batches 598.75\n",
      "Number of test batches after drop_last incomplete batch 598\n",
      "Number of test days to drop after drop_last incomplete batch 598\n",
      "===== Printing Dataset ===== \n",
      "<DailyNetCDFDataset: 63866 days, 53973 cells/day, 2100 files loaded>\n",
      "Fetched start index 0: Time=1850-01-01T00:30:00\n",
      "Fetched end   index 7: Time=1850-01-08T00:00:00\n",
      "Fetched target start index 7: Time=1850-01-08T00:00:00\n",
      "Fetched target end   index 10: Time=1850-01-11T00:00:00\n",
      "===== Starting DataLoader ====\n",
      "input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\n",
      "actual input_tensor.shape = torch.Size([7, 210, 2, 256])\n",
      "target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\n",
      "actual target_tensor.shape = torch.Size([3, 210, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(f\"===== Making the Dataset Class: TRIAL_RUN MODE IS {TRIAL_RUN} ===== \")\n",
    "\n",
    "# load all the data from one folder\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "# Patch locations for positional embedding\n",
    "PATCH_LATLONS_TENSOR = torch.tensor(dataset.patch_latlons, dtype=torch.float32)\n",
    "\n",
    "# TODO: PLAY AROUND WITH DIFFERENT SUBSETS OF TIME FOR TESTING\n",
    "total_days = len(dataset)\n",
    "train_end = int(total_days * 0.7)\n",
    "val_end = int(total_days * 0.85)\n",
    "\n",
    "train_set = Subset(dataset, range(0, train_end))\n",
    "val_set   = Subset(dataset, range(train_end, val_end))\n",
    "test_set  = Subset(dataset, range(val_end, total_days))\n",
    "\n",
    "print(\"Training data length:   \", len(train_set))\n",
    "print(\"Validation data length: \", len(val_set))\n",
    "print(\"Testing data length:    \", len(test_set))\n",
    "\n",
    "total_days = len(train_set) + len(val_set) + len(test_set)\n",
    "print(\"Total days = \", total_days)\n",
    "\n",
    "print(\"Number of training batches\", len(train_set)/BATCH_SIZE)\n",
    "print(\"Number of training batches\", len(val_set)/BATCH_SIZE)\n",
    "\n",
    "print(\"Number of test batches after drop_last incomplete batch\", int(len(test_set)/BATCH_SIZE))\n",
    "print(\"Number of test days to drop after drop_last incomplete batch\", len(test_set)//BATCH_SIZE)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__  see how many files & days loaded\n",
    "\n",
    "# sample is tensor, ts is np.datetime64\n",
    "input_tensor, target_tensor, start_idx, end_idx, target_start, target_end = dataset[0]\n",
    "\n",
    "print(f\"Fetched start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched end   index {end_idx}: Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched target start index {target_start}: Time={dataset.times[target_start]}\")\n",
    "print(f\"Fetched target end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "print(\"===== Starting DataLoader ====\")\n",
    "# wrap in a DataLoader\n",
    "# 1. Use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. Use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "# 3. Use shuffle=False to preserve time order (especially for forecasting)\n",
    "# 4. Use drop_last=True to prevent it from testing on incomplete batches\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2, drop_last=True)\n",
    "\n",
    "print(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "print(f\"actual input_tensor.shape = {input_tensor.shape}\")\n",
    "print(\"target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\")\n",
    "print(f\"actual target_tensor.shape = {target_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d9baa0-427e-49d9-a50a-97c3d60a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IceForecastTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Transformer-based model for forecasting ice conditions based on sequences of\n",
    "    historical patch data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_patch_features_dim : int\n",
    "        The dimensionality of the feature vector for each individual patch (ex. 2 features).\n",
    "        This is the input dimension for the patch embedding layer.\n",
    "    num_patches : int\n",
    "        The total number of geographical patches that the `nCells` data was divided into.\n",
    "        (ex., 256 patches).\n",
    "    context_length : int, optional\n",
    "        The number of historical days (time steps) to use as input for the transformer.\n",
    "        Defaults to 7.\n",
    "    forecast_horizon : int, optional\n",
    "        The number of future days to predict for each patch.\n",
    "        Defaults to 1.\n",
    "    d_model : int, optional\n",
    "        The dimension of the model's hidden states (embedding dimension).\n",
    "        This is the size of the vectors that flow through the Transformer encoder.\n",
    "        Defaults to 128.\n",
    "    nhead : int, optional\n",
    "        The number of attention heads in the multi-head attention mechanism within\n",
    "        each Transformer encoder layer. Defaults to 8.\n",
    "    num_layers : int, optional\n",
    "        The number of Transformer encoder layers in the model. Defaults to 4.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : nn.Linear\n",
    "        Linear layer to project input patch features into the `d_model` hidden space.\n",
    "    encoder : nn.TransformerEncoder\n",
    "        The Transformer encoder module composed of `num_layers` encoder layers.\n",
    "    mlp_head : nn.Sequential\n",
    "        A multi-layer perceptron head for outputting predictions for each patch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_patch_features_dim: int = PATCH_EMBEDDING_INPUT_DIM, # D: The flat feature dimension of a single patch (ex., 512)\n",
    "                 num_patches: int = NUM_PATCHES,  # P: Number of spatial patches\n",
    "                 context_length: int = CONTEXT_LENGTH, # T: Number of historical time steps\n",
    "                 forecast_horizon: int = FORECAST_HORIZON, # Number of future time steps to predict (usually 1)\n",
    "                 d_model: int = D_MODEL,        # d_model: Transformer's embedding dimension\n",
    "                 nhead: int = N_HEAD,           # nhead: Number of attention heads\n",
    "                 num_layers: int = NUM_TRANSFORMER_LAYERS # num_layers: Number of TransformerEncoderLayers\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The transformer should\n",
    "        1. Accept a sequence of days (ex. 7 days of patches). \n",
    "           The context_length parameter says how many days to use for input.\n",
    "        2. Encode each patch with the transformer.\n",
    "        3. Output the patches for regression (ex. predict the 8th day).\n",
    "           The forecast_horizon parameter says how many days to use for the output prediction.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.input_patch_features_dim = input_patch_features_dim\n",
    "   \n",
    "        print(\"Calling IceForecastTransformer __init__\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Patch embedding layer: projects the raw patch features (512)\n",
    "        # into d_model (128) hidden space dimension\n",
    "        self.patch_embed = nn.Linear(input_patch_features_dim, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output tensors are (batch, sequence, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP head: (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        # Make a prediction for every cell per patch\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, CELLS_PER_PATCH * forecast_horizon)\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"End of __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B = Batch size\n",
    "        T = Time (context_length)\n",
    "        P = Patch count\n",
    "        D = Patch Dimension (cells per patch * feature count)\n",
    "        x: Tensor of shape (B, T, P, D)\n",
    "        Output: Tensor of shape (batch_size, forecast_horizon, num_patches)\n",
    "        Output: (B, forecast_horizon, P)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initial input x shape from DataLoader / pre-processing:\n",
    "        # (B, T, P, D) i.e., (Batch_Size, Context_Length, Num_Patches, Input_Patch_Features_Dim)\n",
    "        # Example: (16, 7, 140, 512)\n",
    "        \n",
    "        B, T, P, D = x.shape\n",
    "\n",
    "        # Flatten time and patches for the Transformer Encoder:\n",
    "        # Each (Time, Patch) combination becomes a single token in the sequence.\n",
    "        # Output shape: (B, T * P, D)\n",
    "        # Example: (16, 7 * 140 = 980, 512)\n",
    "        \n",
    "        # Flatten time and patches for the Transformer Encoder: (B, T * P, D)\n",
    "        # This treats each patch at each time step as a distinct token\n",
    "        x = x.view(B, T * P, D)\n",
    "\n",
    "        # Project patch features to the transformer's d_model dimension\n",
    "        x = self.patch_embed(x)  # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        x = self.encoder(x)      # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "\n",
    "        # Reshape back to separate time and patches: (B, T, P, d_model) ex., (16, 7, 140, 128)\n",
    "        x = x.view(B, T, P, self.d_model) \n",
    "\n",
    "        # Mean pooling over the time (context_length) dimension for each patch.\n",
    "        # This aggregates information from all historical time steps for each patch's final prediction.        \n",
    "        x = x.mean(dim=1)  # Output: (B, P, d_model) ex., (16, 140, 128)\n",
    "\n",
    "        # TODO: SOMEHOW SAVE ATTENTION TO MAP LATER\n",
    "\n",
    "        # Apply MLP head to predict values for each cell in each patch\n",
    "        # The MLP head outputs (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        x = self.mlp_head(x) # ex. (16, 140, 256 * 3) = (16, 140, 768)\n",
    "\n",
    "        # Reshape the output to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "        # Explicitly reshape the last dimension to seperate the forecast horizon out\n",
    "        x = x.view(B, P, self.forecast_horizon, CELLS_PER_PATCH) # Reshape into forecast_horizon and CELLS_PER_PATCH\n",
    "        x = x.permute(0, 2, 1, 3) # Permute to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "863146f1-b638-4658-9641-fd2bfdf731ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID            ST USER      NAME          NODES TIME_LIMIT       TIME  SUBMIT_TIME          QOS             START_TIME           FEATURES       NODELIST(REASON\n",
      "41060061         R  brelypo   jupyter       1        6:00:00    2:05:49  2025-07-25T22:09:34  gpu_jupyter     2025-07-25T22:09:36  gpu&a100       nid001013      \n"
     ]
    }
   ],
   "source": [
    "!sqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200db32-3fbb-4f8f-b841-bbcdc9117c11",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7383efb0-3e8d-468d-bf66-0bb91f943ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling IceForecastTransformer __init__\n",
      "Elapsed time: 0.03 seconds\n",
      "End of __init__\n",
      "\n",
      "--- Model Architecture ---\n",
      "IceForecastTransformer(\n",
      "  (patch_embed): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=128, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/10 - Train Loss: 0.0637\n",
      "Validation Loss: 0.0404\n",
      "Epoch 2/10 - Train Loss: 0.0317\n",
      "Validation Loss: 0.0238\n",
      "Epoch 3/10 - Train Loss: 0.0214\n",
      "Validation Loss: 0.0180\n",
      "Epoch 4/10 - Train Loss: 0.0170\n",
      "Validation Loss: 0.0149\n",
      "Epoch 5/10 - Train Loss: 0.0147\n",
      "Validation Loss: 0.0137\n",
      "Epoch 6/10 - Train Loss: 0.0133\n",
      "Validation Loss: 0.0125\n",
      "Epoch 7/10 - Train Loss: 0.0124\n",
      "Validation Loss: 0.0119\n",
      "Epoch 8/10 - Train Loss: 0.0118\n",
      "Validation Loss: 0.0113\n",
      "Epoch 9/10 - Train Loss: 0.0114\n",
      "Validation Loss: 0.0109\n",
      "Epoch 10/10 - Train Loss: 0.0112\n",
      "Validation Loss: 0.0108\n",
      "===============================================\n",
      "Elapsed time for TRAINING: 3013.16 seconds\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch import Tensor\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    import logging\n",
    "    \n",
    "    # Set level to logging.INFO to see the statements\n",
    "    logging.basicConfig(filename='IceForecastTransformerInstance.log', filemode='w', level=logging.INFO)\n",
    "    \n",
    "    model = IceForecastTransformer().to(device)\n",
    "    \n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    print(model)\n",
    "    print(\"--------------------------\\n\")\n",
    "    \n",
    "    logging.info(\"\\n--- Model Architecture ---\")\n",
    "    logging.info(str(model)) # Log the full model structure\n",
    "    logging.info(f\"Total model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    logging.info(\"--------------------------\\n\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(\"       STARTING EPOCHS       \")\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "    logging.info(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        for batch_idx, (input_tensor, target_tensor, start_idx, end_idx, target_start, target_end) in enumerate(train_loader):  \n",
    "    \n",
    "            # Move input and target to the device\n",
    "            # x: (B, context_length, num_patches, input_patch_features_dim), y: (B, forecast_horizon, num_patches)\n",
    "            x = input_tensor.to(device)  # Shape: (B, T, P, C, L)\n",
    "            y = target_tensor.to(device)  # Shape: (B, forecast_horizon, P, L)\n",
    "    \n",
    "            # Reshape x for transformer input\n",
    "            B, T, P, C, L = x.shape\n",
    "            x_reshaped_for_transformer_D = x.view(B, T, P, C * L)\n",
    "    \n",
    "            # Run through transformer\n",
    "            y_pred = model(x_reshaped_for_transformer_D) # y_pred is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, y) # DIRECTLY compare y_pred and y\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "        # --- Validation loop ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Unpack the full tuple\n",
    "                x_val, y_val, start_idx, end_idx, target_start, target_end = batch\n",
    "        \n",
    "                # Move to GPU if available\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "    \n",
    "                # Extract dimensions from x_val for reshaping\n",
    "                # x_val before reshaping: (B_val, T_val, P_val, C_val, L_val)\n",
    "                B_val, T_val, P_val, C_val, L_val = x_val.shape\n",
    "                \n",
    "                # Reshape x_val for transformer input\n",
    "                x_val_reshaped_for_transformer_input = x_val.view(B_val, T_val, P_val, C_val * L_val)\n",
    "    \n",
    "                # Model output is (B, forecast_horizon, P, L)\n",
    "                y_val_pred = model(x_val_reshaped_for_transformer_input) \n",
    "    \n",
    "                # Compute validation loss (y_val_pred and y_val should have identical shapes)\n",
    "                val_loss += criterion(y_val_pred, y_val).item() # y_val is (B, forecast_horizon, P, L)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    logging.info(\"===============================================\")\n",
    "    logging.info(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    logging.info(\"===============================================\")\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79121bc9-c71e-4928-9eea-82f500341911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID            ST USER      NAME          NODES TIME_LIMIT       TIME  SUBMIT_TIME          QOS             START_TIME           FEATURES       NODELIST(REASON\n",
      "41060061         R  brelypo   jupyter       1        6:00:00    2:56:16  2025-07-25T22:09:34  gpu_jupyter     2025-07-25T22:09:36  gpu&a100       nid001013      \n"
     ]
    }
   ],
   "source": [
    "!sqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {},
   "source": [
    "TODO OPTION: Try temporal attention only (ex., Informer, Time Series Transformer).\n",
    "\n",
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ecdd478-4bc1-4a3f-b91d-b40f03db7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at SIC_model_fd_nT_D128_B16_lt40_P210_L256_T7_Fh3_e10.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path where to save or load the model\n",
    "if TRAINING:\n",
    "    PATH = f\"SIC_model_{model_version}.pth\"\n",
    "    \n",
    "    # Save the model's state_dict\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(f\"Saved model at {PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a5f4e-ea4d-4cb1-88f4-6e800dc92fe4",
   "metadata": {},
   "source": [
    "# === BELOW - CAN BE USED ANY TIME FROM A .PTH FILE\n",
    "\n",
    "Make sure and run the cells that contain constants or run all, but comment out the \"save\" and the training loop cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278114f-96bb-4bbc-8e7f-f87c52e2a946",
   "metadata": {},
   "source": [
    "# Re-Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca0dc4b5-7cca-4a6d-99fd-6754ca833879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling IceForecastTransformer __init__\n",
      "Elapsed time: 0.01 seconds\n",
      "End of __init__\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if EVALUATING_ON:\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "    \n",
    "    # Instantiate the model (must have the same architecture as when it was saved)\n",
    "    # Create an identical instance of the original __init__ parameters\n",
    "    loaded_model = IceForecastTransformer()\n",
    "    \n",
    "    # Load the saved state_dict (weights_only=True helps ensure safety of pickle files)\n",
    "    loaded_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # Move the model to the appropriate device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loaded_model.to(device)\n",
    "    \n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e362e-fc7b-4692-93aa-96dc68546d91",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e630a7cf-9771-4b4d-b266-459305b95876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to Metrics_SIC_model_fd_nT_D128_B16_lt40_P210_L256_T7_Fh3_e10.pth.txt\n",
      "Shape of sample_x torch.Size([16, 7, 210, 2, 256])\n",
      "Shape of sample_y torch.Size([16, 3, 210, 256])\n",
      "Fetched sample_x start index tensor([54286, 54287, 54288, 54289, 54290, 54291, 54292, 54293, 54294, 54295,\n",
      "        54296, 54297, 54298, 54299, 54300, 54301]): Time=['1998-09-24T00:00:00' '1998-09-25T00:00:00' '1998-09-26T00:00:00'\n",
      " '1998-09-27T00:00:00' '1998-09-28T00:00:00' '1998-09-29T00:00:00'\n",
      " '1998-09-30T00:00:00' '1998-10-01T00:00:00' '1998-10-02T00:00:00'\n",
      " '1998-10-03T00:00:00' '1998-10-04T00:00:00' '1998-10-05T00:00:00'\n",
      " '1998-10-06T00:00:00' '1998-10-07T00:00:00' '1998-10-08T00:00:00'\n",
      " '1998-10-09T00:00:00']\n",
      "Fetched sample_x end   index tensor([54293, 54294, 54295, 54296, 54297, 54298, 54299, 54300, 54301, 54302,\n",
      "        54303, 54304, 54305, 54306, 54307, 54308]):   Time=['1998-10-01T00:00:00' '1998-10-02T00:00:00' '1998-10-03T00:00:00'\n",
      " '1998-10-04T00:00:00' '1998-10-05T00:00:00' '1998-10-06T00:00:00'\n",
      " '1998-10-07T00:00:00' '1998-10-08T00:00:00' '1998-10-09T00:00:00'\n",
      " '1998-10-10T00:00:00' '1998-10-11T00:00:00' '1998-10-12T00:00:00'\n",
      " '1998-10-13T00:00:00' '1998-10-14T00:00:00' '1998-10-15T00:00:00'\n",
      " '1998-10-16T00:00:00']\n",
      "Fetched sample_y (target) start index tensor([54296, 54297, 54298, 54299, 54300, 54301, 54302, 54303, 54304, 54305,\n",
      "        54306, 54307, 54308, 54309, 54310, 54311]): Time=['1998-10-04T00:00:00' '1998-10-05T00:00:00' '1998-10-06T00:00:00'\n",
      " '1998-10-07T00:00:00' '1998-10-08T00:00:00' '1998-10-09T00:00:00'\n",
      " '1998-10-10T00:00:00' '1998-10-11T00:00:00' '1998-10-12T00:00:00'\n",
      " '1998-10-13T00:00:00' '1998-10-14T00:00:00' '1998-10-15T00:00:00'\n",
      " '1998-10-16T00:00:00' '1998-10-17T00:00:00' '1998-10-18T00:00:00'\n",
      " '1998-10-19T00:00:00']\n",
      "Fetched sample_y (target) end   index tensor([54296, 54297, 54298, 54299, 54300, 54301, 54302, 54303, 54304, 54305,\n",
      "        54306, 54307, 54308, 54309, 54310, 54311]): Time=['1998-10-04T00:00:00' '1998-10-05T00:00:00' '1998-10-06T00:00:00'\n",
      " '1998-10-07T00:00:00' '1998-10-08T00:00:00' '1998-10-09T00:00:00'\n",
      " '1998-10-10T00:00:00' '1998-10-11T00:00:00' '1998-10-12T00:00:00'\n",
      " '1998-10-13T00:00:00' '1998-10-14T00:00:00' '1998-10-15T00:00:00'\n",
      " '1998-10-16T00:00:00' '1998-10-17T00:00:00' '1998-10-18T00:00:00'\n",
      " '1998-10-19T00:00:00']\n",
      "Sample x for inference shape (reshaped): torch.Size([16, 7, 210, 512])\n",
      "Predicted y patches shape: torch.Size([16, 3, 210, 256])\n",
      "Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH) ex., (16, 3, 140, 256)\n",
      "Predicted ice area for Day 0 (specific day) shape: torch.Size([16, 210, 256])\n",
      "Actual ice area for Day 0 (specific day) shape: torch.Size([16, 210, 256])\n",
      "Processing forecast day 0: Predicted shape torch.Size([16, 210, 256]), Actual shape torch.Size([16, 210, 256])\n",
      "Processing forecast day 1: Predicted shape torch.Size([16, 210, 256]), Actual shape torch.Size([16, 210, 256])\n",
      "Processing forecast day 2: Predicted shape torch.Size([16, 210, 256]), Actual shape torch.Size([16, 210, 256])\n"
     ]
    }
   ],
   "source": [
    "if EVALUATING_ON:\n",
    "\n",
    "    import io\n",
    "    \n",
    "    # Create a string buffer to capture output\n",
    "    captured_output = io.StringIO()\n",
    "    \n",
    "    # Redirect stdout to the buffer\n",
    "    sys.stdout = captured_output\n",
    "    \n",
    "    from scipy.stats import entropy\n",
    "    \n",
    "    # Accumulators for errors\n",
    "    all_abs_errors = [] # To store absolute errors for each cell in each patch\n",
    "    all_mse_errors = [] # To store MSE for each cell in each patch\n",
    "    \n",
    "    # Accumulators for histogram data\n",
    "    all_predicted_values_flat = []\n",
    "    all_actual_values_flat = []\n",
    "    \n",
    "    print(\"\\nStarting evaluation and metric calculation...\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: Batch Size: {BATCH_SIZE} Days\")\n",
    "    print(f\"DEBUG: Context Length: {CONTEXT_LENGTH} Days\")\n",
    "    print(f\"DEBUG: Forecast Horizon: {FORECAST_HORIZON} Days\")\n",
    "    print(f\"DEBUG: Number of batches in test_loader (with drop_last=True): {len(test_loader)} Batches\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: len(test_set): {len(test_set)} Days\")\n",
    "    print(f\"DEBUG: len(dataset) for splitting: {len(dataset)} Days\") # Should be 356\n",
    "    print(f\"DEBUG: train_end: {train_end}\")\n",
    "    print(f\"DEBUG: val_end: {val_end}\") # Should be 302\n",
    "    print(f\"DEBUG: range for test_set: {range(val_end, total_days)}\") # Should be range(302, 356)\n",
    "    print(\"==================\")\n",
    "    \n",
    "    # Iterate over the test_loader\n",
    "    # (B, forecast_horizon, P, CELLS_PER_PATCH) to match the model's output.\n",
    "    for i, (sample_x, sample_y, start_idx, end_idx, target_start, target_end) in enumerate(test_loader):\n",
    "        print(f\"Processing batch {i+1}/{len(test_loader)}\")\n",
    "        \n",
    "        # Move to device and apply initial reshape as done in training\n",
    "        sample_x = sample_x.to(device)\n",
    "        sample_y = sample_y.to(device) # Actual target values\n",
    "    \n",
    "        # Initial reshape of x for the Transformer model\n",
    "        B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "        sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "    \n",
    "        # Perform inference\n",
    "        with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "            predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "    \n",
    "        # Ensure predicted_y_patches and sample_y have the same shape for comparison\n",
    "        # Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "        if predicted_y_patches.shape != sample_y.shape:\n",
    "            print(f\"Shape mismatch: Predicted {predicted_y_patches.shape}, Actual {sample_y.shape}\")\n",
    "            continue # Skip this batch if shapes are incompatible\n",
    "    \n",
    "        # Calculate errors for each cell in each patch, across the forecast horizon and batch\n",
    "        # The errors will implicitly be averaged over the batch when we take the mean later\n",
    "        diff = predicted_y_patches - sample_y\n",
    "        abs_error_batch = torch.abs(diff)\n",
    "        mse_error_batch = diff ** 2\n",
    "    \n",
    "        # Accumulate errors (move to CPU for storage if memory is a concern)\n",
    "        all_abs_errors.append(abs_error_batch.cpu())\n",
    "        all_mse_errors.append(mse_error_batch.cpu())\n",
    "    \n",
    "        # Collect data for histograms (flatten all values)\n",
    "        all_predicted_values_flat.append(predicted_y_patches.cpu().numpy().flatten())\n",
    "        all_actual_values_flat.append(sample_y.cpu().numpy().flatten())\n",
    "    \n",
    "    # Concatenate all accumulated tensors\n",
    "    if all_abs_errors and all_mse_errors:\n",
    "        combined_abs_errors = torch.cat(all_abs_errors, dim=0) # Shape: (Total_Samples, FH, P, CPC)\n",
    "        combined_mse_errors = torch.cat(all_mse_errors, dim=0) # Shape: (Total_Samples, FH, P, CPC)\n",
    "    \n",
    "        # Calculate average MSE and Absolute Error for each cell in each patch\n",
    "        # Average over batch size and forecast horizon\n",
    "        # Resulting shape: (NUM_PATCHES, CELLS_PER_PATCH)\n",
    "        mean_abs_error_per_cell_patch = combined_abs_errors.mean(dim=(0, 1)) # Average over batch and forecast horizon\n",
    "        mean_mse_per_cell_patch = combined_mse_errors.mean(dim=(0, 1)) # Average over batch and forecast horizon\n",
    "    \n",
    "        print(\"\\n--- Error Metrics (Averaged per Cell per Patch) ---\")\n",
    "        print(f\"Mean Absolute Error (shape {mean_abs_error_per_cell_patch.shape}):\")\n",
    "        # print(mean_abs_error_per_cell_patch) # Uncomment to see the full tensor\n",
    "        print(f\"Overall Mean Absolute Error:            {mean_abs_error_per_cell_patch.mean().item():.4f}\")\n",
    "    \n",
    "        print(f\"\\nMean Squared Error (shape {mean_mse_per_cell_patch.shape}):\")\n",
    "        # print(mean_mse_per_cell_patch) # Uncomment to see the full tensor\n",
    "    \n",
    "        mse = mean_mse_per_cell_patch.mean().item()\n",
    "        print(f\"Overall Mean Squared Error:             {mse:.4f}\")\n",
    "    \n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Overall Root Mean Squared Error (RMSE): {rmse}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No data processed for error metrics. Check test_loader and data availability.\")\n",
    "    \n",
    "    # --- Histogram and Jensen-Shannon Distance ---\n",
    "    \n",
    "    # Concatenate all flattened values\n",
    "    if all_predicted_values_flat and all_actual_values_flat:\n",
    "        final_predicted_values = np.concatenate(all_predicted_values_flat)\n",
    "        final_actual_values = np.concatenate(all_actual_values_flat)\n",
    "    \n",
    "        print(f\"\\nTotal predicted values collected: {len(final_predicted_values)}\")\n",
    "        print(f\"Total actual values collected: {len(final_actual_values)}\")\n",
    "    \n",
    "        # Define bins for the histogram (e.g., for ice concentration between 0 and 1)\n",
    "        # Adjust bins based on the expected range of your data\n",
    "        bins = np.linspace(0, 1, 51) # 50 bins from 0 to 1\n",
    "    \n",
    "        # Compute histograms\n",
    "        hist_predicted, _ = np.histogram(final_predicted_values, bins=bins, density=True)\n",
    "        hist_actual, _ = np.histogram(final_actual_values, bins=bins, density=True)\n",
    "    \n",
    "        # Normalize histograms to sum to 1 (they are already density=True, but re-normalize for safety)\n",
    "        hist_predicted = hist_predicted / hist_predicted.sum()\n",
    "        hist_actual = hist_actual / hist_actual.sum()\n",
    "    \n",
    "        # Jensen-Shannon Distance function\n",
    "        def jensen_shannon_distance(p, q):\n",
    "            \"\"\"Calculates the Jensen-Shannon distance between two probability distributions.\"\"\"\n",
    "            # Ensure distributions sum to 1\n",
    "            p = p / p.sum()\n",
    "            q = q / q.sum()\n",
    "    \n",
    "            m = 0.5 * (p + q)\n",
    "            # Add a small epsilon to avoid log(0)\n",
    "            epsilon = 1e-10\n",
    "            jsd = 0.5 * (entropy(p + epsilon, m + epsilon) + entropy(q + epsilon, m + epsilon))\n",
    "            return np.sqrt(jsd) # JSD is the square root of JS divergence\n",
    "    \n",
    "        # Calculate Jensen-Shannon Distance\n",
    "        jsd = jensen_shannon_distance(hist_actual, hist_predicted)\n",
    "        print(f\"\\nJensen-Shannon Distance between actual and predicted histograms: {jsd:.4f}\")\n",
    "    \n",
    "        # Plotting Histograms\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(final_actual_values, bins=bins, alpha=0.7, label='Actual Data', color='skyblue', density=True)\n",
    "        plt.hist(final_predicted_values, bins=bins, alpha=0.7, label='Predicted Data', color='salmon', density=True)\n",
    "        plt.title('Distribution of Actual vs. Predicted Ice Concentration Values')\n",
    "        plt.xlabel('Ice Concentration Value')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.savefig(f\"SIE_Distribution_Actual_vs_Predicted_model_{model_version}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "        # When reading the histograms, look for overlap:\n",
    "        # High Overlap: predictions are close to actual values. Decent model.\n",
    "        # Low Overlap: predictions differ from actual values, issues with the model. \n",
    "    \n",
    "    else:\n",
    "        print(\"No data collected for histogram analysis. Check test_loader and data availability.\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete.\")\n",
    "    \n",
    "    # Restore stdout\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    # Now, write the captured output to the file\n",
    "    with open(f'Metrics_{PATH}.txt', 'w') as f:\n",
    "        f.write(captured_output.getvalue())\n",
    "    \n",
    "    print(f\"Metrics saved to Metrics_{PATH}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7955-7de9-4ac1-ae71-ef1f7bedd950",
   "metadata": {},
   "source": [
    "# Make a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e76ed2c4-bee4-4fd0-81b6-79a3e79ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATING_ON:\n",
    "    # Turn off the logging for this part\n",
    "    # https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
    "    logging.disable(level=logging.INFO)\n",
    "    \n",
    "    # Load one batch\n",
    "    data_iter = iter(test_loader)\n",
    "    sample_x, sample_y, start_idx, end_idx, target_start, target_end = next(data_iter)\n",
    "    \n",
    "    print(f\"Shape of sample_x {sample_x.shape}\")\n",
    "    print(f\"Shape of sample_y {sample_y.shape}\")   \n",
    "    \n",
    "    print(f\"Fetched sample_x start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "    print(f\"Fetched sample_x end   index {end_idx}:   Time={dataset.times[end_idx]}\")\n",
    "    \n",
    "    print(f\"Fetched sample_y (target) start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "    print(f\"Fetched sample_y (target) end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "    \n",
    "    # Move to device and apply initial reshape as done in training\n",
    "    sample_x = sample_x.to(device)\n",
    "    sample_y = sample_y.to(device) # Keep sample_y for actual comparison\n",
    "    \n",
    "    # Initial reshape of x for the Transformer model\n",
    "    B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "    sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "    \n",
    "    print(f\"Sample x for inference shape (reshaped): {sample_x_reshaped.shape}\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "        predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "    \n",
    "    print(f\"Predicted y patches shape: {predicted_y_patches.shape}\")\n",
    "    print(f\"Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH) ex., (16, {loaded_model.forecast_horizon}, 140, 256)\")\n",
    "                     \n",
    "    # Option 1: Select a specific day from the forecast horizon (ex., the first day)\n",
    "    # This is the shape (B, NUM_PATCHES, CELLS_PER_PATCH) for that specific day.\n",
    "    predicted_for_day_0 = predicted_y_patches[:, 0, :, :].cpu()\n",
    "    print(f\"Predicted ice area for Day 0 (specific day) shape: {predicted_for_day_0.shape}\")\n",
    "    \n",
    "    # Ensure sample_y has the same structure\n",
    "    actual_for_day_0 = sample_y[:, 0, :, :].cpu()\n",
    "    print(f\"Actual ice area for Day 0 (specific day) shape: {actual_for_day_0.shape}\")\n",
    "    \n",
    "    # Save predictions so that I can use cartopy by switching kernels for the next jupyter cell\n",
    "    np.save(f'patches/ice_area_patches_predicted_{PATH}_day0.npy', predicted_for_day_0)\n",
    "    np.save(f'patches/ice_area_patches_actual_{PATH}_day0.npy', actual_for_day_0)\n",
    "\n",
    "    # Option 2: Iterate through all forecast days\n",
    "    all_predicted_ice_areas = []\n",
    "    all_actual_ice_areas = []\n",
    "    \n",
    "    for day_idx in range(loaded_model.forecast_horizon):\n",
    "        predicted_day = predicted_y_patches[:, day_idx, :, :].cpu()\n",
    "        all_predicted_ice_areas.append(predicted_day)\n",
    "    \n",
    "        actual_day = sample_y[:, day_idx, :, :].cpu()\n",
    "        all_actual_ice_areas.append(actual_day)\n",
    "    \n",
    "        print(f\"Processing forecast day {day_idx}: Predicted shape {predicted_day.shape}, Actual shape {actual_day.shape}\")\n",
    "    \n",
    "        # Save each day's prediction/actual data if needed\n",
    "        # np.save(f'patches/ice_area_patches_predicted_day{day_idx}.npy', predicted_day)\n",
    "        # np.save(f'patches/ice_area_patches_actual_day{day_idx}.npy', actual_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Recover nCells from Patches for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cartopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m EVALUATING_ON:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# SWAP KERNELS IN THE JUPYTER NOTEBOOK #\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMAP_ANIMATION_GENERATION\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmap_gen_utility_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNC_FILE_PROCESSING\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnc_utility_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNC_FILE_PROCESSING\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatchify_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/pscratch/sd/b/brelypo/Predicting_SIC_SIE/MAP_ANIMATION_GENERATION/map_gen_utility_functions.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cmap\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Cartopy for map features, like land and ocean\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mccrs\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcfeature\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMAP_ANIMATION_GENERATION\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmap_label_utility_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cartopy'"
     ]
    }
   ],
   "source": [
    "if EVALUATING_ON:\n",
    "\n",
    "    ########################################\n",
    "    # SWAP KERNELS IN THE JUPYTER NOTEBOOK #\n",
    "    ########################################\n",
    "    \n",
    "    from MAP_ANIMATION_GENERATION.map_gen_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.nc_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.patchify_utils import *\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    predicted_ice_area_patches = np.load(f'patches/SIC_predicted_{model_version}_day0.npy')\n",
    "    actual_y_ice_area_patches = np.load(f'patches/SIC_actual_{model_version}_day0.npy')\n",
    "    \n",
    "    NUM_PATCHES = len(predicted_ice_area_patches[0])\n",
    "    print(\"NUM_PATCHES is\", NUM_PATCHES)\n",
    "    \n",
    "    latCell, lonCell = load_mesh(perlmutterpathMesh)\n",
    "    TOTAL_GRID_CELLS = len(lonCell) \n",
    "    cell_mask = latCell >= LATITUDE_THRESHOLD\n",
    "    \n",
    "    # Extract Freeboard (index 0) and Ice Area (index 1) for predicted and actual\n",
    "    # Predicted output is (B, 1, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "    # Assuming the model predicts ice area, which is the second feature (index 1)\n",
    "    # if the output of the model aligns with the order of features *within* the original patch_dim.\n",
    "    \n",
    "    # Load the original patch-to-cell mapping\n",
    "    # indices_per_patch_id = [\n",
    "    #     [idx_cell_0_0, ..., idx_cell_0_255],\n",
    "    #     [idx_cell_1_0, ..., idx_cell_1_255],\n",
    "    #     ...\n",
    "    # ]\n",
    "    \n",
    "    full_nCells_patch_ids, indices_per_patch_id, patch_latlons = patchify_by_latlon_spillover(\n",
    "                latCell, lonCell, k=256, max_patches=NUM_PATCHES, LATITUDE_THRESHOLD=LATITUDE_THRESHOLD)\n",
    "    \n",
    "    # Select one sample from the batch for visualization (ex., the first one)\n",
    "    # Output is (NUM_PATCHES, CELLS_PER_PATCH) for this single sample\n",
    "    sample_predicted_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    sample_actual_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    \n",
    "    # Initialize empty arrays for the full grid (nCells)\n",
    "    recovered_predicted_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    recovered_actual_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    \n",
    "    # Populate the full grid using the patch data and mapping\n",
    "    for patch_idx in range(NUM_PATCHES):\n",
    "        cell_indices_in_patch = indices_per_patch_id[patch_idx]\n",
    "        \n",
    "        # For predicted values\n",
    "        recovered_predicted_grid[cell_indices_in_patch] = sample_predicted_cells_per_patch[patch_idx]\n",
    "        nan_mask = np.isnan(recovered_predicted_grid)\n",
    "        nan_count = np.sum(nan_mask)\n",
    "    \n",
    "        # For actual values\n",
    "        recovered_actual_grid[cell_indices_in_patch] = sample_actual_cells_per_patch[patch_idx]\n",
    "    \n",
    "    print(f\"Recovered predicted grid shape: {recovered_predicted_grid.shape}\")\n",
    "    print(f\"Recovered actual grid shape: {recovered_actual_grid.shape}\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_predicted_grid, f\"model {model_version} ice area recovered\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_actual_grid, f\"model {model_version} ice area actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabb906-c95f-445f-a9b6-2e02d2f180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIC Kernel 2",
   "language": "python",
   "name": "sic_sie_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
