{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.0\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ead02e-ae14-41bb-a7fd-6242e11003f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count()) # check the number of available CUDA devices\n",
    "# will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor Name: x86_64\n",
      "Physical Cores: 128\n",
      "Logical Cores: 256\n",
      "Current CPU Frequency: 2635.41 MHz\n",
      "Min CPU Frequency: 0.00 MHz\n",
      "Max CPU Frequency: 0.00 MHz\n",
      "Total CPU Usage: 13.3%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd50d6-18a5-4f14-94c6-56d69814e0e6",
   "metadata": {},
   "source": [
    "# Example of one netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    timeDaily_counter             (Time) int32 124B ...\n",
       "    xtime_startDaily              (Time) |S64 2kB ...\n",
       "    xtime_endDaily                (Time) |S64 2kB ...\n",
       "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
       "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"]\n",
    "day_counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'xtime_startDaily' (Time: 31)> Size: 2kB\n",
      "[31 values with dtype=|S64]\n",
      "Dimensions without coordinates: Time\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'0010-01-01_00:00:00' b'0010-01-02_00:00:00' b'0010-01-03_00:00:00'\n",
      " b'0010-01-04_00:00:00' b'0010-01-05_00:00:00' b'0010-01-06_00:00:00'\n",
      " b'0010-01-07_00:00:00' b'0010-01-08_00:00:00' b'0010-01-09_00:00:00'\n",
      " b'0010-01-10_00:00:00' b'0010-01-11_00:00:00' b'0010-01-12_00:00:00'\n",
      " b'0010-01-13_00:00:00' b'0010-01-14_00:00:00' b'0010-01-15_00:00:00'\n",
      " b'0010-01-16_00:00:00' b'0010-01-17_00:00:00' b'0010-01-18_00:00:00'\n",
      " b'0010-01-19_00:00:00' b'0010-01-20_00:00:00' b'0010-01-21_00:00:00'\n",
      " b'0010-01-22_00:00:00' b'0010-01-23_00:00:00' b'0010-01-24_00:00:00'\n",
      " b'0010-01-25_00:00:00' b'0010-01-26_00:00:00' b'0010-01-27_00:00:00'\n",
      " b'0010-01-28_00:00:00' b'0010-01-29_00:00:00' b'0010-01-30_00:00:00'\n",
      " b'0010-01-31_00:00:00']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 465044)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9d6581-da44-4d6f-9582-908d0a86581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(31, 465044), dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'Time': 31, 'nCells': 465044, 'nVertices': 942873})\n"
     ]
    }
   ],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 407MB\n",
      "Dimensions:                       (Time: 31, nCells: 465044, nVertices: 942873)\n",
      "Dimensions without coordinates: Time, nCells, nVertices\n",
      "Data variables:\n",
      "    timeDaily_counter             (Time) int32 124B ...\n",
      "    xtime_startDaily              (Time) |S64 2kB b'0010-01-01_00:00:00' ... ...\n",
      "    xtime_endDaily                (Time) |S64 2kB ...\n",
      "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB 0.0 0.0 ... 0.0\n",
      "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "Attributes: (12/490)\n",
      "    case:                                                         v3.LR.DTEST...\n",
      "    source_id:                                                    9741e0bba2\n",
      "    realm:                                                        seaIce\n",
      "    product:                                                      model-output\n",
      "    title:                                                        MPAS-Seaice...\n",
      "    source:                                                       E3SM Sea Ic...\n",
      "    ...                                                           ...\n",
      "    config_AM_timeSeriesStatsCustom_reference_times:              initial_time\n",
      "    config_AM_timeSeriesStatsCustom_duration_intervals:           repeat_inte...\n",
      "    config_AM_timeSeriesStatsCustom_repeat_intervals:             reset_interval\n",
      "    config_AM_timeSeriesStatsCustom_reset_intervals:              00-00-07_00...\n",
      "    config_AM_timeSeriesStatsCustom_backward_output_offset:       00-00-01_00...\n",
      "    file_id:                                                      smms2lytbk\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e89844-e98f-4b42-b3cd-e3593ea3f151",
   "metadata": {},
   "source": [
    "# Example of Mesh File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a8b6f4-6969-45c9-8859-709234040954",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(\"NC_FILE_PROCESSING/mpassi.IcoswISC30E3r5.20231120.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80b4df70-fa5a-4c94-8be1-f93caae8209c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
       "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
       "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
       "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
       "    angleEdge          (nEdges) float64 11MB ...\n",
       "    dcEdge             (nEdges) float64 11MB ...\n",
       "    dvEdge             (nEdges) float64 11MB ...\n",
       "    indexToEdgeID      (nEdges) int32 6MB ...\n",
       "    latEdge            (nEdges) float64 11MB ...\n",
       "    lonEdge            (nEdges) float64 11MB ...\n",
       "    nEdgesOnEdge       (nEdges) int32 6MB ...\n",
       "    xEdge              (nEdges) float64 11MB ...\n",
       "    yEdge              (nEdges) float64 11MB ...\n",
       "    zEdge              (nEdges) float64 11MB ...\n",
       "    fEdge              (nEdges) float64 11MB ...\n",
       "    cellsOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    edgesOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    kiteAreasOnVertex  (nVertices, vertexDegree) float64 23MB ...\n",
       "    areaTriangle       (nVertices) float64 8MB ...\n",
       "    indexToVertexID    (nVertices) int32 4MB ...\n",
       "    latVertex          (nVertices) float64 8MB ...\n",
       "    lonVertex          (nVertices) float64 8MB ...\n",
       "    xVertex            (nVertices) float64 8MB ...\n",
       "    yVertex            (nVertices) float64 8MB ...\n",
       "    zVertex            (nVertices) float64 8MB ...\n",
       "    fVertex            (nVertices) float64 8MB ...\n",
       "    cellsOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    edgesOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    verticesOnCell     (nCells, maxEdges) int32 11MB ...\n",
       "    areaCell           (nCells) float64 4MB ...\n",
       "    indexToCellID      (nCells) int32 2MB ...\n",
       "    latCell            (nCells) float64 4MB ...\n",
       "    lonCell            (nCells) float64 4MB ...\n",
       "    meshDensity        (nCells) float64 4MB ...\n",
       "    nEdgesOnCell       (nCells) int32 2MB ...\n",
       "    xCell              (nCells) float64 4MB ...\n",
       "    yCell              (nCells) float64 4MB ...\n",
       "    zCell              (nCells) float64 4MB ...\n",
       "    fCell              (nCells) float64 4MB ...\n",
       "    landIceMask        (Time, nCells) int32 2MB ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40476985-dfb0-4da2-b48e-eb2c97f74f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     5      4      0      0      0      0]\n",
      " [    12     11      9      8      0      3]\n",
      " [     4     13     12      2      0      0]\n",
      " ...\n",
      " [465043      0 465040 465041      0      0]\n",
      " [     0 465042      0 465044      0      0]\n",
      " [     0      0      0      0 465043      0]]\n"
     ]
    }
   ],
   "source": [
    "cellsOnCell = mesh[\"cellsOnCell\"].values\n",
    "print(mesh[\"cellsOnCell\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af6cbd2b-ae4f-41d4-b8c1-213fb92b3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465044\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mesh[\"cellsOnCell\"].max().values)\n",
    "print(mesh[\"cellsOnCell\"].min().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21202b02-d87e-4354-aea7-586babbfb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cellsOnCell.npy', cellsOnCell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0aab3b-733a-4f9f-ad63-e0a510b05cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'nEdges': 1408196, 'maxEdges2': 12, 'TWO': 2, 'nVertices': 942873, 'vertexDegree': 3, 'nCells': 465044, 'maxEdges': 6, 'Time': 1})\n"
     ]
    }
   ],
   "source": [
    "print(mesh.coords)\n",
    "print(mesh.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a816fde5-ec19-4430-9875-f4e8cadc10d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 509MB\n",
      "Dimensions:            (nEdges: 1408196, maxEdges2: 12, TWO: 2,\n",
      "                        nVertices: 942873, vertexDegree: 3, nCells: 465044,\n",
      "                        maxEdges: 6, Time: 1)\n",
      "Dimensions without coordinates: nEdges, maxEdges2, TWO, nVertices,\n",
      "                                vertexDegree, nCells, maxEdges, Time\n",
      "Data variables: (12/40)\n",
      "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
      "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
      "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
      "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
      "    angleEdge          (nEdges) float64 11MB ...\n",
      "    dcEdge             (nEdges) float64 11MB ...\n",
      "    ...                 ...\n",
      "    nEdgesOnCell       (nCells) int32 2MB ...\n",
      "    xCell              (nCells) float64 4MB ...\n",
      "    yCell              (nCells) float64 4MB ...\n",
      "    zCell              (nCells) float64 4MB ...\n",
      "    fCell              (nCells) float64 4MB ...\n",
      "    landIceMask        (Time, nCells) int32 2MB ...\n",
      "Attributes: (12/1313)\n",
      "    model_name:                                                      mpas\n",
      "    core_name:                                                       ocean\n",
      "    source:                                                          MPAS\n",
      "    Conventions:                                                     MPAS\n",
      "    git_version:                                                     v2.1.0-1...\n",
      "    on_a_sphere:                                                     YES\n",
      "    ...                                                              ...\n",
      "    MPAS_Mesh_NCO_Version:                                           5.1.9\n",
      "    MPAS_Mesh_ESMF_Version:                                          8.4.2\n",
      "    MPAS_Mesh_geometric_features_Version:                            1.3.0\n",
      "    MPAS_Mesh_Metis_Version:                                         5.1.1\n",
      "    MPAS_Mesh_pyremap_Version:                                       1.2.0\n",
      "    NCO:                                                             netCDF O...\n"
     ]
    }
   ],
   "source": [
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fff3ae1-5dc1-4ac7-af6d-eaa97bee17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Pre-processing + Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_AREA = 1e-6\n",
    "\n",
    "def compute_freeboard(area: np.ndarray, \n",
    "                      ice_volume: np.ndarray, \n",
    "                      snow_volume: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow volume and area.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    area : np.ndarray\n",
    "        Sea ice concentration / area (same shape as ice_volume and snow_volume).\n",
    "    ice_volume : np.ndarray\n",
    "        Sea ice volume per grid cell.\n",
    "    snow_volume : np.ndarray\n",
    "        Snow volume per grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : np.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize arrays\n",
    "    height_ice = np.zeros_like(ice_volume)\n",
    "    height_snow = np.zeros_like(snow_volume)\n",
    "\n",
    "    # Valid mask: avoid dividing by very small or zero area\n",
    "    valid = area > MIN_AREA\n",
    "\n",
    "    # Safely compute heights where valid\n",
    "    height_ice[valid] = ice_volume[valid] / area[valid]\n",
    "    height_snow[valid] = snow_volume[valid] / area[valid]\n",
    "\n",
    "    # Compute freeboard using the physical formula\n",
    "    freeboard = (\n",
    "        height_ice * (D_WATER - D_ICE) / D_WATER +\n",
    "        height_snow * (D_WATER - D_SNOW) / D_WATER\n",
    "    )\n",
    "\n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a739e27-622f-4a24-b031-270622c5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freeboard(freeboard, min_val=-0.2, max_val=1.2):\n",
    "    return np.clip((freeboard - min_val) / (max_val - min_val), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7df3ff-a8ea-46cc-9053-97bfbb41cea8",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "\n",
    "TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "\n",
    "TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "\n",
    "TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "Too few workers: GPUs might become idle waiting for data.\n",
    "Too many workers: Can lead to increased CPU memory usage and context switching overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646812d0-b13a-4fff-a261-9fb5f6835cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 64\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple\n",
    "from NC_FILE_PROCESSING.patchify_utils import patchify_by_latlon_spillover\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set level to logging.INFO to see the statements\n",
    "logging.basicConfig(filename='DailyNetCDFDataset.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional transform applied to the data tensor *only*.\n",
    "    decode_time : bool\n",
    "        Let xarray convert CF-style time coordinates to np.datetime64.\n",
    "    drop_missing : bool\n",
    "        If True, drops any days where one of the requested variables is missing.\n",
    "    latitude_threshold\n",
    "        The minimum latitude to use for Artic data\n",
    "    context_length\n",
    "        The number of days to fetch for input in the prediction step\n",
    "    forecast_horizon\n",
    "        The number of days to predict in the future\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        mesh_dir: str = mesh_dir,\n",
    "        transform: Callable = None,\n",
    "        decode_time: bool = True,\n",
    "        drop_missing: bool = True,\n",
    "        latitude_threshold = 40,\n",
    "        context_length = 7,\n",
    "        forecast_horizon = 1\n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "\n",
    "        Handle the raw data:\n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature (ice area, ice volume, etc.)\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Store the datetime information from each nCells array from the daily data\n",
    "        3) Extract raw data\n",
    "        \n",
    "        Perform pre-processing:\n",
    "        4) Apply a mask to nCells to look just at regions in certain latitudes\n",
    "            nCells >= 40 degrees is 53973 cells\n",
    "            nCells >= 50 degrees is 35623 cells\n",
    "        5) Derive Freeboard from ice area, snow volume, and ice volume\n",
    "        6) Custom patchify and store patch_ids so the data loader can use them\n",
    "        7) Concatenate the data across Time\n",
    "        8) Normalize the data (Ice area is already between 0 and 1; Freeboard is not) \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.transform = transform\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        self.data_dir = data_dir\n",
    "        self.file_paths = sorted(\n",
    "            [\n",
    "                os.path.join(data_dir, f)\n",
    "                for f in os.listdir(data_dir)\n",
    "                if f.endswith(\".nc\")\n",
    "            ]\n",
    "        )\n",
    "        logging.info(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        # for f in self.file_paths:\n",
    "        #     logging.info(f\"  - {f}\")     # Print all the file names in the folder\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "\n",
    "        # Open all the netCDF files and concatenate them along Time dimension\n",
    "        logging.info(\"Loading datasets with xarray.open_mfdataset...\")\n",
    "        \n",
    "        ds = xr.open_mfdataset(\n",
    "            self.file_paths,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=\"Time\", # Use the NetCDF's Time dimension for concatenation\n",
    "            decode_times=False,\n",
    "            parallel=False,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Finished loading full dataset into a local variable.\")\n",
    "\n",
    "        logging.info(f\"Dataset dimensions: {ds.dims}\")\n",
    "        logging.info(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        # --- 2. Store a list of datetimes from each file -> helps with retrieving 1 day's data later\n",
    "        all_times = []\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "        \n",
    "            # Decode byte strings and fix the format\n",
    "            xtime_strs = ds[\"xtime_startDaily\"].str.decode(\"utf-8\").values\n",
    "            xtime_strs = [s.replace(\"_\", \" \") for s in xtime_strs]  # \"0010-01-01_00:00:00\" → \"0010-01-01 00:00:00\"\n",
    "        \n",
    "            # Convert to datetime.datetime objects\n",
    "            times = [datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\") for s in xtime_strs]\n",
    "            all_times.extend(times)\n",
    "        \n",
    "        # Store in self.times\n",
    "        self.times = all_times\n",
    "        self.times = np.array(self.times, dtype='datetime64[s]')\n",
    "\n",
    "        # Checking the dates\n",
    "        logging.info(f\"Parsed {len(self.times)} total dates\")\n",
    "        logging.info(f\"First few: {str(self.times[:5])}\")\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        logging.info(f\"Total days collected: {len(self.times)}\")\n",
    "        logging.info(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        logging.info(f\"First 35 days: {self.times[:35]}\")\n",
    "        logging.info(f\"First days 360 to 400 days: {self.times[360:401]}\")\n",
    "\n",
    "        # Load the mesh file. Latitudes and Longitudes are in radians.\n",
    "        mesh = xr.open_dataset(mesh_dir)\n",
    "        latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "        lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "        self.cell_mask = latCell >= latitude_threshold\n",
    "        logging.info(f\"Mask size: {np.count_nonzero(self.cell_mask)}\")\n",
    "\n",
    "        self.full_to_masked = {\n",
    "            full_idx: new_idx\n",
    "            for new_idx, full_idx in enumerate(np.where(self.cell_mask)[0])\n",
    "        }\n",
    "\n",
    "        # --- 3. Extract raw data \n",
    "        self.freeboard_all = []\n",
    "        self.ice_area_all = []\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # Extract raw data\n",
    "            area = ds[\"timeDaily_avg_iceAreaCell\"].values\n",
    "            ice_volume = ds[\"timeDaily_avg_iceVolumeCell\"].values\n",
    "            snow_volume = ds[\"timeDaily_avg_snowVolumeCell\"].values\n",
    "\n",
    "            # --- 4. Apply a mask to the nCells\n",
    "            area = area[:, self.cell_mask]\n",
    "            ice_volume = ice_volume[:, self.cell_mask]\n",
    "            snow_volume = snow_volume[:, self.cell_mask]\n",
    "\n",
    "            # --- 5. Derive Freeboard from ice area, snow volume and ice volume\n",
    "            freeboard = compute_freeboard(area, ice_volume, snow_volume)\n",
    "\n",
    "            # These will be deleted later to save space\n",
    "            self.freeboard_all.append(freeboard) \n",
    "            self.ice_area_all.append(area)\n",
    "\n",
    "        # --- 6. Custom patchify function       \n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id = patchify_by_latlon_spillover(\n",
    "            latCell, lonCell, k=256, max_patches=140, lat_threshold=latitude_threshold)\n",
    "\n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "\n",
    "        # --- 7. Concatenate the data across Time\n",
    "\n",
    "        # Concatenate across time\n",
    "        self.freeboard = np.concatenate(self.freeboard_all, axis=0)  # (T, nCells)\n",
    "        self.ice_area = np.concatenate(self.ice_area_all, axis=0)    # (T, nCells)\n",
    "\n",
    "        # Discard the lists that are not needed anymore -- save space\n",
    "        del self.freeboard_all, self.ice_area_all\n",
    "\n",
    "        logging.info(f\"Freeboard {self.freeboard.shape}\")\n",
    "        logging.info(f\"Ice Area  {self.ice_area.shape}\")\n",
    "\n",
    "        # --- 8. Normalize the data (Area is already between 0 and 1; Freeboard is not)\n",
    "        self.freeboard_min = self.freeboard[0].min()\n",
    "        self.freeboard_max = self.freeboard[0].max()\n",
    "        \n",
    "        logging.info(f\"Freeboard min: {self.freeboard_min}\" )\n",
    "        logging.info(f\"Freeboard max: {self.freeboard_max}\")\n",
    "\n",
    "        self.freeboard_all = normalize_freeboard(\n",
    "            freeboard, min_val=self.freeboard_min, max_val=self.freeboard_max)\n",
    "\n",
    "        logging.info(\"=== Normalized Freeboard ===\")\n",
    "        logging.info(\"End of __init__\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns how many time steps are in the full dataset (ex. x days for Daily data). \"\"\"\n",
    "        logging.info(\"Calling __len__\")\n",
    "        return len(self.times)\n",
    "\n",
    "    def get_patch_tensor(self, day_idx: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Retrieves the feature data for a specific day, organized into patches.\n",
    "\n",
    "        This method extracts 'freeboard' and 'ice_area' data for a given day\n",
    "        and then reshapes it according to the pre-defined patches. Each patch\n",
    "        will contain its own set of feature values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        day_idx : int\n",
    "            The integer index of the day to retrieve data for, relative to the\n",
    "            concatenated dataset's time dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing the feature data organized by patches for the\n",
    "            specified day.\n",
    "            Shape: (num_patches, num_features, patch_size)\n",
    "            Where:\n",
    "            - num_patches: Total number of patches (ex., 140).\n",
    "            - num_features: The number of features per cell (currently 2: freeboard, ice_area).\n",
    "            - patch_size: The number of cells within each patch.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        freeboard_day = self.freeboard[day_idx]  # (nCells,)\n",
    "        ice_area_day = self.ice_area[day_idx]    # (nCells,)\n",
    "        features = np.stack([freeboard_day, ice_area_day], axis=0)  # (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(patch_tensors)  # (context_length, num_patches, num_features, patch_size)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Given an input of a certain date id, get the input and the target tensors\n",
    "        2. Return all the patches for the input and the target\n",
    "           Features are: [freeboard, ice_area] over masked cells. \n",
    "           \n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        logging.info(\"Calling __getitem__\")\n",
    "\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.context_length\n",
    "        target_start = end_idx + 1              # added this - TODO - CHECK FOR ERRORS\n",
    "        target_end = end_idx + self.forecast_horizon\n",
    "\n",
    "        if target_end > len(self.freeboard):\n",
    "            raise IndexError(\"Requested time window exceeds dataset\")\n",
    "        \n",
    "        # Build input tensor\n",
    "        input_seq = [self.get_patch_tensor(i) for i in range(start_idx, end_idx)]\n",
    "        input_tensor = torch.stack(input_seq)\n",
    "    \n",
    "        # Build target tensor: shape (forecast_horizon, num_patches)\n",
    "        target_seq = self.ice_area[end_idx:target_end]  # (forecast_horizon, nCells)\n",
    "        target_patches = []\n",
    "        for day in target_seq:\n",
    "            patch_day = [\n",
    "                torch.tensor(day[patch_indices]) for patch_indices in self.indices_per_patch_id\n",
    "            ]\n",
    "            patch_day_tensor = torch.stack(patch_day)  # (num_patches,)\n",
    "            target_patches.append(patch_day_tensor)\n",
    "        \n",
    "        target_tensor = torch.stack(target_patches)  # (forecast_horizon, num_patches)\n",
    "        \n",
    "        logging.info(f\"Input  tensor shape {input_tensor.shape}\")\n",
    "        logging.info(f\"Target tensor shape {target_tensor.shape}\")\n",
    "\n",
    "        logging.info(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "        logging.info(\"target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\")\n",
    "\n",
    "        logging.info(f\"Fetched start index {start_idx}: Time={self.times[start_idx]}\")\n",
    "        logging.info(f\"Fetched end   index {end_idx}: Time={self.times[end_idx]}\")\n",
    "        \n",
    "        logging.info(f\"Fetched target start index {target_end}: Time={self.times[target_end]}\")\n",
    "        logging.info(f\"Fetched target end   index {target_end}: Time={self.times[target_end]}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "        return input_tensor, target_tensor, start_idx, end_idx, target_start, target_end # TODO, CHECK FOR ERRORS\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} days, \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{len(self.file_paths)} files loaded>\"\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class ===== \n",
      "Built 140 patches of size ~256\n",
      "Cluster sizes:\n",
      "min size 256\n",
      "max size 429204\n",
      "smallest count (np.int64(0), 256)\n",
      "max count (np.int64(-1), 429204)\n",
      "number of patches: 141\n",
      "Training data length:    255\n",
      "Validation data length:  55\n",
      "Testing data length:     55\n",
      "Total days =  365\n",
      "===== Printing Dataset ===== \n",
      "<DailyNetCDFDataset: 365 days, 53973 cells/day, 12 files loaded>\n",
      "Fetched start index 0: Time=0010-01-01T00:00:00\n",
      "Fetched end   index 7: Time=0010-01-08T00:00:00\n",
      "Fetched target start index 8: Time=0010-01-09T00:00:00\n",
      "Fetched target end   index 8: Time=0010-01-09T00:00:00\n",
      "===== Starting DataLoader ====\n",
      "input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\n",
      "target_tensor should be of shape (forecast_horizon, num_patches)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(\"===== Making the Dataset Class ===== \")\n",
    "\n",
    "# OPTION 1: LOADING FROM ONE BIG FOLDER:\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "total_days = len(dataset)\n",
    "train_end = int(total_days * 0.7)\n",
    "val_end = int(total_days * 0.85)\n",
    "\n",
    "train_set = Subset(dataset, range(0, train_end))\n",
    "val_set   = Subset(dataset, range(train_end, val_end))\n",
    "test_set  = Subset(dataset, range(val_end, total_days))\n",
    "\n",
    "print(\"Training data length:   \", len(train_set))\n",
    "print(\"Validation data length: \", len(val_set))\n",
    "print(\"Testing data length:    \", len(test_set))\n",
    "\n",
    "total_days = len(train_set) + len(val_set) + len(test_set)\n",
    "print(\"Total days = \", total_days)\n",
    "\n",
    "# OPTION 2: LOADING FROM SEPARATE FOLDERS:\n",
    "# train_dataset = DailyNetCDFDataset(data_dir=\"/train\", mesh_dir=mesh_dir)\n",
    "# val_dataset   = DailyNetCDFDataset(data_dir=\"/valid\", mesh_dir=mesh_dir)\n",
    "# test_dataset  = DailyNetCDFDataset(data_dir=\"/test\",  mesh_dir=mesh_dir)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__ → see how many files & days loaded\n",
    "\n",
    "input_tensor, target_tensor, start_idx, end_idx, target_start, target_end = dataset[0]        # sample is tensor, ts is np.datetime64\n",
    "\n",
    "print(f\"Fetched start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched end   index {end_idx}: Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched target start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "print(f\"Fetched target end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "print(\"===== Starting DataLoader ====\")\n",
    "# wrap in a DataLoader\n",
    "# 1. Use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. Use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "# 3. Use shuffle=False to preserve time order (especially for forecasting)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "print(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "print(\"target_tensor should be of shape (forecast_horizon, num_patches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3d47c-9e26-41da-9b6b-7afe0faf54ab",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Constants / Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bd6b153-3646-4b50-b1e7-2807eac982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 7         # T: Number of historical time steps used for input\n",
    "FORECAST_HORIZON = 1       # Number of future time steps to predict (ex. 1 day for next time step)\n",
    "NUM_PATCHES = 140          # P: Number of spatial patches\n",
    "NUM_FEATURES = 2           # C: Number of features per cell (ex., Freeboard, Ice Area)\n",
    "CELLS_PER_PATCH = 256      # L: Number of cells within each patch\n",
    "D_MODEL = 128              # d_model: Dimension of the transformer's internal representations (embedding dimension)\n",
    "N_HEAD = 8                 # nhead: Number of attention heads\n",
    "NUM_TRANSFORMER_LAYERS = 4 # num_layers: Number of TransformerEncoderLayers\n",
    "\n",
    "# The input dimension for the patch embedding linear layer.\n",
    "# Each patch at a given time step has NUM_FEATURES * CELLS_PER_PATCH features.\n",
    "# This is the 'D' dimension used in the Transformer's input tensor (B, T, P, D).\n",
    "PATCH_EMBEDDING_INPUT_DIM = NUM_FEATURES * CELLS_PER_PATCH # 2 * 256 = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer Class\n",
    "<!-- outputs = model(features)\n",
    "model.train()\n",
    "model.eval() -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9d9baa0-427e-49d9-a50a-97c3d60a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IceForecastTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Transformer-based model for forecasting ice conditions based on sequences of\n",
    "    historical patch data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_patch_features_dim : int\n",
    "        The dimensionality of the feature vector for each individual patch (ex. 2 features).\n",
    "        This is the input dimension for the patch embedding layer.\n",
    "    num_patches : int\n",
    "        The total number of geographical patches that the `nCells` data was divided into.\n",
    "        (ex., 256 patches).\n",
    "    context_length : int, optional\n",
    "        The number of historical days (time steps) to use as input for the transformer.\n",
    "        Defaults to 7.\n",
    "    forecast_horizon : int, optional\n",
    "        The number of future days to predict for each patch.\n",
    "        Defaults to 1.\n",
    "    d_model : int, optional\n",
    "        The dimension of the model's hidden states (embedding dimension).\n",
    "        This is the size of the vectors that flow through the Transformer encoder.\n",
    "        Defaults to 128.\n",
    "    nhead : int, optional\n",
    "        The number of attention heads in the multi-head attention mechanism within\n",
    "        each Transformer encoder layer. Defaults to 8.\n",
    "    num_layers : int, optional\n",
    "        The number of Transformer encoder layers in the model. Defaults to 4.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : nn.Linear\n",
    "        Linear layer to project input patch features into the `d_model` hidden space.\n",
    "    encoder : nn.TransformerEncoder\n",
    "        The Transformer encoder module composed of `num_layers` encoder layers.\n",
    "    mlp_head : nn.Sequential\n",
    "        A multi-layer perceptron head for outputting predictions for each patch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_patch_features_dim: int, # D: The flat feature dimension of a single patch (ex., 512)\n",
    "                 num_patches: int,              # P: Number of spatial patches\n",
    "                 context_length: int,           # T: Number of historical time steps\n",
    "                 forecast_horizon: int,         # Number of future time steps to predict (usually 1)\n",
    "                 d_model: int = D_MODEL,        # d_model: Transformer's embedding dimension\n",
    "                 nhead: int = N_HEAD,           # nhead: Number of attention heads\n",
    "                 num_layers: int = NUM_TRANSFORMER_LAYERS # num_layers: Number of TransformerEncoderLayers\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The transformer should\n",
    "        1. Accept a sequence of days (ex. 7 days of patches). \n",
    "           The context_length parameter says how many days to use for input.\n",
    "        2. Encode each patch with the transformer\n",
    "        3. Output the patches for regression (ex. predict the 8th day)\n",
    "           The forecast_horizon parameter says how many days to use for the output prediction\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.input_patch_features_dim = input_patch_features_dim\n",
    "   \n",
    "        print(\"Calling IceForecastTransformer __init__\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Patch embedding layer: projects the raw patch features (512)\n",
    "        # into d_model (128) hidden space dimension\n",
    "        self.patch_embed = nn.Linear(input_patch_features_dim, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output tensors are (batch, sequence, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP head:\n",
    "        # Make a prediction for every cell per patch\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, CELLS_PER_PATCH) # TODO: CHECK - Should this be multiplied by forecast_horizon???\n",
    "        )\n",
    "\n",
    "        # Positional Encoding (from your previous code, assuming it's implemented)\n",
    "        # self.pos_encoder = PositionalEncoding(d_model)\n",
    "        # TODO: IMPLEMENT A simple positional embedding or a standard sine/cosine one.\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"End of __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B = Batch size\n",
    "        T = Time (context_length)\n",
    "        P = Patch count\n",
    "        D = Patch Dimension (cells per patch * feature count)\n",
    "        x: Tensor of shape (B, T, P, D)\n",
    "        Output: Tensor of shape (batch_size, forecast_horizon, num_patches)\n",
    "        Output: (B, forecast_horizon, P)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"Calling forward\")\n",
    "        \n",
    "        # Initial input x shape from DataLoader / pre-processing:\n",
    "        # (B, T, P, D) i.e., (Batch_Size, Context_Length, Num_Patches, Input_Patch_Features_Dim)\n",
    "        # Example: (16, 7, 140, 512)\n",
    "\n",
    "        logging.info(\"Expected x shape: (B, T, P, D) ex., (16, 7, 140, 512)\")\n",
    "        logging.info(\"Actual   x shape: \", x.shape)\n",
    "        \n",
    "        B, T, P, D = x.shape\n",
    "\n",
    "        # Flatten time and patches for the Transformer Encoder:\n",
    "        # Each (Time, Patch) combination becomes a single token in the sequence.\n",
    "        # Output shape: (B, T * P, D)\n",
    "        # Example: (16, 7 * 140 = 980, 512)\n",
    "        \n",
    "        # Flatten time and patches for the Transformer Encoder: (B, T * P, D)\n",
    "        # This treats each patch at each time step as a distinct token\n",
    "        x = x.view(B, T * P, D)\n",
    "\n",
    "        # Project patch features to the transformer's d_model dimension\n",
    "        x = self.patch_embed(x)  # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "        logging.info(\"Expected patch embedding dimensions: (B, T * P, d_model) ex., (16, 980, 128)\")\n",
    "        logging.info(\"Actual   patch embedding dimensions: \", x.shape)\n",
    "\n",
    "        # TODO: Add positional encoding HERE\n",
    "        # x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        x = self.encoder(x)      # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "\n",
    "        # Reshape back to separate time and patches: (B, T, P, d_model) ex., (16, 7, 140, 128)\n",
    "        x = x.view(B, T, P, self.d_model) \n",
    "\n",
    "        # Mean pooling over the time (context_length) dimension for each patch.\n",
    "        # This aggregates information from all historical time steps for each patch's final prediction.        \n",
    "        x = x.mean(dim=1)  # Output: (B, P, d_model) ex., (16, 140, 128)\n",
    "\n",
    "        # TODO: SOMEHOW SAVE ATTENTION TO MAP LATER\n",
    "\n",
    "        # Apply MLP head to predict values for each cell in each patch\n",
    "        # The MLP head outputs CELLS_PER_PATCH values for each of the P patches\n",
    "        x = self.mlp_head(x)  # Output: (B, P, CELLS_PER_PATCH) ex., (16, 140, 256)\n",
    "\n",
    "        # Add forecast_horizon dimension\n",
    "        # The target 'y' is (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "        # This makes the output shape match the target 'y' or the forecast_horizon\n",
    "        x = x.unsqueeze(self.forecast_horizon) # Output: (B, 1, P, CELLS_PER_PATCH) ex., (16, 1, 140, 256)\n",
    "\n",
    "        logging.info(\"Expected output dimensions: (B, 1, P, CELLS_PER_PATCH) ex., (16, 1, 140, 256)\")\n",
    "        logging.info(\"Actual   output dimensions: \", x.shape)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200db32-3fbb-4f8f-b841-bbcdc9117c11",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7383efb0-3e8d-468d-bf66-0bb91f943ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import Tensor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Set level to logging.INFO to see the statements\n",
    "# logging.basicConfig(filename='IceForecastTransformer.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "# model = IceForecastTransformer(\n",
    "#     input_patch_features_dim=PATCH_EMBEDDING_INPUT_DIM,\n",
    "#     num_patches=NUM_PATCHES,\n",
    "#     context_length=CONTEXT_LENGTH,\n",
    "#     forecast_horizon=FORECAST_HORIZON\n",
    "# ).to(device)\n",
    "\n",
    "# print(\"\\n--- Model Architecture ---\")\n",
    "# print(model)\n",
    "# print(\"--------------------------\\n\")\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 100\n",
    "\n",
    "# start_time = time.time()\n",
    "# logging.info(\" ===============================\")\n",
    "# logging.info(\" =      STARTING EPOCHS        =\")\n",
    "# logging.info(\" ===============================\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch_idx, (x, y) in enumerate(train_loader):  \n",
    "        \n",
    "#         # x: (B, context_length, num_patches, input_patch_features_dim), y: (B, forecast_horizon, num_patches)\n",
    "#         x = x.to(device) # Move to GPU if available\n",
    "#         y = y.to(device) # y is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "\n",
    "#         logging.info(\"Expected x shape is (B, T, P, C, L) ex., (16, 7, 140, 2, 256)\")\n",
    "#         logging.info(\"Actual   x shape is \", x.shape)\n",
    "\n",
    "#         logging.info(\"Expected y shape is (B, forecast_horizon, P, L) ex., (16, 1, 140, 256)\")\n",
    "#         logging.info(\"Actual   y shape is \", y.shape)\n",
    "\n",
    "#         # Reshape x for transformer input\n",
    "#         B, T, P, C, L = x.shape\n",
    "#         x_reshaped_for_transformer_D = x.view(B, T, P, C * L)\n",
    "\n",
    "#         logging.info(\"Expected reshaped x is (B, T, P, D_input)\")\n",
    "#         logging.info(\"Actual   reshaped x is \", x_reshaped_for_transformer_D.shape)  # should now be (B, T, P, 512)\n",
    "    \n",
    "#         # Run through transformer\n",
    "#         y_pred = model(x_reshaped_for_transformer_D) # y_pred is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "\n",
    "#         logging.info(\"Expected y_pred shape is (B, forecast_horizon , P, L)\")\n",
    "#         logging.info(\"Actual   y_pred shape is \", y_pred.shape)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(y_pred, y) # DIRECTLY compare y_pred and y\n",
    "    \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "#     # --- Validation loop ---\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for x_val, y_val in val_loader:\n",
    "#             x_val = x_val.to(device)\n",
    "#             y_val = y_val.to(device)\n",
    "\n",
    "#             # Extract dimensions from x_val for reshaping\n",
    "#             # x_val before reshaping: (B_val, T_val, P_val, C_val, L_val)\n",
    "#             B_val, T_val, P_val, C_val, L_val = x_val.shape\n",
    "            \n",
    "#             # Reshape x_val for transformer input\n",
    "#             x_val_reshaped_for_transformer_input = x_val.view(B_val, T_val, P_val, C_val * L_val)\n",
    "\n",
    "#             # Model output is (B, forecast_horizon, P, L)\n",
    "#             y_val_pred = model(x_val_reshaped_for_transformer_input) \n",
    "\n",
    "#             # Compute validation loss (y_val_pred and y_val should have identical shapes)\n",
    "#             val_loss += criterion(y_val_pred, y_val).item() # y_val is (B, forecast_horizon, P, L)\n",
    "    \n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"===============================================\")\n",
    "# print(f\"Elapsed time for TRAINING: {end_time - start_time:.2f} seconds\")\n",
    "# print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {},
   "source": [
    "TODO: Add Positional Encoding to represent time steps.\n",
    "\n",
    "TODO OPTION: Try temporal attention only (ex., Informer, Time Series Transformer).\n",
    "\n",
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d61b69c0-01b8-491a-b71b-79ac6a7798d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where to save the model\n",
    "# PATH = \"sea_ice_concentration_model.pth\"\n",
    "\n",
    "# # Save the model's state_dict\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278114f-96bb-4bbc-8e7f-f87c52e2a946",
   "metadata": {},
   "source": [
    "# Re-Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca0dc4b5-7cca-4a6d-99fd-6754ca833879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling IceForecastTransformer __init__\n",
      "Elapsed time: 0.01 seconds\n",
      "End of __init__\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the path where to load the model\n",
    "PATH = \"sea_ice_concentration_model.pth\"\n",
    "\n",
    "# Instantiate the model (must have the same architecture as when it was saved)\n",
    "# Create an identical instance of the original __init__ parameters\n",
    "# Make sure global constants (like D_MODEL, N_HEAD, etc.) are consistent.\n",
    "loaded_model = IceForecastTransformer(\n",
    "    input_patch_features_dim=PATCH_EMBEDDING_INPUT_DIM,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=NUM_TRANSFORMER_LAYERS\n",
    ")\n",
    "\n",
    "# Load the saved state_dict (weights_only=True helps ensure safety of pickle files)\n",
    "loaded_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7955-7de9-4ac1-ae71-ef1f7bedd950",
   "metadata": {},
   "source": [
    "# Make a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e76ed2c4-bee4-4fd0-81b6-79a3e79ded48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched sample_x start index tensor([310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
      "        324, 325]): Time=['0010-11-07T00:00:00' '0010-11-08T00:00:00' '0010-11-09T00:00:00'\n",
      " '0010-11-10T00:00:00' '0010-11-11T00:00:00' '0010-11-12T00:00:00'\n",
      " '0010-11-13T00:00:00' '0010-11-14T00:00:00' '0010-11-15T00:00:00'\n",
      " '0010-11-16T00:00:00' '0010-11-17T00:00:00' '0010-11-18T00:00:00'\n",
      " '0010-11-19T00:00:00' '0010-11-20T00:00:00' '0010-11-21T00:00:00'\n",
      " '0010-11-22T00:00:00']\n",
      "Fetched sample_x end   index tensor([317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330,\n",
      "        331, 332]):   Time=['0010-11-14T00:00:00' '0010-11-15T00:00:00' '0010-11-16T00:00:00'\n",
      " '0010-11-17T00:00:00' '0010-11-18T00:00:00' '0010-11-19T00:00:00'\n",
      " '0010-11-20T00:00:00' '0010-11-21T00:00:00' '0010-11-22T00:00:00'\n",
      " '0010-11-23T00:00:00' '0010-11-24T00:00:00' '0010-11-25T00:00:00'\n",
      " '0010-11-26T00:00:00' '0010-11-27T00:00:00' '0010-11-28T00:00:00'\n",
      " '0010-11-29T00:00:00']\n",
      "Fetched sample_y (target) start index tensor([318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331,\n",
      "        332, 333]): Time=['0010-11-15T00:00:00' '0010-11-16T00:00:00' '0010-11-17T00:00:00'\n",
      " '0010-11-18T00:00:00' '0010-11-19T00:00:00' '0010-11-20T00:00:00'\n",
      " '0010-11-21T00:00:00' '0010-11-22T00:00:00' '0010-11-23T00:00:00'\n",
      " '0010-11-24T00:00:00' '0010-11-25T00:00:00' '0010-11-26T00:00:00'\n",
      " '0010-11-27T00:00:00' '0010-11-28T00:00:00' '0010-11-29T00:00:00'\n",
      " '0010-11-30T00:00:00']\n",
      "Fetched sample_y (target) end   index tensor([318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331,\n",
      "        332, 333]): Time=['0010-11-15T00:00:00' '0010-11-16T00:00:00' '0010-11-17T00:00:00'\n",
      " '0010-11-18T00:00:00' '0010-11-19T00:00:00' '0010-11-20T00:00:00'\n",
      " '0010-11-21T00:00:00' '0010-11-22T00:00:00' '0010-11-23T00:00:00'\n",
      " '0010-11-24T00:00:00' '0010-11-25T00:00:00' '0010-11-26T00:00:00'\n",
      " '0010-11-27T00:00:00' '0010-11-28T00:00:00' '0010-11-29T00:00:00'\n",
      " '0010-11-30T00:00:00']\n",
      "Sample x for inference shape (reshaped): torch.Size([16, 7, 140, 512])\n",
      "Predicted y patches shape: torch.Size([16, 1, 140, 256])\n",
      "Expected shape: (B, 1, NUM_PATCHES, CELLS_PER_PATCH) e.g., (16, 1, 140, 256)\n"
     ]
    }
   ],
   "source": [
    "# Turn off the logging for this part\n",
    "# https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
    "logging.disable(level=logging.INFO)\n",
    "\n",
    "# Load one batch for demonstration\n",
    "data_iter = iter(test_loader)\n",
    "sample_x, sample_y, start_idx, end_idx, target_start, target_end = next(data_iter)\n",
    "\n",
    "print(f\"Fetched sample_x start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched sample_x end   index {end_idx}:   Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched sample_y (target) start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "print(f\"Fetched sample_y (target) end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "# Move to device and apply initial reshape as done in training\n",
    "sample_x = sample_x.to(device)\n",
    "sample_y = sample_y.to(device) # Keep sample_y for actual comparison\n",
    "\n",
    "# Initial reshape of x for the Transformer model\n",
    "B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "\n",
    "print(f\"Sample x for inference shape (reshaped): {sample_x_reshaped.shape}\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "    predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "\n",
    "print(f\"Predicted y patches shape: {predicted_y_patches.shape}\")\n",
    "print(\"Expected shape: (B, 1, NUM_PATCHES, CELLS_PER_PATCH) e.g., (16, 1, 140, 256)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Recover nCells from Patches for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabb906-c95f-445f-a9b6-2e02d2f180e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sea Ice Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
