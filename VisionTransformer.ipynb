{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/homes/b/brelypo/.conda/envs/sic_sie_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.0\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd50d6-18a5-4f14-94c6-56d69814e0e6",
   "metadata": {},
   "source": [
    "# Example of one netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    timeDaily_counter             (Time) int32 124B ...\n",
       "    xtime_startDaily              (Time) |S64 2kB ...\n",
       "    xtime_endDaily                (Time) |S64 2kB ...\n",
       "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
       "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"]\n",
    "day_counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'xtime_startDaily' (Time: 31)> Size: 2kB\n",
      "[31 values with dtype=|S64]\n",
      "Dimensions without coordinates: Time\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'0010-01-01_00:00:00' b'0010-01-02_00:00:00' b'0010-01-03_00:00:00'\n",
      " b'0010-01-04_00:00:00' b'0010-01-05_00:00:00' b'0010-01-06_00:00:00'\n",
      " b'0010-01-07_00:00:00' b'0010-01-08_00:00:00' b'0010-01-09_00:00:00'\n",
      " b'0010-01-10_00:00:00' b'0010-01-11_00:00:00' b'0010-01-12_00:00:00'\n",
      " b'0010-01-13_00:00:00' b'0010-01-14_00:00:00' b'0010-01-15_00:00:00'\n",
      " b'0010-01-16_00:00:00' b'0010-01-17_00:00:00' b'0010-01-18_00:00:00'\n",
      " b'0010-01-19_00:00:00' b'0010-01-20_00:00:00' b'0010-01-21_00:00:00'\n",
      " b'0010-01-22_00:00:00' b'0010-01-23_00:00:00' b'0010-01-24_00:00:00'\n",
      " b'0010-01-25_00:00:00' b'0010-01-26_00:00:00' b'0010-01-27_00:00:00'\n",
      " b'0010-01-28_00:00:00' b'0010-01-29_00:00:00' b'0010-01-30_00:00:00'\n",
      " b'0010-01-31_00:00:00']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 465044)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e9d6581-da44-4d6f-9582-908d0a86581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(31, 465044), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'Time': 31, 'nCells': 465044, 'nVertices': 942873})\n"
     ]
    }
   ],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 407MB\n",
      "Dimensions:                       (Time: 31, nCells: 465044, nVertices: 942873)\n",
      "Dimensions without coordinates: Time, nCells, nVertices\n",
      "Data variables:\n",
      "    timeDaily_counter             (Time) int32 124B ...\n",
      "    xtime_startDaily              (Time) |S64 2kB b'0010-01-01_00:00:00' ... ...\n",
      "    xtime_endDaily                (Time) |S64 2kB ...\n",
      "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB 0.0 0.0 ... 0.0\n",
      "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "Attributes: (12/490)\n",
      "    case:                                                         v3.LR.DTEST...\n",
      "    source_id:                                                    9741e0bba2\n",
      "    realm:                                                        seaIce\n",
      "    product:                                                      model-output\n",
      "    title:                                                        MPAS-Seaice...\n",
      "    source:                                                       E3SM Sea Ic...\n",
      "    ...                                                           ...\n",
      "    config_AM_timeSeriesStatsCustom_reference_times:              initial_time\n",
      "    config_AM_timeSeriesStatsCustom_duration_intervals:           repeat_inte...\n",
      "    config_AM_timeSeriesStatsCustom_repeat_intervals:             reset_interval\n",
      "    config_AM_timeSeriesStatsCustom_reset_intervals:              00-00-07_00...\n",
      "    config_AM_timeSeriesStatsCustom_backward_output_offset:       00-00-01_00...\n",
      "    file_id:                                                      smms2lytbk\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_AREA = 1e-6\n",
    "\n",
    "def compute_freeboard(area: np.ndarray, \n",
    "                      ice_volume: np.ndarray, \n",
    "                      snow_volume: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow volume and area.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    area : np.ndarray\n",
    "        Sea ice concentration / area (same shape as ice_volume and snow_volume).\n",
    "    ice_volume : np.ndarray\n",
    "        Sea ice volume per grid cell.\n",
    "    snow_volume : np.ndarray\n",
    "        Snow volume per grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : np.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize arrays\n",
    "    height_ice = np.zeros_like(ice_volume)\n",
    "    height_snow = np.zeros_like(snow_volume)\n",
    "\n",
    "    # Valid mask: avoid dividing by very small or zero area\n",
    "    valid = area > MIN_AREA\n",
    "\n",
    "    # Safely compute heights where valid\n",
    "    height_ice[valid] = ice_volume[valid] / area[valid]\n",
    "    height_snow[valid] = snow_volume[valid] / area[valid]\n",
    "\n",
    "    # Compute freeboard using the physical formula\n",
    "    freeboard = (\n",
    "        height_ice * (D_WATER - D_ICE) / D_WATER +\n",
    "        height_snow * (D_WATER - D_SNOW) / D_WATER\n",
    "    )\n",
    "\n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593ea2d5-1e69-44d0-a3aa-5fb8f206f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import timedelta\n",
    "from typing import List, Union, Callable, Tuple\n",
    "from NC_FILE_PROCESSING.nc_utility_functions import *\n",
    "from perlmutterpath import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional transform applied to the data tensor *only*.\n",
    "    decode_time : bool\n",
    "        Let xarray convert CF-style time coordinates to np.datetime64.\n",
    "    drop_missing : bool\n",
    "        If True, drops any days where one of the requested variables is missing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        transform: Callable = None,\n",
    "        decode_time: bool = True,\n",
    "        drop_missing: bool = True,\n",
    "        cell_mask=None\n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Store the datetime information from each nCells array from the daily data\n",
    "        3) Apply a mask to nCells to look just at regions above 40 degrees north (TODO: IMPLEMENT THE MASK)\n",
    "        4) Patchify and store patch_ids so the data loader can use them (TODO: IMPLEMENT THIS)\n",
    "        5) Perform pre-processing (calculate Freeboard from ice area, ice volume, and snow volume\n",
    "        6) Normalize the data (TODO: IMPLEMENT THIS)\n",
    "        7) Concatenate the data across Time \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        self.data_dir = data_dir\n",
    "        self.file_paths = sorted(\n",
    "            [\n",
    "                os.path.join(data_dir, f)\n",
    "                for f in os.listdir(data_dir)\n",
    "                if f.endswith(\".nc\")\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        # for f in self.file_paths:\n",
    "        #     print(f\"  - {f}\")     # Print all the file names in the folder\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "\n",
    "        # Open all the netCDF files and concatenate them along Time dimension\n",
    "        print(\"Loading datasets with xarray.open_mfdataset...\")\n",
    "        \n",
    "        ds = xr.open_mfdataset(\n",
    "            self.file_paths,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=\"Time\", # Use the NetCDF's Time dimension for concatenation\n",
    "            decode_times=False,\n",
    "            parallel=False,\n",
    "        )\n",
    "\n",
    "        print(\"Finished loading full dataset into a local variable.\")\n",
    "\n",
    "        print(f\"Dataset dimensions: {ds.dims}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        # --- 2. Store a list of datetimes from each file -> helps with retrieving 1 day's data later\n",
    "        all_times = []\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "        \n",
    "            # Decode byte strings and fix the format\n",
    "            xtime_strs = ds[\"xtime_startDaily\"].str.decode(\"utf-8\").values\n",
    "            xtime_strs = [s.replace(\"_\", \" \") for s in xtime_strs]  # \"0010-01-01_00:00:00\" → \"0010-01-01 00:00:00\"\n",
    "        \n",
    "            # Convert to datetime.datetime objects\n",
    "            times = [datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\") for s in xtime_strs]\n",
    "            all_times.extend(times)\n",
    "        \n",
    "        # Store in self.times\n",
    "        self.times = all_times\n",
    "        self.times = np.array(self.times, dtype='datetime64[s]')\n",
    "\n",
    "        # Checking the dates\n",
    "        print(f\"Parsed {len(self.times)} total dates\")\n",
    "        print(\"First few:\", self.times[:5])\n",
    "\n",
    "        print(f\"Total days collected: {len(self.times)}\")\n",
    "        print(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        print(f\"First 35 days: {self.times[:35]}\")\n",
    "        print(f\"First days 360 to 400 days: {self.times[360:401]}\")\n",
    "\n",
    "        # --- 3. Apply a mask to the nCells\n",
    "        # TODO: MASK DATA\n",
    "\n",
    "        # --- 4. Get patch IDs\n",
    "        # TODO: implement this\n",
    "        # self.patch_ids = ???\n",
    "\n",
    "        # --- 5. Derive Freeboard from ice area, snow volume and ice volume\n",
    "        self.freeboard_all = []\n",
    "        self.ice_area_all = []\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # Extract raw data\n",
    "            area = ds[\"timeDaily_avg_iceAreaCell\"].values\n",
    "            ice_volume = ds[\"timeDaily_avg_iceVolumeCell\"].values\n",
    "            snow_volume = ds[\"timeDaily_avg_snowVolumeCell\"].values\n",
    "\n",
    "            # Optional mask\n",
    "            if cell_mask is not None:\n",
    "                area = area[:, cell_mask]\n",
    "                ice_volume = ice_volume[:, cell_mask]\n",
    "                snow_volume = snow_volume[:, cell_mask]\n",
    "            \n",
    "            freeboard = compute_freeboard(area, ice_volume, snow_volume)\n",
    "\n",
    "            # These will be deleted later to save space\n",
    "            self.freeboard_all.append(freeboard) \n",
    "            self.ice_area_all.append(area)\n",
    "\n",
    "        \n",
    "        # --- 6. Normalize the data (TODO: IMPLEMENT THIS)\n",
    "\n",
    "        \n",
    "        # --- 7. Concatenate the data across Time\n",
    "\n",
    "        # Concatenate across time\n",
    "        self.freeboard = np.concatenate(self.freeboard_all, axis=0)  # (T, nCells)\n",
    "        self.ice_area = np.concatenate(self.ice_area_all, axis=0)    # (T, nCells)\n",
    "\n",
    "        # Discard the lists that are not needed anymore -- save space\n",
    "        del self.freeboard_all, self.ice_area_all\n",
    "\n",
    "        print(\"Freeboard\", self.freeboard.shape)\n",
    "        print(\"Ice Area\", self.ice_area.shape)\n",
    "\n",
    "        print(\"End of __init__\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns how many time steps? (Days for Daily data) \"\"\"\n",
    "        \n",
    "        print(\"Calling __len__\")\n",
    "        return len(self.times)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Select one time step (ex. 1 day). \n",
    "        It currently returns (features, timestamp) for a single day.\n",
    "        2. TODO: Return a set of patches for one time step\n",
    "        Features are: [freeboard, ice_area] over masked cells. \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # 1. Select timestep (day)\n",
    "        print(\"Calling __getitem__\")\n",
    "    \n",
    "        freeboard_day = self.freeboard[idx]  # shape: (nCells,)\n",
    "        ice_area_day = self.ice_area[idx]    # shape: (nCells,)\n",
    "        print(\"Freeboard shape\", freeboard_day.shape)\n",
    "        print(\"Ice Area shape\", ice_area_day.shape)\n",
    "        \n",
    "        features = np.stack([freeboard_day, ice_area_day], axis=0)  # shape: (2, nCells)\n",
    "        data_tensor = torch.as_tensor(features, dtype=torch.float32)\n",
    "    \n",
    "        if self.transform:\n",
    "            data_tensor = self.transform(data_tensor)\n",
    "            \n",
    "        print(f\"Fetched index {idx}: Time={self.times[idx]}, shape={data_tensor.shape}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        \n",
    "        return data_tensor, self.times[idx] # TODO: RETURN PATCHES INSTEAD OF ALL DATA PER DAY\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} days, \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{len(self.file_paths)} files loaded>\"\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class ===== \n",
      "Found 12 NetCDF files:\n",
      "Loading datasets with xarray.open_mfdataset...\n",
      "Finished loading full dataset into a local variable.\n",
      "Dataset dimensions: FrozenMappingWarningOnValuesAccess({'Time': 365, 'nCells': 465044, 'nVertices': 942873})\n",
      "Dataset variables: ['timeDaily_counter', 'xtime_startDaily', 'xtime_endDaily', 'timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell', 'timeDaily_avg_snowVolumeCell', 'timeDaily_avg_uVelocityGeo', 'timeDaily_avg_vVelocityGeo']\n",
      "Parsed 365 total dates\n",
      "First few: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00']\n",
      "Total days collected: 365\n",
      "Unique days: 365\n",
      "First 35 days: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00' '0010-01-06T00:00:00'\n",
      " '0010-01-07T00:00:00' '0010-01-08T00:00:00' '0010-01-09T00:00:00'\n",
      " '0010-01-10T00:00:00' '0010-01-11T00:00:00' '0010-01-12T00:00:00'\n",
      " '0010-01-13T00:00:00' '0010-01-14T00:00:00' '0010-01-15T00:00:00'\n",
      " '0010-01-16T00:00:00' '0010-01-17T00:00:00' '0010-01-18T00:00:00'\n",
      " '0010-01-19T00:00:00' '0010-01-20T00:00:00' '0010-01-21T00:00:00'\n",
      " '0010-01-22T00:00:00' '0010-01-23T00:00:00' '0010-01-24T00:00:00'\n",
      " '0010-01-25T00:00:00' '0010-01-26T00:00:00' '0010-01-27T00:00:00'\n",
      " '0010-01-28T00:00:00' '0010-01-29T00:00:00' '0010-01-30T00:00:00'\n",
      " '0010-01-31T00:00:00' '0010-02-01T00:00:00' '0010-02-02T00:00:00'\n",
      " '0010-02-03T00:00:00' '0010-02-04T00:00:00']\n",
      "First days 360 to 400 days: ['0010-12-27T00:00:00' '0010-12-28T00:00:00' '0010-12-29T00:00:00'\n",
      " '0010-12-30T00:00:00' '0010-12-31T00:00:00']\n",
      "Freeboard (365, 465044)\n",
      "Ice Area (365, 465044)\n",
      "End of __init__\n",
      "Elapsed time: 7.732597351074219 seconds\n",
      "===== Printing Dataset ===== \n",
      "Calling __len__\n",
      "<DailyNetCDFDataset: 365 days, 465044 cells/day, 12 files loaded>\n",
      "Calling __getitem__\n",
      "Freeboard shape (465044,)\n",
      "Ice Area shape (465044,)\n",
      "Fetched index 0: Time=0010-01-01T00:00:00, shape=torch.Size([2, 465044])\n",
      "Elapsed time: 0.012010335922241211 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"===== Making the Dataset Class ===== \")\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__ → see how many files & days loaded\n",
    "sample, ts = dataset[0]        # sample is tensor, ts is np.datetime64\n",
    "\n",
    "# wrap in a DataLoader\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# quickly get engineered time-features\n",
    "# df_time = dataset.time_to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35ba678-9162-4260-a2c8-0a27c331d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # THIS ONE IS AN OPTION FOR IF I IMPLEMENT PATCHES\n",
    "\n",
    "# class PatchTransformer(nn.Module):\n",
    "#     def __init__(self, patch_dim, num_patches, d_model=128, nhead=8, num_layers=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         print(\"Calling __init__\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         self.patch_embed = nn.Linear(patch_dim, d_model)  # input projection\n",
    "\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, num_patches, d_model))  # learnable positional encoding\n",
    "\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "#         self.mlp_head = nn.Sequential(\n",
    "#             nn.LayerNorm(d_model),\n",
    "#             nn.Linear(d_model, 1)  # regression or classification\n",
    "#         )\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "#         print(\"End of __init__\")\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (batch_size, num_patches, patch_dim)\n",
    "#         \"\"\"\n",
    "\n",
    "#         print(\"Calling forward\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         x = self.patch_embed(x) + self.pos_embed\n",
    "#         x = self.encoder(x)  # (batch_size, num_patches, d_model)\n",
    "\n",
    "#         # Option 1: Mean over tokens\n",
    "#         x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "\n",
    "#         # attn: shape (num_layers, num_heads, num_tokens, num_tokens)\n",
    "#         attn = self.transformer(..., output_attentions=True)\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "#         print(\"End of __init__\")\n",
    "\n",
    "#         return self.mlp_head(x)  # output shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf18cf-4c67-4a20-9c47-32d17d9a3205",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98bd27f0-13ca-4c77-890d-fa7524ce3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class IceForecastTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, model_dim, n_heads, n_layers, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads)\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "#         self.head = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch, seq_len, input_dim)\n",
    "#         x = self.input_proj(x)\n",
    "#         x = x.permute(1, 0, 2)  # (seq_len, batch, model_dim)\n",
    "#         x = self.transformer(x)\n",
    "#         x = x[-1]  # use final token for prediction\n",
    "#         # attn: shape (num_layers, num_heads, num_tokens, num_tokens)\n",
    "#         attn = self.transformer(..., output_attentions=True)\n",
    "#         return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57ee364-ef5f-466e-9d5a-9d7bc6b789f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import torch.optim as optim\n",
    "\n",
    "# num_epochs = 100\n",
    "\n",
    "# model = PatchTransformer(patch_dim=2, num_patches=100)  # TODO: adjust\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # This version expects patching to be done in the Dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x, _ in loader:  # x: (B, num_patches, patch_dim)\n",
    "#         y_pred = model(x)\n",
    "#         loss = criterion(y_pred, targets)  # you still need to define targets\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "TODO: Add Positional Encoding to represent time steps.\n",
    "\n",
    "TODO: Use patch embedding (like in Vision Transformers).\n",
    "\n",
    "TODO OPTION: Try temporal attention only (e.g., Informer, Time Series Transformer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sea Ice Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
