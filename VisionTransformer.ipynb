{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/homes/b/brelypo/.conda/envs/sic_sie_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.0\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    timeDaily_counter             (Time) int32 124B ...\n",
       "    xtime_startDaily              (Time) |S64 2kB ...\n",
       "    xtime_endDaily                (Time) |S64 2kB ...\n",
       "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
       "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"]\n",
    "day_counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'xtime_startDaily' (Time: 31)> Size: 2kB\n",
      "[31 values with dtype=|S64]\n",
      "Dimensions without coordinates: Time\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'0010-01-01_00:00:00' b'0010-01-02_00:00:00' b'0010-01-03_00:00:00'\n",
      " b'0010-01-04_00:00:00' b'0010-01-05_00:00:00' b'0010-01-06_00:00:00'\n",
      " b'0010-01-07_00:00:00' b'0010-01-08_00:00:00' b'0010-01-09_00:00:00'\n",
      " b'0010-01-10_00:00:00' b'0010-01-11_00:00:00' b'0010-01-12_00:00:00'\n",
      " b'0010-01-13_00:00:00' b'0010-01-14_00:00:00' b'0010-01-15_00:00:00'\n",
      " b'0010-01-16_00:00:00' b'0010-01-17_00:00:00' b'0010-01-18_00:00:00'\n",
      " b'0010-01-19_00:00:00' b'0010-01-20_00:00:00' b'0010-01-21_00:00:00'\n",
      " b'0010-01-22_00:00:00' b'0010-01-23_00:00:00' b'0010-01-24_00:00:00'\n",
      " b'0010-01-25_00:00:00' b'0010-01-26_00:00:00' b'0010-01-27_00:00:00'\n",
      " b'0010-01-28_00:00:00' b'0010-01-29_00:00:00' b'0010-01-30_00:00:00'\n",
      " b'0010-01-31_00:00:00']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 465044)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'Time': 31, 'nCells': 465044, 'nVertices': 942873})\n"
     ]
    }
   ],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 407MB\n",
      "Dimensions:                       (Time: 31, nCells: 465044, nVertices: 942873)\n",
      "Dimensions without coordinates: Time, nCells, nVertices\n",
      "Data variables:\n",
      "    timeDaily_counter             (Time) int32 124B ...\n",
      "    xtime_startDaily              (Time) |S64 2kB ...\n",
      "    xtime_endDaily                (Time) |S64 2kB ...\n",
      "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "Attributes: (12/490)\n",
      "    case:                                                         v3.LR.DTEST...\n",
      "    source_id:                                                    9741e0bba2\n",
      "    realm:                                                        seaIce\n",
      "    product:                                                      model-output\n",
      "    title:                                                        MPAS-Seaice...\n",
      "    source:                                                       E3SM Sea Ic...\n",
      "    ...                                                           ...\n",
      "    config_AM_timeSeriesStatsCustom_reference_times:              initial_time\n",
      "    config_AM_timeSeriesStatsCustom_duration_intervals:           repeat_inte...\n",
      "    config_AM_timeSeriesStatsCustom_repeat_intervals:             reset_interval\n",
      "    config_AM_timeSeriesStatsCustom_reset_intervals:              00-00-07_00...\n",
      "    config_AM_timeSeriesStatsCustom_backward_output_offset:       00-00-01_00...\n",
      "    file_id:                                                      smms2lytbk\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08a941da-41e7-4ffe-996c-d4554c5a4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "593ea2d5-1e69-44d0-a3aa-5fb8f206f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datetime import timedelta\n",
    "from NC_FILE_PROCESSING.nc_utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files (e.g. 202501.nc, 202502.nc, …).\n",
    "    variable_names : str | List[str]\n",
    "        Variable(s) to extract.  Default \"timeDaily_avg_iceAreaCell\".\n",
    "    transform : Callable | None\n",
    "        Optional transform applied to the data tensor *only*.\n",
    "    decode_time : bool\n",
    "        Let xarray convert CF-style time coordinates to np.datetime64.\n",
    "    drop_missing : bool\n",
    "        If True, drops any days where one of the requested variables is missing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        variable_names: Union[str, List[str]] = \"timeDaily_avg_iceAreaCell\",\n",
    "        transform: Callable = None,\n",
    "        decode_time: bool = True,\n",
    "        drop_missing: bool = True,\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.variable_names = (\n",
    "            [variable_names] if isinstance(variable_names, str) else variable_names\n",
    "        )\n",
    "\n",
    "        # --- 1. Gather month files (sorted for deterministic order) ---------\n",
    "        self.file_paths = sorted(\n",
    "            [\n",
    "                os.path.join(data_dir, f)\n",
    "                for f in os.listdir(data_dir)\n",
    "                if f.endswith(\".nc\")\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        for f in self.file_paths:\n",
    "            print(f\"  - {f}\")\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "\n",
    "        # --- 2. Load & concatenate along time --------------------------------\n",
    "        print(\"Loading datasets with xarray.open_mfdataset...\")\n",
    "\n",
    "        # --- 2. Load & concatenate along time --------------------------------\n",
    "        self.ds = xr.open_mfdataset(\n",
    "            self.file_paths,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=\"Time\",       # capital \"T\"\n",
    "            decode_times=False,\n",
    "            parallel=False,\n",
    "        )\n",
    "        \n",
    "        # List of datetimes from each file\n",
    "        all_times = []\n",
    "        \n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "        \n",
    "            # Decode byte strings and fix the format\n",
    "            xtime_strs = ds[\"xtime_startDaily\"].str.decode(\"utf-8\").values\n",
    "            xtime_strs = [s.replace(\"_\", \" \") for s in xtime_strs]  # \"0010-01-01_00:00:00\" → \"0010-01-01 00:00:00\"\n",
    "        \n",
    "            # Convert to datetime.datetime objects\n",
    "            times = [datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\") for s in xtime_strs]\n",
    "            all_times.extend(times)\n",
    "        \n",
    "        # Store in self.times (can remain as list or convert to np.datetime64 if needed)\n",
    "        self.times = all_times\n",
    "        self.times = np.array(self.times, dtype='datetime64[s]')\n",
    "        \n",
    "        # Optional check\n",
    "        print(f\"Parsed {len(self.times)} total dates\")\n",
    "        print(\"First few:\", self.times[:5])\n",
    "\n",
    "\n",
    "        print(f\"Total days collected: {len(self.times)}\")\n",
    "        print(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        print(f\"First 35 days: {self.times[:35]}\")\n",
    "\n",
    "        print(\"Finished loading dataset.\")\n",
    "\n",
    "        print(f\"Dataset dimensions: {self.ds.dims}\")\n",
    "        print(f\"Dataset variables: {list(self.ds.data_vars)}\")\n",
    "\n",
    "        # --- 3. Sub-select requested variables ------------------------------\n",
    "        missing = [v for v in self.variable_names if v not in self.ds]\n",
    "        if missing:\n",
    "            raise KeyError(f\"Variable(s) {missing} not found in dataset.\")\n",
    "\n",
    "        print(f\"Subsetting variables: {self.variable_names}\")\n",
    "        self.subset = self.ds[self.variable_names]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.times)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "        daily = self.subset.isel(Time=idx).to_array().values\n",
    "        data_tensor = torch.as_tensor(daily, dtype=torch.float32)\n",
    "\n",
    "        if data_tensor.shape[0] == 1:\n",
    "            data_tensor = data_tensor.squeeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            data_tensor = self.transform(data_tensor)\n",
    "\n",
    "        print(f\"Fetched index {idx}: Time={self.times[idx]}, shape={data_tensor.shape}\")\n",
    "        return data_tensor, self.times[idx]\n",
    "\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 NetCDF files:\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-02-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-03-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-04-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-05-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-06-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-07-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-08-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-09-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-10-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-11-01.nc\n",
      "  - /global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-12-01.nc\n",
      "Loading datasets with xarray.open_mfdataset...\n",
      "Parsed 365 total dates\n",
      "First few: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00']\n",
      "Total days collected: 365\n",
      "Unique days: 365\n",
      "First 35 days: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00' '0010-01-06T00:00:00'\n",
      " '0010-01-07T00:00:00' '0010-01-08T00:00:00' '0010-01-09T00:00:00'\n",
      " '0010-01-10T00:00:00' '0010-01-11T00:00:00' '0010-01-12T00:00:00'\n",
      " '0010-01-13T00:00:00' '0010-01-14T00:00:00' '0010-01-15T00:00:00'\n",
      " '0010-01-16T00:00:00' '0010-01-17T00:00:00' '0010-01-18T00:00:00'\n",
      " '0010-01-19T00:00:00' '0010-01-20T00:00:00' '0010-01-21T00:00:00'\n",
      " '0010-01-22T00:00:00' '0010-01-23T00:00:00' '0010-01-24T00:00:00'\n",
      " '0010-01-25T00:00:00' '0010-01-26T00:00:00' '0010-01-27T00:00:00'\n",
      " '0010-01-28T00:00:00' '0010-01-29T00:00:00' '0010-01-30T00:00:00'\n",
      " '0010-01-31T00:00:00' '0010-02-01T00:00:00' '0010-02-02T00:00:00'\n",
      " '0010-02-03T00:00:00' '0010-02-04T00:00:00']\n",
      "Finished loading dataset.\n",
      "Dataset dimensions: FrozenMappingWarningOnValuesAccess({'Time': 365, 'nCells': 465044, 'nVertices': 942873})\n",
      "Dataset variables: ['timeDaily_counter', 'xtime_startDaily', 'xtime_endDaily', 'timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell', 'timeDaily_avg_snowVolumeCell', 'timeDaily_avg_uVelocityGeo', 'timeDaily_avg_vVelocityGeo']\n",
      "Subsetting variables: ['timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell']\n",
      "<__main__.DailyNetCDFDataset object at 0x7f01bc336f20>\n",
      "Fetched index 0: Time=0010-01-01T00:00:00, shape=torch.Size([2, 465044])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"/global/u2/b/brelypo/python_model_visualization/Predicting_SIC_SIE/train\"\n",
    "dataset = DailyNetCDFDataset(\n",
    "    data_dir,\n",
    "    variable_names=[\"timeDaily_avg_iceAreaCell\", \"timeDaily_avg_iceVolumeCell\"],\n",
    ")\n",
    "\n",
    "print(dataset)                 # → see how many files & days loaded\n",
    "sample, ts = dataset[0]        # sample is tensor, ts is np.datetime64\n",
    "\n",
    "# wrap in a DataLoader as usual\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# quickly get engineered time-features if you want them numerically\n",
    "df_time = dataset.time_to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6c677-b7e6-4cc2-bb71-36aae89f23b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sea Ice Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
