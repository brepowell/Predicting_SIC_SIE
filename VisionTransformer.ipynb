{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.0\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40ead02e-ae14-41bb-a7fd-6242e11003f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count()) # check the number of available CUDA devices\n",
    "# will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor Name: x86_64\n",
      "Physical Cores: 128\n",
      "Logical Cores: 256\n",
      "Current CPU Frequency: 2249.30 MHz\n",
      "Min CPU Frequency: 1500.00 MHz\n",
      "Max CPU Frequency: 2000.00 MHz\n",
      "Total CPU Usage: 25.4%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd50d6-18a5-4f14-94c6-56d69814e0e6",
   "metadata": {},
   "source": [
    "# Example of one netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    timeDaily_counter             (Time) int32 124B ...\n",
       "    xtime_startDaily              (Time) |S64 2kB ...\n",
       "    xtime_endDaily                (Time) |S64 2kB ...\n",
       "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
       "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
       "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"]\n",
    "day_counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'xtime_startDaily' (Time: 31)> Size: 2kB\n",
      "[31 values with dtype=|S64]\n",
      "Dimensions without coordinates: Time\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'0010-01-01_00:00:00' b'0010-01-02_00:00:00' b'0010-01-03_00:00:00'\n",
      " b'0010-01-04_00:00:00' b'0010-01-05_00:00:00' b'0010-01-06_00:00:00'\n",
      " b'0010-01-07_00:00:00' b'0010-01-08_00:00:00' b'0010-01-09_00:00:00'\n",
      " b'0010-01-10_00:00:00' b'0010-01-11_00:00:00' b'0010-01-12_00:00:00'\n",
      " b'0010-01-13_00:00:00' b'0010-01-14_00:00:00' b'0010-01-15_00:00:00'\n",
      " b'0010-01-16_00:00:00' b'0010-01-17_00:00:00' b'0010-01-18_00:00:00'\n",
      " b'0010-01-19_00:00:00' b'0010-01-20_00:00:00' b'0010-01-21_00:00:00'\n",
      " b'0010-01-22_00:00:00' b'0010-01-23_00:00:00' b'0010-01-24_00:00:00'\n",
      " b'0010-01-25_00:00:00' b'0010-01-26_00:00:00' b'0010-01-27_00:00:00'\n",
      " b'0010-01-28_00:00:00' b'0010-01-29_00:00:00' b'0010-01-30_00:00:00'\n",
      " b'0010-01-31_00:00:00']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 465044)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e9d6581-da44-4d6f-9582-908d0a86581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(31, 465044), dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'Time': 31, 'nCells': 465044, 'nVertices': 942873})\n"
     ]
    }
   ],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 407MB\n",
      "Dimensions:                       (Time: 31, nCells: 465044, nVertices: 942873)\n",
      "Dimensions without coordinates: Time, nCells, nVertices\n",
      "Data variables:\n",
      "    timeDaily_counter             (Time) int32 124B ...\n",
      "    xtime_startDaily              (Time) |S64 2kB b'0010-01-01_00:00:00' ... ...\n",
      "    xtime_endDaily                (Time) |S64 2kB ...\n",
      "    timeDaily_avg_iceAreaCell     (Time, nCells) float32 58MB 0.0 0.0 ... 0.0\n",
      "    timeDaily_avg_iceVolumeCell   (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_snowVolumeCell  (Time, nCells) float32 58MB ...\n",
      "    timeDaily_avg_uVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "    timeDaily_avg_vVelocityGeo    (Time, nVertices) float32 117MB ...\n",
      "Attributes: (12/490)\n",
      "    case:                                                         v3.LR.DTEST...\n",
      "    source_id:                                                    9741e0bba2\n",
      "    realm:                                                        seaIce\n",
      "    product:                                                      model-output\n",
      "    title:                                                        MPAS-Seaice...\n",
      "    source:                                                       E3SM Sea Ic...\n",
      "    ...                                                           ...\n",
      "    config_AM_timeSeriesStatsCustom_reference_times:              initial_time\n",
      "    config_AM_timeSeriesStatsCustom_duration_intervals:           repeat_inte...\n",
      "    config_AM_timeSeriesStatsCustom_repeat_intervals:             reset_interval\n",
      "    config_AM_timeSeriesStatsCustom_reset_intervals:              00-00-07_00...\n",
      "    config_AM_timeSeriesStatsCustom_backward_output_offset:       00-00-01_00...\n",
      "    file_id:                                                      smms2lytbk\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e89844-e98f-4b42-b3cd-e3593ea3f151",
   "metadata": {},
   "source": [
    "# Example of Mesh File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "20a8b6f4-6969-45c9-8859-709234040954",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(\"NC_FILE_PROCESSING/mpassi.IcoswISC30E3r5.20231120.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80b4df70-fa5a-4c94-8be1-f93caae8209c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
       "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
       "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
       "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
       "    angleEdge          (nEdges) float64 11MB ...\n",
       "    dcEdge             (nEdges) float64 11MB ...\n",
       "    dvEdge             (nEdges) float64 11MB ...\n",
       "    indexToEdgeID      (nEdges) int32 6MB ...\n",
       "    latEdge            (nEdges) float64 11MB ...\n",
       "    lonEdge            (nEdges) float64 11MB ...\n",
       "    nEdgesOnEdge       (nEdges) int32 6MB ...\n",
       "    xEdge              (nEdges) float64 11MB ...\n",
       "    yEdge              (nEdges) float64 11MB ...\n",
       "    zEdge              (nEdges) float64 11MB ...\n",
       "    fEdge              (nEdges) float64 11MB ...\n",
       "    cellsOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    edgesOnVertex      (nVertices, vertexDegree) int32 11MB ...\n",
       "    kiteAreasOnVertex  (nVertices, vertexDegree) float64 23MB ...\n",
       "    areaTriangle       (nVertices) float64 8MB ...\n",
       "    indexToVertexID    (nVertices) int32 4MB ...\n",
       "    latVertex          (nVertices) float64 8MB ...\n",
       "    lonVertex          (nVertices) float64 8MB ...\n",
       "    xVertex            (nVertices) float64 8MB ...\n",
       "    yVertex            (nVertices) float64 8MB ...\n",
       "    zVertex            (nVertices) float64 8MB ...\n",
       "    fVertex            (nVertices) float64 8MB ...\n",
       "    cellsOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    edgesOnCell        (nCells, maxEdges) int32 11MB ...\n",
       "    verticesOnCell     (nCells, maxEdges) int32 11MB ...\n",
       "    areaCell           (nCells) float64 4MB ...\n",
       "    indexToCellID      (nCells) int32 2MB ...\n",
       "    latCell            (nCells) float64 4MB ...\n",
       "    lonCell            (nCells) float64 4MB ...\n",
       "    meshDensity        (nCells) float64 4MB ...\n",
       "    nEdgesOnCell       (nCells) int32 2MB ...\n",
       "    xCell              (nCells) float64 4MB ...\n",
       "    yCell              (nCells) float64 4MB ...\n",
       "    zCell              (nCells) float64 4MB ...\n",
       "    fCell              (nCells) float64 4MB ...\n",
       "    landIceMask        (Time, nCells) int32 2MB ..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40476985-dfb0-4da2-b48e-eb2c97f74f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     5      4      0      0      0      0]\n",
      " [    12     11      9      8      0      3]\n",
      " [     4     13     12      2      0      0]\n",
      " ...\n",
      " [465043      0 465040 465041      0      0]\n",
      " [     0 465042      0 465044      0      0]\n",
      " [     0      0      0      0 465043      0]]\n"
     ]
    }
   ],
   "source": [
    "cellsOnCell = mesh[\"cellsOnCell\"].values\n",
    "print(mesh[\"cellsOnCell\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af6cbd2b-ae4f-41d4-b8c1-213fb92b3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465044\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mesh[\"cellsOnCell\"].max().values)\n",
    "print(mesh[\"cellsOnCell\"].min().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21202b02-d87e-4354-aea7-586babbfb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cellsOnCell.npy', cellsOnCell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e0aab3b-733a-4f9f-ad63-e0a510b05cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "    *empty*\n",
      "FrozenMappingWarningOnValuesAccess({'nEdges': 1408196, 'maxEdges2': 12, 'TWO': 2, 'nVertices': 942873, 'vertexDegree': 3, 'nCells': 465044, 'maxEdges': 6, 'Time': 1})\n"
     ]
    }
   ],
   "source": [
    "print(mesh.coords)\n",
    "print(mesh.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a816fde5-ec19-4430-9875-f4e8cadc10d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 509MB\n",
      "Dimensions:            (nEdges: 1408196, maxEdges2: 12, TWO: 2,\n",
      "                        nVertices: 942873, vertexDegree: 3, nCells: 465044,\n",
      "                        maxEdges: 6, Time: 1)\n",
      "Dimensions without coordinates: nEdges, maxEdges2, TWO, nVertices,\n",
      "                                vertexDegree, nCells, maxEdges, Time\n",
      "Data variables: (12/40)\n",
      "    edgesOnEdge        (nEdges, maxEdges2) int32 68MB ...\n",
      "    weightsOnEdge      (nEdges, maxEdges2) float64 135MB ...\n",
      "    cellsOnEdge        (nEdges, TWO) int32 11MB ...\n",
      "    verticesOnEdge     (nEdges, TWO) int32 11MB ...\n",
      "    angleEdge          (nEdges) float64 11MB ...\n",
      "    dcEdge             (nEdges) float64 11MB ...\n",
      "    ...                 ...\n",
      "    nEdgesOnCell       (nCells) int32 2MB ...\n",
      "    xCell              (nCells) float64 4MB ...\n",
      "    yCell              (nCells) float64 4MB ...\n",
      "    zCell              (nCells) float64 4MB ...\n",
      "    fCell              (nCells) float64 4MB ...\n",
      "    landIceMask        (Time, nCells) int32 2MB ...\n",
      "Attributes: (12/1313)\n",
      "    model_name:                                                      mpas\n",
      "    core_name:                                                       ocean\n",
      "    source:                                                          MPAS\n",
      "    Conventions:                                                     MPAS\n",
      "    git_version:                                                     v2.1.0-1...\n",
      "    on_a_sphere:                                                     YES\n",
      "    ...                                                              ...\n",
      "    MPAS_Mesh_NCO_Version:                                           5.1.9\n",
      "    MPAS_Mesh_ESMF_Version:                                          8.4.2\n",
      "    MPAS_Mesh_geometric_features_Version:                            1.3.0\n",
      "    MPAS_Mesh_Metis_Version:                                         5.1.1\n",
      "    MPAS_Mesh_pyremap_Version:                                       1.2.0\n",
      "    NCO:                                                             netCDF O...\n"
     ]
    }
   ],
   "source": [
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fff3ae1-5dc1-4ac7-af6d-eaa97bee17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Pre-processing + Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_AREA = 1e-6\n",
    "\n",
    "def compute_freeboard(area: np.ndarray, \n",
    "                      ice_volume: np.ndarray, \n",
    "                      snow_volume: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow volume and area.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    area : np.ndarray\n",
    "        Sea ice concentration / area (same shape as ice_volume and snow_volume).\n",
    "    ice_volume : np.ndarray\n",
    "        Sea ice volume per grid cell.\n",
    "    snow_volume : np.ndarray\n",
    "        Snow volume per grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : np.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize arrays\n",
    "    height_ice = np.zeros_like(ice_volume)\n",
    "    height_snow = np.zeros_like(snow_volume)\n",
    "\n",
    "    # Valid mask: avoid dividing by very small or zero area\n",
    "    valid = area > MIN_AREA\n",
    "\n",
    "    # Safely compute heights where valid\n",
    "    height_ice[valid] = ice_volume[valid] / area[valid]\n",
    "    height_snow[valid] = snow_volume[valid] / area[valid]\n",
    "\n",
    "    # Compute freeboard using the physical formula\n",
    "    freeboard = (\n",
    "        height_ice * (D_WATER - D_ICE) / D_WATER +\n",
    "        height_snow * (D_WATER - D_SNOW) / D_WATER\n",
    "    )\n",
    "\n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9a739e27-622f-4a24-b031-270622c5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freeboard(freeboard, min_val=-0.2, max_val=1.2):\n",
    "    return np.clip((freeboard - min_val) / (max_val - min_val), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7df3ff-a8ea-46cc-9053-97bfbb41cea8",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "\n",
    "TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "\n",
    "TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "\n",
    "TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "Too few workers: GPUs might become idle waiting for data.\n",
    "Too many workers: Can lead to increased CPU memory usage and context switching overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "646812d0-b13a-4fff-a261-9fb5f6835cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 64\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple\n",
    "from NC_FILE_PROCESSING.patchify_utils import patchify_by_latlon_spillover\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional transform applied to the data tensor *only*.\n",
    "    decode_time : bool\n",
    "        Let xarray convert CF-style time coordinates to np.datetime64.\n",
    "    drop_missing : bool\n",
    "        If True, drops any days where one of the requested variables is missing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        mesh_dir: str = mesh_dir,\n",
    "        transform: Callable = None,\n",
    "        decode_time: bool = True,\n",
    "        drop_missing: bool = True,\n",
    "        latitude_threshold = 40\n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "\n",
    "        Handle the raw data:\n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature (ice area, ice volume, etc.)\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Store the datetime information from each nCells array from the daily data\n",
    "        3) Extract raw data\n",
    "        \n",
    "        Perform pre-processing:\n",
    "        4) Apply a mask to nCells to look just at regions in certain latitudes\n",
    "            nCells >= 40 degrees is 53973 cells\n",
    "            nCells >= 50 degrees is 35623 cells\n",
    "        5) Derive Freeboard from ice area, snow volume, and ice volume\n",
    "        6) Custom patchify and store patch_ids so the data loader can use them (TODO: IMPLEMENT THIS)\n",
    "        7) Concatenate the data across Time\n",
    "        8) Normalize the data (Ice area is already between 0 and 1; Freeboard is not) \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.transform = transform\n",
    "\n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        self.data_dir = data_dir\n",
    "        self.file_paths = sorted(\n",
    "            [\n",
    "                os.path.join(data_dir, f)\n",
    "                for f in os.listdir(data_dir)\n",
    "                if f.endswith(\".nc\")\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        # for f in self.file_paths:\n",
    "        #     print(f\"  - {f}\")     # Print all the file names in the folder\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "\n",
    "        # Open all the netCDF files and concatenate them along Time dimension\n",
    "        print(\"Loading datasets with xarray.open_mfdataset...\")\n",
    "        \n",
    "        ds = xr.open_mfdataset(\n",
    "            self.file_paths,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=\"Time\", # Use the NetCDF's Time dimension for concatenation\n",
    "            decode_times=False,\n",
    "            parallel=False,\n",
    "        )\n",
    "\n",
    "        print(\"Finished loading full dataset into a local variable.\")\n",
    "\n",
    "        print(f\"Dataset dimensions: {ds.dims}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        # --- 2. Store a list of datetimes from each file -> helps with retrieving 1 day's data later\n",
    "        all_times = []\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "        \n",
    "            # Decode byte strings and fix the format\n",
    "            xtime_strs = ds[\"xtime_startDaily\"].str.decode(\"utf-8\").values\n",
    "            xtime_strs = [s.replace(\"_\", \" \") for s in xtime_strs]  # \"0010-01-01_00:00:00\" → \"0010-01-01 00:00:00\"\n",
    "        \n",
    "            # Convert to datetime.datetime objects\n",
    "            times = [datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\") for s in xtime_strs]\n",
    "            all_times.extend(times)\n",
    "        \n",
    "        # Store in self.times\n",
    "        self.times = all_times\n",
    "        self.times = np.array(self.times, dtype='datetime64[s]')\n",
    "\n",
    "        # Checking the dates\n",
    "        print(f\"Parsed {len(self.times)} total dates\")\n",
    "        print(\"First few:\", self.times[:5])\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        print(f\"Total days collected: {len(self.times)}\")\n",
    "        print(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        print(f\"First 35 days: {self.times[:35]}\")\n",
    "        print(f\"First days 360 to 400 days: {self.times[360:401]}\")\n",
    "\n",
    "        # Load the mesh file. Latitudes and Longitudes are in radians.\n",
    "        mesh = xr.open_dataset(mesh_dir)\n",
    "        latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "        lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "        self.cell_mask = latCell >= latitude_threshold\n",
    "        print(\"Mask size: \", np.count_nonzero(self.cell_mask))\n",
    "\n",
    "        self.full_to_masked = {\n",
    "            full_idx: new_idx\n",
    "            for new_idx, full_idx in enumerate(np.where(self.cell_mask)[0])\n",
    "        }\n",
    "\n",
    "        # --- 3. Extract raw data \n",
    "        self.freeboard_all = []\n",
    "        self.ice_area_all = []\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # Extract raw data\n",
    "            area = ds[\"timeDaily_avg_iceAreaCell\"].values\n",
    "            ice_volume = ds[\"timeDaily_avg_iceVolumeCell\"].values\n",
    "            snow_volume = ds[\"timeDaily_avg_snowVolumeCell\"].values\n",
    "\n",
    "            # --- 4. Apply a mask to the nCells\n",
    "            area = area[:, self.cell_mask]\n",
    "            ice_volume = ice_volume[:, self.cell_mask]\n",
    "            snow_volume = snow_volume[:, self.cell_mask]\n",
    "\n",
    "            # --- 5. Derive Freeboard from ice area, snow volume and ice volume\n",
    "            freeboard = compute_freeboard(area, ice_volume, snow_volume)\n",
    "\n",
    "            # These will be deleted later to save space\n",
    "            self.freeboard_all.append(freeboard) \n",
    "            self.ice_area_all.append(area)\n",
    "\n",
    "        # --- 6. Custom patchify function       \n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id = patchify_by_latlon_spillover(\n",
    "            latCell, lonCell, k=256, max_patches=140, lat_threshold=latitude_threshold)\n",
    "\n",
    "\n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "\n",
    "        # --- 7. Concatenate the data across Time\n",
    "\n",
    "        # Concatenate across time\n",
    "        self.freeboard = np.concatenate(self.freeboard_all, axis=0)  # (T, nCells)\n",
    "        self.ice_area = np.concatenate(self.ice_area_all, axis=0)    # (T, nCells)\n",
    "\n",
    "        # Discard the lists that are not needed anymore -- save space\n",
    "        del self.freeboard_all, self.ice_area_all\n",
    "\n",
    "        print(\"Freeboard\", self.freeboard.shape)\n",
    "        print(\"Ice Area\", self.ice_area.shape)\n",
    "\n",
    "        # --- 8. Normalize the data (Area is already between 0 and 1; Freeboard is not)\n",
    "        self.freeboard_min = self.freeboard[0].min()\n",
    "        self.freeboard_max = self.freeboard[0].max()\n",
    "        \n",
    "        print(\"Freeboard min:\", self.freeboard_min)\n",
    "        print(\"Freeboard max:\", self.freeboard_max)\n",
    "\n",
    "        self.freeboard_all = normalize_freeboard(\n",
    "            freeboard, min_val=self.freeboard_min, max_val=self.freeboard_max)\n",
    "\n",
    "        print(\"=== Normalized Freeboard ===\")\n",
    "        \n",
    "        print(\"End of __init__\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns how many time steps? (Days for Daily data) \"\"\"\n",
    "        \n",
    "        print(\"Calling __len__\")\n",
    "        return len(self.times)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Select one time step (ex. 1 day). \n",
    "        2. Return all the patches for one time step\n",
    "        Features are: [freeboard, ice_area] over masked cells. \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"Calling __getitem__\")\n",
    "        \n",
    "        freeboard_day = self.freeboard[idx]  # shape: (nCells,)\n",
    "        ice_area_day = self.ice_area[idx]    # shape: (nCells,)\n",
    "        print(\"Freeboard shape\", freeboard_day.shape)\n",
    "        print(\"Ice Area shape\", ice_area_day.shape)\n",
    "\n",
    "        features = np.stack([freeboard_day, ice_area_day], axis=0)  # shape: (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # shape: (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        patch_tensor = torch.stack(patch_tensors)  # shape: (num_patches, 2, patch_size)\n",
    "\n",
    "        if self.transform:\n",
    "            patch_tensor = self.transform(patch_tensor)\n",
    "\n",
    "        print(f\"Fetched index {idx}: Time={self.times[idx]}\")\n",
    "        print(f\"[num_patches, num_features, patch_size]={patch_tensor.size()}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        return patch_tensor, self.times[idx]  # shape: (num_patches, 2, patch_size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} days, \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{len(self.file_paths)} files loaded>\"\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class ===== \n",
      "Found 12 NetCDF files:\n",
      "Loading datasets with xarray.open_mfdataset...\n",
      "Finished loading full dataset into a local variable.\n",
      "Dataset dimensions: FrozenMappingWarningOnValuesAccess({'Time': 365, 'nCells': 465044, 'nVertices': 942873})\n",
      "Dataset variables: ['timeDaily_counter', 'xtime_startDaily', 'xtime_endDaily', 'timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell', 'timeDaily_avg_snowVolumeCell', 'timeDaily_avg_uVelocityGeo', 'timeDaily_avg_vVelocityGeo']\n",
      "Parsed 365 total dates\n",
      "First few: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00']\n",
      "Total days collected: 365\n",
      "Unique days: 365\n",
      "First 35 days: ['0010-01-01T00:00:00' '0010-01-02T00:00:00' '0010-01-03T00:00:00'\n",
      " '0010-01-04T00:00:00' '0010-01-05T00:00:00' '0010-01-06T00:00:00'\n",
      " '0010-01-07T00:00:00' '0010-01-08T00:00:00' '0010-01-09T00:00:00'\n",
      " '0010-01-10T00:00:00' '0010-01-11T00:00:00' '0010-01-12T00:00:00'\n",
      " '0010-01-13T00:00:00' '0010-01-14T00:00:00' '0010-01-15T00:00:00'\n",
      " '0010-01-16T00:00:00' '0010-01-17T00:00:00' '0010-01-18T00:00:00'\n",
      " '0010-01-19T00:00:00' '0010-01-20T00:00:00' '0010-01-21T00:00:00'\n",
      " '0010-01-22T00:00:00' '0010-01-23T00:00:00' '0010-01-24T00:00:00'\n",
      " '0010-01-25T00:00:00' '0010-01-26T00:00:00' '0010-01-27T00:00:00'\n",
      " '0010-01-28T00:00:00' '0010-01-29T00:00:00' '0010-01-30T00:00:00'\n",
      " '0010-01-31T00:00:00' '0010-02-01T00:00:00' '0010-02-02T00:00:00'\n",
      " '0010-02-03T00:00:00' '0010-02-04T00:00:00']\n",
      "First days 360 to 400 days: ['0010-12-27T00:00:00' '0010-12-28T00:00:00' '0010-12-29T00:00:00'\n",
      " '0010-12-30T00:00:00' '0010-12-31T00:00:00']\n",
      "Mask size:  53973\n",
      "Built 140 patches of size ~256\n",
      "Cluster sizes:\n",
      "min size 256\n",
      "max size 429204\n",
      "smallest count (np.int64(0), 256)\n",
      "max count (np.int64(-1), 429204)\n",
      "number of patches: 141\n",
      "Freeboard (365, 53973)\n",
      "Ice Area (365, 53973)\n",
      "Freeboard min: 0.0\n",
      "Freeboard max: 1.7908666\n",
      "=== Normalized Freeboard ===\n",
      "End of __init__\n",
      "Elapsed time: 5.092681646347046 seconds\n",
      "===== Printing Dataset ===== \n",
      "Calling __len__\n",
      "<DailyNetCDFDataset: 365 days, 53973 cells/day, 12 files loaded>\n",
      "Calling __getitem__\n",
      "Freeboard shape (53973,)\n",
      "Ice Area shape (53973,)\n",
      "Fetched index 0: Time=0010-01-01T00:00:00\n",
      "[num_patches, num_features, patch_size]=torch.Size([140, 2, 256])\n",
      "Elapsed time: 0.008283376693725586 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"===== Making the Dataset Class ===== \")\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__ → see how many files & days loaded\n",
    "sample, ts = dataset[0]        # sample is tensor, ts is np.datetime64\n",
    "\n",
    "# wrap in a DataLoader\n",
    "# 1. use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                    num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "# quickly get engineered time-features\n",
    "# df_time = dataset.time_to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "<!-- outputs = model(features)\n",
    "model.train()\n",
    "model.eval() -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e35ba678-9162-4260-a2c8-0a27c331d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # THIS ONE IS AN OPTION FOR IF I IMPLEMENT PATCHES\n",
    "\n",
    "# class PatchTransformer(nn.Module):\n",
    "#     def __init__(self, patch_dim, num_patches, d_model=128, nhead=8, num_layers=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         print(\"Calling __init__\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         self.patch_embed = nn.Linear(patch_dim, d_model)  # input projection\n",
    "\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, num_patches, d_model))  # learnable positional encoding\n",
    "\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "#         self.mlp_head = nn.Sequential(\n",
    "#             nn.LayerNorm(d_model),\n",
    "#             nn.Linear(d_model, 1)  # regression or classification\n",
    "#         )\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "#         print(\"End of __init__\")\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (batch_size, num_patches, patch_dim)\n",
    "#         \"\"\"\n",
    "\n",
    "#         print(\"Calling forward\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         x = self.patch_embed(x) + self.pos_embed\n",
    "#         x = self.encoder(x)  # (batch_size, num_patches, d_model)\n",
    "\n",
    "#         # Option 1: Mean over tokens\n",
    "#         x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "\n",
    "#         # attn: shape (num_layers, num_heads, num_tokens, num_tokens)\n",
    "#         attn = self.transformer(..., output_attentions=True)\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "#         print(\"End of __init__\")\n",
    "\n",
    "#         return self.mlp_head(x)  # output shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98bd27f0-13ca-4c77-890d-fa7524ce3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class IceForecastTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, model_dim, n_heads, n_layers, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads)\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "#         self.head = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch, seq_len, input_dim)\n",
    "#         x = self.input_proj(x)\n",
    "#         x = x.permute(1, 0, 2)  # (seq_len, batch, model_dim)\n",
    "#         x = self.transformer(x)\n",
    "#         x = x[-1]  # use final token for prediction\n",
    "#         # attn: shape (num_layers, num_heads, num_tokens, num_tokens)\n",
    "#         attn = self.transformer(..., output_attentions=True)\n",
    "#         return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b57ee364-ef5f-466e-9d5a-9d7bc6b789f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import torch.optim as optim\n",
    "\n",
    "# num_epochs = 100\n",
    "\n",
    "# model = PatchTransformer(patch_dim=2, num_patches=100)  # TODO: adjust\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss() # loss function\n",
    "\n",
    "# # This version expects patching to be done in the Dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x, _ in loader:  # x: (B, num_patches, patch_dim)\n",
    "#         y_pred = model(x)\n",
    "#         loss = criterion(y_pred, targets)  # you still need to define targets\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "TODO: Add Positional Encoding to represent time steps.\n",
    "\n",
    "TODO: Use patch embedding (like in Vision Transformers).\n",
    "\n",
    "TODO OPTION: Try temporal attention only (e.g., Informer, Time Series Transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d61b69c0-01b8-491a-b71b-79ac6a7798d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msea_ice_concentration_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model's state_dict\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), PATH)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the model\n",
    "PATH = \"sea_ice_concentration_model.pth\"\n",
    "\n",
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Retrieving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalizing for min-max\n",
    "# original = normalized * (max_val - min_val) + min_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sea Ice Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
