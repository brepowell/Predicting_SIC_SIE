{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xarray version 2025.6.1\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print('Xarray version', xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9c1711-4d37-49fe-aa18-76a04aa25569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perlmutterpath import *  # Contains the data_dir and mesh_dir variables\n",
    "NUM_FEATURES = 2              # C: Number of features per cell (ex., Freeboard, Ice Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb8f648-272a-4897-bd50-fc2e9aa308c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "\n",
    "# Available patchify functions\n",
    "PATCHIFY_FUNCTIONS = {\n",
    "    \"agglomerative\": compute_agglomerative_patches,\n",
    "    \"breadth_first_bfs_basic\": build_patches_from_seeds_bfs_basic,\n",
    "    \"breadth_first_improved_padded\": build_patches_from_seeds_improved_padded,\n",
    "    \"dbscan\": get_clusters_dbscan,\n",
    "    \"kmeans\": cluster_patches_kmeans,\n",
    "    \"knn_basic\": compute_knn_patches,\n",
    "    \"knn_disjoint\": compute_disjoint_knn_patches,\n",
    "    \"latlon_spillover\": patchify_by_latlon_spillover,              # RELIABLE PERFORMANCE\n",
    "    \"latitude_neighbors\": patchify_by_latitude,\n",
    "    \"latitude_simple\": patchify_by_latitude_simple,\n",
    "    \"latitude_spillover_redo\": patchify_with_spillover,            # checking\n",
    "    \"lon_spilldown\": patchify_by_lon_spilldown,                    # TERRIBLE PERFORMANCE\n",
    "    \"rows\": get_rows_of_patches,                                   # RELIABLE PERFORMANCE\n",
    "    \"staggered_polar_descent\": patchify_staggered_polar_descent,\n",
    "}\n",
    "\n",
    "PATCHIFY_ABBREVIATIONS = {\n",
    "    \"agglomerative\": \"AGG\",\n",
    "    \"breadth_first_bfs_basic\": \"BFSB\",\n",
    "    \"breadth_first_improved_padded\": \"BPIP\",\n",
    "    \"dbscan\": \"DBSCAN\",\n",
    "    \"kmeans\": \"KM\",\n",
    "    \"knn_basic\": \"KNN\",\n",
    "    \"knn_disjoint\": \"DKNN\",\n",
    "    \"latlon_spillover\": \"LLSO\",               # RELIABLE PERFORMANCE\n",
    "    \"latitude_neighbors\": \"LAT\",\n",
    "    \"latitude_simple\": \"LSIM\",\n",
    "    \"latitude_spillover_redo\": \"PSO\",         # Uses PSO (Patchify SpillOver) - TRY THIS\n",
    "    \"lon_spilldown\": \"LSD\",                   # lOW PERFORMING\n",
    "    \"rows\": \"ROW\",                            # RELIABLE PERFORMANCE\n",
    "    \"staggered_polar_descent\": \"SPD\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0706a6-e559-4b22-9584-7e0911854542",
   "metadata": {},
   "source": [
    "# Variables for the Model\n",
    "\n",
    "Check over these CAREFULLY!\n",
    "\n",
    "Note that if you use the login node for training for the Jupyter notebook version (even for the trial dataset that is much smaller), you run the risk of getting the error: # OutOfMemoryError: CUDA out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166144bc-da7c-4a65-b45b-3c666d2be4a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Variables"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Run Settings:\n",
    "PATCHIFY_TO_USE = \"rows\"   # Change this to use other patching techniques\n",
    "#PATCHIFY_TO_USE = os.environ.get(\"SLURM_PATCHIFY_TO_USE\", \"rows\") # for SLURM\n",
    "\n",
    "TRIAL_RUN =                    False   # SET THIS TO USE THE PRACTICE SET (MUCH FASTER AND SMALLER, for debugging)\n",
    "NORMALIZE_ON =                 True    # SET THIS TO USE NORMALIZATION ON FREEBOARD (Results are independent of patchify used)\n",
    "TRAINING =                     True    # SET THIS TO RUN THE TRAINING LOOP (Use on full dataset for results)\n",
    "EVALUATING_ON =                False    # SET THIS TO RUN THE METRICS AT THE BOTTOM (Use on full dataset for results)\n",
    "PLOT_DAY_BY_DAY_METRICS =      False    # See a comparison of metrics per forecast day\n",
    "\n",
    "# Only run ONCE!!\n",
    "PLOT_DATA_SPLIT_DISTRIBUTION = False    # Run the data split function to see the train, val, test distribution\n",
    "\n",
    "# Run Settings (already performed, not needed now - KEEP FALSE!!!)\n",
    "PLOT_DATA_FULL_DISTRIBUTION = False   # SET THIS TO PLOT THE OUTLIERS (Run ONCE with full set. Results are independent of variables set here)\n",
    "MAX_FREEBOARD_ON =            False   # To normalize with a pre-defined maximum for outlier handling\n",
    "MAP_WITH_CARTOPY_ON =         False   # Make sure the Cartopy library is included in the kernel\n",
    "\n",
    "# --- Time-Related Variables:\n",
    "CONTEXT_LENGTH = 7            # T: Number of historical time steps used for input\n",
    "FORECAST_HORIZON = 7          # Number of future time steps to predict (ex. 1 day for next time step)\n",
    "\n",
    "# --- Model Hyperparameters:\n",
    "D_MODEL = 128                 # d_model: Dimension of the transformer's internal representations (embedding dimension)\n",
    "N_HEAD = 8                    # nhead: Number of attention heads\n",
    "NUM_TRANSFORMER_LAYERS = 4    # num_layers: Number of TransformerEncoderLayers\n",
    "BATCH_SIZE = 8                # 16 for context/forecast of 7 and 3; lower for longer range\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# --- Performance-Related Variables:\n",
    "NUM_WORKERS = 8   # changed from 64\n",
    "\n",
    "# --- Feature-Related Variables:\n",
    "MAX_FREEBOARD_FOR_NORMALIZATION = 1    # Only works when you set MAX_FREEBOARD_ON too; bad results\n",
    "\n",
    "# --- Space-Related Variables:\n",
    "LATITUDE_THRESHOLD = 40          # Determines number of cells and patches (could use -90 to use the entire dataset).\n",
    "CELLS_PER_PATCH = 256            # L: Number of cells within each patch (based on ViT paper 16 x 16 = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38d745-255f-4af4-8869-521733fd3b72",
   "metadata": {},
   "source": [
    "## Other Variables Dependent on Those Above ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60335920-4778-48aa-9cf4-d8a19332bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nCells:        465044\n",
      "Mask size:           53973\n",
      "cells_per_patch:     256\n",
      "n_patches:           210\n"
     ]
    }
   ],
   "source": [
    "mesh = xr.open_dataset(mesh_dir)\n",
    "latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "mesh.close()\n",
    "print(\"Total nCells:       \", len(latCell))\n",
    "\n",
    "mask = latCell >= LATITUDE_THRESHOLD\n",
    "masked_ncells_size = np.count_nonzero(mask)\n",
    "print(\"Mask size:          \", masked_ncells_size)\n",
    "\n",
    "NUM_PATCHES = masked_ncells_size // CELLS_PER_PATCH    # P: Approximate number of spatial patches to expect\n",
    "\n",
    "print(\"cells_per_patch:    \", CELLS_PER_PATCH)\n",
    "print(\"n_patches:          \", NUM_PATCHES)\n",
    "\n",
    "# The input dimension for the patch embedding linear layer.\n",
    "# Each patch at a given time step has NUM_FEATURES * CELLS_PER_PATCH features.\n",
    "# This is the 'D' dimension used in the Transformer's input tensor (B, T, P, D).\n",
    "PATCH_EMBEDDING_INPUT_DIM = NUM_FEATURES * CELLS_PER_PATCH # 2 * 256 = 512\n",
    "\n",
    "DEFAULT_PATCHIFY_METHOD_FUNC = PATCHIFY_FUNCTIONS[PATCHIFY_TO_USE]\n",
    "\n",
    "# --- Common Parameters for all functions ---\n",
    "COMMON_PARAMS = {\n",
    "    \"latCell\": latCell,\n",
    "    \"lonCell\": lonCell,\n",
    "    \"cells_per_patch\": CELLS_PER_PATCH, \n",
    "    \"num_patches\": NUM_PATCHES,\n",
    "    \"latitude_threshold\": LATITUDE_THRESHOLD,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "cellsOnCell = np.load(f'cellsOnCell.npy')\n",
    "\n",
    "# --- Function-specific Parameters (if any) ---\n",
    "SPECIFIC_PARAMS = {\n",
    "    \"latitude_spillover_redo\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"latitude_simple\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"latitude_neighbors\": {\"step_deg\": 5, \"max_lat\": 90},\n",
    "    \"breadth_first_improved_padded\": {\"cellsOnCell\": cellsOnCell, \"pad_to_exact_size\": True},\n",
    "    \"breadth_first_bfs_basic\": {\"cellsOnCell\": cellsOnCell},\n",
    "    \"agglomerative\": {\"n_neighbors\": 5},\n",
    "    \"kmeans\": {},\n",
    "    \"dbscan\": {},\n",
    "    \"rows\": {},\n",
    "    \"knn_basic\": {},\n",
    "    \"knn_disjoint\": {},\n",
    "    \"latlon_spillover\": {},\n",
    "    \"lon_spilldown\": {},\n",
    "    \"staggered_polar_descent\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a1e686-766a-49bd-b27c-d851e180a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Version: fd_nTM_D128_B8_lt40_P210_L256_T7_Fh7_e10_ROW\n",
      "Dataset Name: fd_nTM_D128_B8_lt40_P210_L256_T7_Fh7_e10_ROW_Dataset.zarr\n"
     ]
    }
   ],
   "source": [
    "if TRIAL_RUN:\n",
    "    model_mode = \"tr\" # Training Dataset\n",
    "else:\n",
    "    model_mode = \"fd\" # Full Dataset\n",
    "\n",
    "if NORMALIZE_ON:\n",
    "    if MAX_FREEBOARD_ON:\n",
    "        norm = f\"nT{MAX_FREEBOARD_FOR_NORMALIZATION}\" # Using the specified max value\n",
    "    else:\n",
    "        norm = \"nTM\" # Using the absolute max\n",
    "else:\n",
    "    norm = \"nF\" # Normalization is off\n",
    "\n",
    "# Get the abbreviation, with a fallback for functions not yet mapped\n",
    "patching_strategy_abbr = PATCHIFY_ABBREVIATIONS.get(PATCHIFY_TO_USE, \"UNKNWN\")\n",
    "\n",
    "if patching_strategy_abbr == \"UNKNWN\":\n",
    "    raise ValueError(\"Check the name of the patchify function\")\n",
    "                   \n",
    "# Model nome convention - fd:full data, etc.\n",
    "model_version = (\n",
    "    f\"{model_mode}_{norm}_D{D_MODEL}_B{BATCH_SIZE}_lt{LATITUDE_THRESHOLD}_P{NUM_PATCHES}_L{CELLS_PER_PATCH}\"\n",
    "    f\"_T{CONTEXT_LENGTH}_Fh{FORECAST_HORIZON}_e{NUM_EPOCHS}_{patching_strategy_abbr}\"\n",
    ")\n",
    "\n",
    "# Place to save and load the data\n",
    "PROCESSED_DATA_DIR = (\n",
    "    f\"{model_version}_Dataset.zarr\"\n",
    ")\n",
    "\n",
    "print(f\"Model Version: {model_version}\")\n",
    "print(f\"Dataset Name: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11cf9a-a867-4740-9d75-36f580e8d386",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "* TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "* TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "* TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "* TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "* Too few workers: GPUs might become idle waiting for data.\n",
    "* Too many workers: Can lead to increased CPU memory usage and context switching overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# More Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version 3.10.5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca026cf-05df-431c-9928-7284e71696c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn version 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print('Seaborn version', sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d15e35-d5b4-41e2-956d-95b6504edcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "if TRAINING and not torch.cuda.is_available():\n",
    "    raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "else:\n",
    "    print(torch.cuda.device_count()) # check the number of available CUDA devices\n",
    "    # will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor Name: x86_64\n",
      "Physical Cores: 64\n",
      "Logical Cores: 128\n",
      "Current CPU Frequency: 2447.42 MHz\n",
      "Min CPU Frequency: 1500.00 MHz\n",
      "Max CPU Frequency: 2450.00 MHz\n",
      "Total CPU Usage: 0.3%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Pre-processing + Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_SIC = 1e-6\n",
    "\n",
    "def compute_freeboard(sea_ice_concentration: np.ndarray, \n",
    "                      ice_volume: np.ndarray, \n",
    "                      snow_volume: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow grid cell averaged thickness and sea_ice_concentration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sea_ice_concentration : np.ndarray\n",
    "        Sea ice concentration in percent (called timeDaily_avg_iceAreaCell in E3SM)\n",
    "    ice_volume : np.ndarray\n",
    "        Grid cell averaged ice thickness in meters (called timeDaily_avg_iceVolumeCell in E3SM)\n",
    "    snow_volume : np.ndarray\n",
    "        Grid cell averaged snow thickness in meters (called timeDaily_avg_snowVolumeCell in E3SM)\n",
    "    All arrays are the same shape\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : np.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize the freeboard array with zeros\n",
    "    freeboard = np.zeros_like(sea_ice_concentration)\n",
    "    \n",
    "    # Create a boolean mask for valid areas to prevent division by zero\n",
    "    valid_mask = sea_ice_concentration > MIN_SIC\n",
    "    \n",
    "    # Calculate freeboard only for the valid cells in a single vectorized step\n",
    "    # This avoids creating intermediate height_ice and height_snow arrays.\n",
    "    if np.any(valid_mask):\n",
    "        valid_area = sea_ice_concentration[valid_mask]\n",
    "        valid_ice_avg_thickness = ice_volume[valid_mask]\n",
    "        valid_snow_avg_thickness = snow_volume[valid_mask]\n",
    "\n",
    "        freeboard[valid_mask] = (\n",
    "            (valid_ice_avg_thickness / valid_area) * (D_WATER - D_ICE) / D_WATER +\n",
    "            (valid_snow_avg_thickness / valid_area) * (D_WATER - D_SNOW) / D_WATER\n",
    "        )\n",
    "    \n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set level to logging.INFO to see the statements\n",
    "logging.basicConfig(filename=f'{model_version}_dataset.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional - transform applied to the data tensor *only*.\n",
    "    latitude_threshold\n",
    "        The minimum latitude to use for Arctic data\n",
    "    context_length\n",
    "        The number of days to fetch for input in the prediction step\n",
    "    forecast_horizon\n",
    "        The number of days to predict in the future\n",
    "    plot_outliers_and_imbalance\n",
    "        Optional - check outliers and imbalance on the variables Ice Area and Freeboard\n",
    "    trial_run\n",
    "        Optional - use the data in the trial directory instead of the full dataset\n",
    "        Specify the name of the trial director in perlmutterpath.py\n",
    "    num_patches\n",
    "        How many patches to use for the patchify function\n",
    "    cells_per_patch\n",
    "        How many cells to have in each patch for patchify\n",
    "    patchify_func : Callable\n",
    "        The patchify function to use (ex., patchify_by_latlon_spillover).\n",
    "    patchify_func_key : str\n",
    "        The string key identifying the patchify function (e.g., \"latlon_spillover\")\n",
    "        used to look up its specific parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        mesh_dir: str = mesh_dir,\n",
    "        transform: Callable = None,\n",
    "        latitude_threshold: int = LATITUDE_THRESHOLD,\n",
    "        context_length: int = CONTEXT_LENGTH,\n",
    "        forecast_horizon: int = FORECAST_HORIZON,\n",
    "        normalize_on: bool = NORMALIZE_ON,\n",
    "        plot_outliers_and_imbalance: bool = PLOT_DATA_FULL_DISTRIBUTION, # set FALSE NOW THAT I HAVE IT FOR FULL DATASET\n",
    "        trial_run: bool = TRIAL_RUN, # Use the trial data directory\n",
    "        num_patches: int = NUM_PATCHES,\n",
    "        cells_per_patch: int = CELLS_PER_PATCH,\n",
    "        patchify_func: Callable = DEFAULT_PATCHIFY_METHOD_FUNC, # Default patchify function\n",
    "        patchify_func_key: str = PATCHIFY_TO_USE, # Key to look up specific params\n",
    "        max_freeboard_for_normalization: int = MAX_FREEBOARD_FOR_NORMALIZATION,\n",
    "        max_freeboard_on: bool = MAX_FREEBOARD_ON,\n",
    "        processed_data_path: str = PROCESSED_DATA_DIR,       \n",
    "    \n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "\n",
    "        Handle the raw data:\n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature (ice area, ice volume, etc.)\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Load the mesh and initialize the cell mask\n",
    "        3) Store a list of datetimes from each file \n",
    "        4) Extract raw data\n",
    "        \n",
    "        Perform pre-processing:\n",
    "        5) Apply a mask to nCells to look just at regions in certain latitudes\n",
    "            nCells >= 40 degrees is 53973 cells\n",
    "            nCells >= 50 degrees is 35623 cells\n",
    "        6) Derive Freeboard from ice area, snow volume, and ice volume\n",
    "        7) Custom patchify and store patch_ids so the data loader can use them\n",
    "        8) Optional: Plot the outliers and data imbalance for Ice Area and Freeboard\n",
    "        9) Optional: Normalize the data (Ice area is already between 0 and 1; Freeboard is not) \"\"\"\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.latitude_threshold = latitude_threshold\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.normalize_on = normalize_on\n",
    "        self.plot_outliers_and_imbalance = plot_outliers_and_imbalance\n",
    "        self.trial_run = trial_run\n",
    "        self.num_patches = num_patches\n",
    "        self.cells_per_patch = cells_per_patch\n",
    "        self.patchify_func = patchify_func # Store the specified patchify function\n",
    "        self.patchify_func_key = patchify_func_key # Store the key for looking up specific params\n",
    "        self.max_freeboard_for_normalization = max_freeboard_for_normalization\n",
    "        self.max_freeboard_on = max_freeboard_on\n",
    "        self.processed_data_path = processed_data_path # Store the processed data path\n",
    "        \n",
    "        # --- Check for pre-processed data first ---\n",
    "        if self.processed_data_path and os.path.exists(self.processed_data_path):\n",
    "            logging.info(f\"Loading pre-processed data from Zarr store: {self.processed_data_path}\")\n",
    "            try:\n",
    "\n",
    "                processed_ds = xr.open_zarr(self.processed_data_path)\n",
    "                self.times = processed_ds[\"time\"].values\n",
    "                self.ice_area = processed_ds[\"ice_area\"].values\n",
    "                self.freeboard = processed_ds[\"freeboard\"].values\n",
    "                self.cell_mask = processed_ds[\"cell_mask\"].values\n",
    "                \n",
    "                # Convert string representations back to dicts using eval()\n",
    "                # Be cautious with eval() if the source of the Zarr store is untrusted.\n",
    "                self.full_to_masked = eval(processed_ds[\"full_to_masked\"].item())\n",
    "                self.masked_to_full = eval(processed_ds[\"masked_to_full\"].item())\n",
    "                self.patch_latlons = processed_ds[\"patch_latlons\"].values\n",
    "                self.algorithm = processed_ds[\"algorithm\"].item()\n",
    "                self.num_raw_files = processed_ds[\"num_raw_files\"].item()\n",
    "                \n",
    "                # Reconstruct the list of lists (jagged array) from the flattened indices and offsets\n",
    "                flattened_indices = processed_ds[\"flattened_indices\"].values\n",
    "                patch_offsets = processed_ds[\"patch_offsets\"].values\n",
    "                self.indices_per_patch_id = []\n",
    "                start_idx = 0\n",
    "\n",
    "                # Convert array of arrays back to list of lists/arrays\n",
    "                # Ensure it's a list of lists of ints\n",
    "                for offset in patch_offsets:\n",
    "                    self.indices_per_patch_id.append(flattened_indices[start_idx:offset])\n",
    "                    start_idx = offset\n",
    "                \n",
    "                logging.info(f\"Loaded pre-processed data in {time.time() - start_time:.2f} seconds.\")\n",
    "                print(f\"Loaded pre-processed data in {time.time() - start_time:.2f} seconds.\")\n",
    "                return # Exit __init__ if data loaded successfully\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading pre-processed data from {self.processed_data_path}: {e}\")\n",
    "                print(f\"Error loading pre-processed data: {e}. Falling back to raw NetCDF loading.\")\n",
    "                # Fall through to raw NetCDF loading if pre-processed load fails\n",
    "\n",
    "        logging.info(\"No valid pre-processed data found. Loading from raw NetCDF files.\")\n",
    "\n",
    "        \n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        if trial_run:\n",
    "            # Use glob pattern for open_mfdataset\n",
    "            file_pattern = os.path.join(self.data_dir, \"v3.LR.historical_0051.mpassi.hist.am.timeSeriesStatsDaily.202*.nc\")\n",
    "        else:\n",
    "            # Use glob pattern for open_mfdataset for the full dataset\n",
    "            file_pattern = os.path.join(self.data_dir, \"v3.LR.historical_0051.mpassi.hist.am.timeSeriesStatsDaily.*.nc\")\n",
    "\n",
    "        # Check if any files match the pattern using glob\n",
    "        file_paths = sorted(glob.glob(file_pattern))\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found matching the pattern in {self.data_dir}\")\n",
    "\n",
    "        self.num_raw_files = len(file_paths) # Store the count of raw files\n",
    "        logging.info(f\"Found {self.num_raw_files} NetCDF files matching the file pattern\")\n",
    "                \n",
    "        # --- 2. Load the mesh file. Latitudes and Longitudes are in radians. ---\n",
    "        mesh = xr.open_dataset(mesh_dir)\n",
    "        self.latCell = np.degrees(mesh[\"latCell\"].values)\n",
    "        self.lonCell = np.degrees(mesh[\"lonCell\"].values)\n",
    "        mesh.close()\n",
    "        \n",
    "        # Initialize the cell mask\n",
    "        self.cell_mask = self.latCell >= latitude_threshold        \n",
    "        masked_ncells_size = np.count_nonzero(self.cell_mask)\n",
    "        logging.info(f\"Mask size: {masked_ncells_size}\")\n",
    "\n",
    "        self.full_to_masked = {\n",
    "            full_idx: new_idx\n",
    "            for new_idx, full_idx in enumerate(np.where(self.cell_mask)[0])\n",
    "        }\n",
    "\n",
    "        # Also store reverse mapping: masked -> full for recovery of data later\n",
    "        self.masked_to_full = {\n",
    "            v: k for k, v in self.full_to_masked.items()\n",
    "        }\n",
    "\n",
    "        logging.info(f\"=== Extracting raw data and times using xarray.open_mfdataset === \")\n",
    "\n",
    "        # Use open_mfdataset for efficient parallel loading\n",
    "        # Specify 'timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell', 'timeDaily_avg_snowVolumeCell'\n",
    "        # as the variables to load.\n",
    "        # Use combine='nested' and concat_dim='Time' because the NetCDF files do not have explicit time coordinates,\n",
    "        # but the 'Time' dimension exists and files are sorted by name.\n",
    "        # parallel=True enables Dask for parallel file opening and processing.\n",
    "        with xr.open_mfdataset(\n",
    "            file_pattern,\n",
    "            combine='nested',\n",
    "            concat_dim='Time',\n",
    "            parallel=True,  # Might cause an error\n",
    "            data_vars=['timeDaily_avg_iceAreaCell', 'timeDaily_avg_iceVolumeCell', 'timeDaily_avg_snowVolumeCell', 'xtime_startDaily'],\n",
    "            decode_times=True # Let xarray handle time decoding\n",
    "        ) as combined_ds:\n",
    "            # Extract times from byte string format and convert to datetime64[s]\n",
    "            xtime_byte_array = combined_ds[\"xtime_startDaily\"].values\n",
    "            xtime_unicode_array = xtime_byte_array.astype(str)\n",
    "            xtime_cleaned_array = np.char.replace(xtime_unicode_array, \"_\", \" \")\n",
    "            self.times = np.asarray(xtime_cleaned_array, dtype='datetime64[s]')\n",
    "\n",
    "            # Extract raw data and apply mask directly to Dask arrays, then compute to NumPy\n",
    "            # .compute() triggers the actual loading into memory\n",
    "            # .values gives you NumPy arrays\n",
    "            self.ice_area = combined_ds[\"timeDaily_avg_iceAreaCell\"][:, self.cell_mask].compute().values\n",
    "            ice_volume_combined = combined_ds[\"timeDaily_avg_iceVolumeCell\"][:, self.cell_mask].compute().values\n",
    "            snow_volume_combined = combined_ds[\"timeDaily_avg_snowVolumeCell\"][:, self.cell_mask].compute().values\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        logging.info(f\"Total days collected: {len(self.times)}\")\n",
    "        logging.info(f\"Unique days: {len(np.unique(self.times))}\")\n",
    "        logging.info(f\"First 35 days: {self.times[:35]}\")\n",
    "        logging.info(f\"Last 35 days: {self.times[-35:]}\")\n",
    "\n",
    "        logging.info(f\"Shape of combined ice_area array: {self.ice_area.shape}\")\n",
    "        logging.info(f\"Shape of combined ice_volume_combined array: {ice_volume_combined.shape}\")\n",
    "        logging.info(f\"Shape of combined snow_volume_combined array: {snow_volume_combined.shape}\")\n",
    "\n",
    "        logging.info(f\"Elapsed time for data file reading: {time.perf_counter() - start_time} seconds\")\n",
    "        \n",
    "        # --- 6. Derive Freeboard from ice area, snow volume and ice volume\n",
    "        logging.info(f\"=== Calculating Freeboard === \")\n",
    "\n",
    "        # Convert the xarray objects using .values so the freeboard calculation gets numpy arrays\n",
    "        self.freeboard = compute_freeboard(self.ice_area, ice_volume_combined, snow_volume_combined)\n",
    "        logging.info(f\"Elapsed time for freeboard calculation: {time.perf_counter() - start_time} seconds\")\n",
    "        \n",
    "        logging.info(f\"=== Patchifying === \")\n",
    "\n",
    "        # Get the parameters for the patchification function\n",
    "        patchify_call_params = COMMON_PARAMS.copy()\n",
    "        \n",
    "        # Retrieve only the specific parameters for the chosen patchify function\n",
    "        patchify_call_params.update(SPECIFIC_PARAMS.get(self.patchify_func_key, {}))\n",
    "        \n",
    "        # --- 7. Use the dynamic patchify function\n",
    "        #     Returns \n",
    "        # full_nCells_patch_ids : np.ndarray\n",
    "        #     Array of shape (nCells,) giving patch ID or -1 if unassigned.\n",
    "        # indices_per_patch_id : List[np.ndarray]\n",
    "        #     List of patches, each a list of cell indices (np.ndarray of ints) that correspond with nCells array.\n",
    "        # patch_latlons : np.ndarray\n",
    "        #     Array of shape (n_patches, 2) containing (latitude, longitude) for one\n",
    "        #     representative cell per patch (the first cell added to the patch)\n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id, self.patch_latlons, self.algorithm = self.patchify_func(**patchify_call_params)\n",
    "\n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        # This ensures there's no out of bounds problem,\n",
    "        # like index 296237 is out of bounds for axis 1 with size 53973\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "        logging.info(f\"Elapsed time for patchifying with the {self.algorithm} algorithm: {time.perf_counter() - start_time} seconds\")\n",
    "\n",
    "        # --- 8. Optional --- OUTLIER DETECTION AND DATA IMBALANCE CHECK ---\n",
    "        if self.trial_run:\n",
    "            status = \"trial\"\n",
    "        else:\n",
    "            status = \"prod\"\n",
    "            \n",
    "        if self.plot_outliers_and_imbalance:\n",
    "            logging.info(f\"=== Plotting Outliers and Imbalance === \")\n",
    "            check_and_plot_freeboard(self.freeboard, self.times, f\"{status}_fb_pre_norm\")\n",
    "            analyze_ice_area_imbalance(self.ice_area)\n",
    "            plot_ice_area_imbalance(self.ice_area, status)\n",
    "            logging.info(f\"Elapsed time for plotting the outliers and imbalance {time.perf_counter() - start_time} seconds\")\n",
    "\n",
    "        # --- 9. Optional --- Normalize the data (Area is already between 0 and 1; Freeboard is not)\n",
    "        if self.normalize_on:\n",
    "            logging.info(f\"=== Normalizing Freeboard with Scikit-learn MinMaxScaler === \")\n",
    "    \n",
    "            # Reshape freeboard data from (days, cells) to (total_samples, 1) for the scaler\n",
    "            freeboard_reshaped = self.freeboard.reshape(-1, 1)\n",
    "\n",
    "            # Determine the min and max values for the scaler based on the flag\n",
    "            if self.max_freeboard_on:\n",
    "                logging.info(f\"Using custom max freeboard value: {self.max_freeboard_for_normalization}\")\n",
    "                min_val = 0\n",
    "                max_val = self.max_freeboard_for_normalization\n",
    "            else:\n",
    "                logging.info(f\"Using min and max values from the data.\")\n",
    "                min_val = freeboard_reshaped.min()\n",
    "                max_val = freeboard_reshaped.max()\n",
    "\n",
    "            # Initialize and fit the scaler with the determined range\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler.fit([[min_val], [max_val]]) \n",
    "\n",
    "            # Transform the freeboard data and reshape it back to the original shape\n",
    "            self.freeboard = scaler.transform(freeboard_reshaped).reshape(self.freeboard.shape)\n",
    "            \n",
    "            # Apply clipping to ensure all values are within the 0 to 1 range\n",
    "            self.freeboard = np.clip(self.freeboard, 0, 1)\n",
    "\n",
    "            logging.info(f\"Freeboard min (pre-norm): {min_val} meters\" )\n",
    "            logging.info(f\"Freeboard max (pre-norm): {max_val} meters\")\n",
    "            logging.info(f\"Freeboard min (post-norm): {self.freeboard.min()}\" )\n",
    "            logging.info(f\"Freeboard max (post-norm): {self.freeboard.max()}\")\n",
    "\n",
    "            if self.plot_outliers_and_imbalance:\n",
    "                logging.info(f\"Elapsed time for normalizing the Freeboard: {time.perf_counter() - start_time} seconds\")\n",
    "                check_and_plot_freeboard(self.freeboard, self.times, f\"{status}_fb_post_norm\")\n",
    "        \n",
    "        # --- Save processed data if a path is provided and it's not a trial run ---\n",
    "        #if self.processed_data_path and not self.trial_run:\n",
    "        if self.processed_data_path:\n",
    "            \n",
    "            logging.info(f\"Attempting to save processed data to Zarr store: {self.processed_data_path}\")\n",
    "            try:\n",
    "\n",
    "                # Flatten the list of lists into a single 1D array\n",
    "                flattened_indices = np.concatenate(self.indices_per_patch_id)\n",
    "                \n",
    "                # Create an array of cumulative sums to mark the start and end of each patch\n",
    "                patch_offsets = np.cumsum([len(p) for p in self.indices_per_patch_id])\n",
    "                \n",
    "                # Create a new xarray dataset\n",
    "                # Create a new xarray dataset\n",
    "                processed_ds_to_save = xr.Dataset(\n",
    "                    {\n",
    "                        \"ice_area\": ((\"time\", \"nCells_masked\"), self.ice_area),\n",
    "                        \"freeboard\": ((\"time\", \"nCells_masked\"), self.freeboard),\n",
    "                        \"cell_mask\": (\"nCells_full\", self.cell_mask),\n",
    "                        \"full_to_masked\": ((), str(self.full_to_masked)),\n",
    "                        \"masked_to_full\": ((), str(self.masked_to_full)),\n",
    "                        \"algorithm\": ((), self.algorithm),\n",
    "                        \"flattened_indices\": (\"nCells_in_patches\", flattened_indices),\n",
    "                        \"patch_offsets\": (\"patch_idx\", patch_offsets),\n",
    "                        \"patch_latlons\": ((\"patch_idx\", \"latlon_coord\"), self.patch_latlons),\n",
    "                        \"num_raw_files\": ((), self.num_raw_files),\n",
    "                    },\n",
    "                    coords={\n",
    "                        \"time\": self.times,\n",
    "                        \"nCells_masked\": np.arange(self.ice_area.shape[1]),\n",
    "                        \"nCells_full\": np.arange(len(self.latCell)),\n",
    "                        \"patch_idx\": np.arange(len(self.indices_per_patch_id)),\n",
    "                        \"latlon_coord\": [\"latitude\", \"longitude\"]\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Save to Zarr format\n",
    "                # Use compute=True to ensure all Dask arrays are written to disk immediately\n",
    "                processed_ds_to_save.to_zarr(self.processed_data_path, mode='w', compute=True)\n",
    "                logging.info(f\"Processed data successfully saved to {self.processed_data_path}\")\n",
    "                print(f\"Processed data successfully saved to {self.processed_data_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save processed data to Zarr: {e}\")\n",
    "                print(f\"Failed to save processed data to Zarr: {e}\")\n",
    "\n",
    "        logging.info(\"End of __init__\")\n",
    "        end_time = time.perf_counter()\n",
    "        logging.info(f\"Elapsed time for DailyNetCDFDataset __init__: {end_time - start_time} seconds\")\n",
    "        print(f\"Elapsed time for DailyNetCDFDataset __init__: {end_time - start_time} seconds\")\n",
    "        print(f\"In minutes:                {(end_time - start_time)//60} minutes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of possible starting indices (idx) for a valid sequence.\n",
    "        A valid sequence needs `self.context_length` days for input and `self.forecast_horizon` days for target.\n",
    "        \n",
    "        ex) If the total number of days is 365, the context_length is 7 and the forecast_horizon is 3, then\n",
    "        \n",
    "        365 - (7 + 3) + 1 = 365 - 10 + 1 = 356 valid starting indices\n",
    "        return len(self.freeboard) - (self.context_length + self.forecast_horizon)\n",
    "        \"\"\"\n",
    "        required_length = self.context_length + self.forecast_horizon\n",
    "        if len(self.freeboard) < required_length:\n",
    "            return 0 # Not enough raw data to form even one sample\n",
    "\n",
    "        # The number of valid starting indices\n",
    "        return len(self.freeboard) - required_length + 1\n",
    "\n",
    "    def get_patch_tensor(self, day_idx: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Retrieves the feature data for a specific day, organized into patches.\n",
    "\n",
    "        This method extracts 'freeboard' and 'ice_area' data for a given day\n",
    "        and then reshapes it according to the pre-defined patches. Each patch\n",
    "        will contain its own set of feature values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        day_idx : int\n",
    "            The integer index of the day to retrieve data for, relative to the\n",
    "            concatenated dataset's time dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing the feature data organized by patches for the\n",
    "            specified day.\n",
    "            Shape: (context_length, num_patches, num_features, patch_size)\n",
    "            Where:\n",
    "            - num_patches: Total number of patches (ex., 140).\n",
    "            - num_features: The number of features per cell (currently 2: freeboard, ice_area).\n",
    "            - patch_size: The number of cells within each patch.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        freeboard_day = self.freeboard[day_idx]  # (nCells,)\n",
    "        ice_area_day = self.ice_area[day_idx]    # (nCells,)\n",
    "        features = np.stack([freeboard_day, ice_area_day], axis=0)  # (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(patch_tensors)  # (context_length, num_patches, num_features, patch_size)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Given an input of a certain date id, get the input and the target tensors\n",
    "        2. Return all the patches for the input and the target\n",
    "           Features are: [freeboard, ice_area] over masked cells. \n",
    "           \n",
    "        \"\"\"\n",
    "        # Start with the id of the day in question\n",
    "        start_idx = idx\n",
    "\n",
    "        # end_idx is the exclusive end of the input sequence,\n",
    "        # and the inclusive start of the target sequence.\n",
    "        end_idx = idx + self.context_length\n",
    "        target_start = end_idx\n",
    "\n",
    "        # the target sequence ends after forecast horizon\n",
    "        target_end = end_idx + self.forecast_horizon\n",
    "\n",
    "        if target_end > len(self.freeboard):\n",
    "            raise IndexError(\n",
    "                f\"Requested time window exceeds dataset. \"\n",
    "                f\"Problematic idx: {idx}, \"\n",
    "                f\"Context Length: {self.context_length}, \"\n",
    "                f\"Forecast Horizon: {self.forecast_horizon}, \"\n",
    "                f\"Calculated target_end: {target_end}, \"\n",
    "                f\"Actual dataset length (len(self.freeboard)): {len(self.freeboard)}\"\n",
    "            )\n",
    "\n",
    "        # Build input tensor\n",
    "        input_seq = [self.get_patch_tensor(i) for i in range(start_idx, end_idx)]\n",
    "        input_tensor = torch.stack(input_seq)\n",
    "    \n",
    "        # Build target tensor: shape (forecast_horizon, num_patches)\n",
    "        target_seq = self.ice_area[end_idx:target_end]\n",
    "        target_patches = []\n",
    "        for day in target_seq:\n",
    "            patch_day = [\n",
    "                torch.tensor(day[patch_indices]) for patch_indices in self.indices_per_patch_id\n",
    "            ]\n",
    "            \n",
    "            # After stacking, patch_day_tensor will be (num_patches, CELLS_PER_PATCH)\n",
    "            patch_day_tensor = torch.stack(patch_day)  # (num_patches,)\n",
    "            target_patches.append(patch_day_tensor)\n",
    "\n",
    "        # Final target tensor shape: (forecast_horizon, num_patches, CELLS_PER_PATCH)\n",
    "        target_tensor = torch.stack(target_patches)  # (forecast_horizon, num_patches)\n",
    "        \n",
    "        return input_tensor, target_tensor, start_idx, end_idx, target_start, target_end\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} viable days (only includes up to the last possible input date), \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{self.num_raw_files} files loaded, \"\n",
    "            f\"Patchify Algorithm: {self.algorithm}>\" # What patchify algorithm was used\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Making the Dataset Class: TRIAL_RUN MODE IS False ===== \n",
      "Loaded pre-processed data in 1753372360.91 seconds.\n",
      "========== SPLITTING THE DATASET ===================\n",
      "Training data length:    59130\n",
      "Validation data length:  2190\n",
      "Testing data length:     2555\n",
      "Total days =  63875\n",
      "Number of training batches 7391\n",
      "Number of training batches 273\n",
      "Number of test batches after drop_last incomplete batch 319\n",
      "Number of test days to drop after drop_last incomplete batch 319\n",
      "===== Printing Dataset ===== \n",
      "<DailyNetCDFDataset: 63862 viable days (only includes up to the last possible input date), 53973 cells/day, 2100 files loaded, Patchify Algorithm: row_by_row>\n",
      "===== Sample at dataset[0] ===== \n",
      "Fetched start index 0: Time=1850-01-01T00:30:00.000000000\n",
      "Fetched end   index 7: Time=1850-01-08T00:00:00.000000000\n",
      "Fetched target start index 7: Time=1850-01-08T00:00:00.000000000\n",
      "Fetched target end   index 14: Time=1850-01-15T00:00:00.000000000\n",
      "===== Start and End Dates for Each Set =====\n",
      "Training set start date: 1850-01-01T00:30:00.000000000\n",
      "Training set end date: 2011-12-31T00:00:00.000000000\n",
      "Validation set start date: 2012-01-01T00:00:00.000000000\n",
      "Validation set end date: 2017-12-31T00:00:00.000000000\n",
      "Testing set start date: 2018-01-01T00:00:00.000000000\n",
      "Testing set end date: 2024-12-31T00:00:00.000000000\n",
      "===== Starting DataLoader ====\n",
      "input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\n",
      "actual input_tensor.shape = torch.Size([7, 210, 2, 256])\n",
      "target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\n",
      "actual target_tensor.shape = torch.Size([7, 210, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(f\"===== Making the Dataset Class: TRIAL_RUN MODE IS {TRIAL_RUN} ===== \")\n",
    "\n",
    "# load all the data from one folder\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "# Patch locations for positional embedding\n",
    "PATCH_LATLONS_TENSOR = torch.tensor(dataset.patch_latlons, dtype=torch.float32)\n",
    "\n",
    "print(\"========== SPLITTING THE DATASET ===================\")\n",
    "# DIFFERENT SUBSET OPTIONS FOR TRAINING / VALIDATION / TESTING for the trial data vs. full dataset\n",
    "if TRIAL_RUN:\n",
    "    total_days = len(dataset)\n",
    "    train_end = int(total_days * 0.7)\n",
    "    val_end = int(total_days * 0.85)\n",
    "    \n",
    "    train_set = Subset(dataset, range(0, train_end))\n",
    "    val_set   = Subset(dataset, range(train_end, val_end))\n",
    "    test_set  = Subset(dataset, range(val_end, total_days))\n",
    "    \n",
    "else:\n",
    "    # --- Custom Splitting by Year ---\n",
    "    \n",
    "    # Convert dataset.times to pandas DatetimeIndex for easier year-based filtering\n",
    "    all_times_pd = pd.to_datetime(dataset.times)\n",
    "\n",
    "    # Define the start and end years for each set - keep this for the full dataset\n",
    "    train_start_year = 1850\n",
    "    train_end_year = 2011   \n",
    "    val_start_year = 2012\n",
    "    val_end_year = 2017\n",
    "    test_start_year = 2018\n",
    "    test_end_year = 2024\n",
    "    \n",
    "    # Get the boolean masks for each set\n",
    "    train_mask = (all_times_pd.year >= train_start_year) & (all_times_pd.year <= train_end_year)\n",
    "    val_mask = (all_times_pd.year >= val_start_year) & (all_times_pd.year <= val_end_year)\n",
    "    test_mask = (all_times_pd.year >= test_start_year) & (all_times_pd.year <= test_end_year)\n",
    "\n",
    "    # Get the integer indices where the masks are True\n",
    "    train_indices = np.where(train_mask)[0].tolist()\n",
    "    val_indices = np.where(val_mask)[0].tolist()\n",
    "    test_indices = np.where(test_mask)[0].tolist()\n",
    "    \n",
    "    # Create Subsets using the obtained indices\n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    val_set   = Subset(dataset, val_indices)\n",
    "    test_set  = Subset(dataset, test_indices)\n",
    "\n",
    "    train_end = train_indices[-1]\n",
    "    val_end = val_indices[-1]\n",
    "\n",
    "print(\"Training data length:   \", len(train_set))\n",
    "print(\"Validation data length: \", len(val_set))\n",
    "print(\"Testing data length:    \", len(test_set))\n",
    "\n",
    "total_days = len(train_set) + len(val_set) + len(test_set)\n",
    "print(\"Total days = \", total_days)\n",
    "\n",
    "print(\"Number of training batches\", len(train_set)//BATCH_SIZE)\n",
    "print(\"Number of training batches\", len(val_set)//BATCH_SIZE)\n",
    "\n",
    "print(\"Number of test batches after drop_last incomplete batch\", len(test_set)//BATCH_SIZE)\n",
    "print(\"Number of test days to drop after drop_last incomplete batch\", len(test_set)//BATCH_SIZE)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__ → see how many files & days loaded\n",
    "\n",
    "print(\"===== Sample at dataset[0] ===== \")\n",
    "input_tensor, target_tensor, start_idx, end_idx, target_start, target_end = dataset[0]\n",
    "\n",
    "print(f\"Fetched start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched end   index {end_idx}: Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched target start index {target_start}: Time={dataset.times[target_start]}\")\n",
    "print(f\"Fetched target end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "def print_set_dates(dataset_subset, set_name):\n",
    "    \"\"\" Print start and end dates for each set (Training, Validation, Testing)\"\"\"\n",
    "    if len(dataset_subset) == 0:\n",
    "        print(f\"{set_name} set: No data available.\")\n",
    "        return\n",
    "\n",
    "    # Get the global indices of the first and last elements in the subset\n",
    "    first_global_idx = dataset_subset.indices[0]\n",
    "    last_global_idx = dataset_subset.indices[-1]\n",
    "\n",
    "    # Note: For the training, validation, and testing sets, each item (idx) represents the *start*\n",
    "    # of a `context_length + forecast_horizon` window.\n",
    "    # So, the start date of a set is the `dataset.times` value at the global index of its first item.\n",
    "    start_date = dataset.times[first_global_idx]\n",
    "    end_date = dataset.times[last_global_idx]\n",
    "\n",
    "    print(f\"{set_name} set start date: {start_date}\")\n",
    "    print(f\"{set_name} set end date: {end_date}\")\n",
    "    logging.info(f\"{set_name} set start date: {start_date}\")\n",
    "    logging.info(f\"{set_name} set end date: {end_date}\")\n",
    "    \n",
    "    return str(start_date), str(end_date)\n",
    "\n",
    "print(\"===== Start and End Dates for Each Set =====\")\n",
    "train_set_start_year, train_set_end_year = print_set_dates(train_set, \"Training\")\n",
    "val_set_start_year, val_set_end_year = print_set_dates(val_set, \"Validation\")\n",
    "test_set_start_year, test_set_end_year = print_set_dates(test_set, \"Testing\")\n",
    "\n",
    "train_set_start_year, train_set_end_year = train_set_start_year[:4], train_set_end_year[:4]\n",
    "val_set_start_year, val_set_end_year = val_set_start_year[:4], val_set_end_year[:4]\n",
    "test_set_start_year, test_set_end_year = test_set_start_year[:4], test_set_end_year[:4]\n",
    "\n",
    "print(\"===== Starting DataLoader ====\")\n",
    "# wrap in a DataLoader\n",
    "# 1. Use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. Use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "# 3. Use shuffle=False to preserve time order (especially for forecasting)\n",
    "# 4. Use drop_last=True to prevent it from testing on incomplete batches\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2, drop_last=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2, drop_last=True)\n",
    "\n",
    "print(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "print(f\"actual input_tensor.shape = {input_tensor.shape}\")\n",
    "print(\"target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\")\n",
    "print(f\"actual target_tensor.shape = {target_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d9baa0-427e-49d9-a50a-97c3d60a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IceForecastTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Transformer-based model for forecasting ice conditions based on sequences of\n",
    "    historical patch data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_patch_features_dim : int\n",
    "        The dimensionality of the feature vector for each individual patch (ex. 2 features).\n",
    "        This is the input dimension for the patch embedding layer.\n",
    "    num_patches : int\n",
    "        The total number of geographical patches that the `nCells` data was divided into.\n",
    "        (ex., 256 patches).\n",
    "    context_length : int, optional\n",
    "        The number of historical days (time steps) to use as input for the transformer.\n",
    "        Defaults to 7.\n",
    "    forecast_horizon : int, optional\n",
    "        The number of future days to predict for each patch.\n",
    "        Defaults to 1.\n",
    "    d_model : int, optional\n",
    "        The dimension of the model's hidden states (embedding dimension).\n",
    "        This is the size of the vectors that flow through the Transformer encoder.\n",
    "        Defaults to 128.\n",
    "    nhead : int, optional\n",
    "        The number of attention heads in the multi-head attention mechanism within\n",
    "        each Transformer encoder layer. Defaults to 8.\n",
    "    num_layers : int, optional\n",
    "        The number of Transformer encoder layers in the model. Defaults to 4.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : nn.Linear\n",
    "        Linear layer to project input patch features into the `d_model` hidden space.\n",
    "    encoder : nn.TransformerEncoder\n",
    "        The Transformer encoder module composed of `num_layers` encoder layers.\n",
    "    mlp_head : nn.Sequential\n",
    "        A multi-layer perceptron head for outputting predictions for each patch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_patch_features_dim: int = PATCH_EMBEDDING_INPUT_DIM, # D: The flat feature dimension of a single patch (ex., 512)\n",
    "                 num_patches: int = NUM_PATCHES,  # P: Number of spatial patches\n",
    "                 context_length: int = CONTEXT_LENGTH, # T: Number of historical time steps\n",
    "                 forecast_horizon: int = FORECAST_HORIZON, # Number of future time steps to predict (usually 1)\n",
    "                 d_model: int = D_MODEL,        # d_model: Transformer's embedding dimension\n",
    "                 nhead: int = N_HEAD,           # nhead: Number of attention heads\n",
    "                 num_layers: int = NUM_TRANSFORMER_LAYERS, # num_layers: Number of TransformerEncoderLayers\n",
    "                 dropout: float = 0.1,  \n",
    "                 dim_feedforward: int = PATCH_EMBEDDING_INPUT_DIM, \n",
    "                 activation: str = \"gelu\" \n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The transformer should\n",
    "        1. Accept a sequence of days (ex. 7 days of patches). \n",
    "           The context_length parameter says how many days to use for input.\n",
    "        2. Encode each patch with the transformer.\n",
    "        3. Output the patches for regression (ex. predict the 8th day).\n",
    "           The forecast_horizon parameter says how many days to use for the output prediction.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.input_patch_features_dim = input_patch_features_dim\n",
    "   \n",
    "        print(\"Calling IceForecastTransformer __init__\")\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Patch embedding layer: projects the raw patch features (512)\n",
    "        # into d_model (128) hidden space dimension\n",
    "        self.patch_embed = nn.Linear(input_patch_features_dim, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output tensors are (batch, sequence, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,               \n",
    "            activation=activation,          \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP head: (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        # Make a prediction for every cell per patch. \n",
    "        # The Sigmoid is CRITICAL. It ensures there are no out of bounds predictions.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, CELLS_PER_PATCH * forecast_horizon),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"End of IceForecastTransformer __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B = Batch size\n",
    "        T = Time (context_length)\n",
    "        P = Patch count\n",
    "        D = Patch Dimension (cells per patch * feature count)\n",
    "        x: Tensor of shape (B, T, P, D)\n",
    "        Output: Tensor of shape (batch_size, forecast_horizon, num_patches)\n",
    "        Output: (B, forecast_horizon, P)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initial input x shape from DataLoader / pre-processing:\n",
    "        # (B, T, P, D) i.e., (Batch_Size, Context_Length, Num_Patches, Input_Patch_Features_Dim)\n",
    "        # Example: (16, 7, 140, 512)\n",
    "        \n",
    "        B, T, P, D = x.shape\n",
    "\n",
    "        # Flatten time and patches for the Transformer Encoder:\n",
    "        # Each (Time, Patch) combination becomes a single token in the sequence.\n",
    "        # Output shape: (B, T * P, D)\n",
    "        # Example: (16, 7 * 140 = 980, 512)\n",
    "        \n",
    "        # Flatten time and patches for the Transformer Encoder: (B, T * P, D)\n",
    "        # This treats each patch at each time step as a distinct token\n",
    "        x = x.view(B, T * P, D)\n",
    "\n",
    "        # Project patch features to the transformer's d_model dimension\n",
    "        x = self.patch_embed(x)  # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        x = self.encoder(x)      # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "\n",
    "        # Reshape back to separate time and patches: (B, T, P, d_model) ex., (16, 7, 140, 128)\n",
    "        x = x.view(B, T, P, self.d_model) \n",
    "\n",
    "        # Mean pooling over the time (context_length) dimension for each patch.\n",
    "        # This aggregates information from all historical time steps for each patch's final prediction.        \n",
    "        x = x.mean(dim=1)  # Output: (B, P, d_model) ex., (16, 140, 128)\n",
    "\n",
    "        # Apply MLP head to predict values for each cell in each patch\n",
    "        # The MLP head outputs (B, P, CELLS_PER_PATCH * forecast_horizon)\n",
    "        x = self.mlp_head(x) # ex. (16, 140, 256 * 3) = (16, 140, 768)\n",
    "\n",
    "        # Reshape the output to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "        # Explicitly reshape the last dimension to seperate the forecast horizon out\n",
    "        x = x.view(B, P, self.forecast_horizon, CELLS_PER_PATCH) # Reshape into forecast_horizon and CELLS_PER_PATCH\n",
    "        x = x.permute(0, 2, 1, 3) # Permute to (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200db32-3fbb-4f8f-b841-bbcdc9117c11",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7383efb0-3e8d-468d-bf66-0bb91f943ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling IceForecastTransformer __init__\n",
      "Elapsed time: 0.03 seconds\n",
      "End of IceForecastTransformer __init__\n",
      "\n",
      "--- Model Architecture ---\n",
      "IceForecastTransformer(\n",
      "  (patch_embed): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=128, out_features=1792, bias=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/10 - Train Loss: 0.0273\n",
      "Validation Loss: 0.0089\n",
      "Epoch 2/10 - Train Loss: 0.0069\n",
      "Validation Loss: 0.0066\n",
      "Epoch 3/10 - Train Loss: 0.0059\n",
      "Validation Loss: 0.0057\n",
      "Epoch 4/10 - Train Loss: 0.0055\n",
      "Validation Loss: 0.0060\n",
      "Epoch 5/10 - Train Loss: 0.0052\n",
      "Validation Loss: 0.0055\n",
      "Epoch 6/10 - Train Loss: 0.0049\n",
      "Validation Loss: 0.0047\n",
      "Epoch 7/10 - Train Loss: 0.0048\n",
      "Validation Loss: 0.0052\n",
      "Epoch 8/10 - Train Loss: 0.0046\n",
      "Validation Loss: 0.0045\n",
      "Epoch 9/10 - Train Loss: 0.0045\n",
      "Validation Loss: 0.0057\n",
      "Epoch 10/10 - Train Loss: 0.0044\n",
      "Validation Loss: 0.0060\n",
      "===============================================\n",
      "Elapsed time for TRAINING: 2721.71 seconds\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch import Tensor\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    import logging\n",
    "    \n",
    "    # Set level to logging.INFO to see the statements\n",
    "    logging.basicConfig(filename='IceForecastTransformerInstance.log', filemode='w', level=logging.INFO)\n",
    "    \n",
    "    model = IceForecastTransformer().to(device)\n",
    "    \n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    print(model)\n",
    "    print(\"--------------------------\\n\")\n",
    "    \n",
    "    logging.info(\"\\n--- Model Architecture ---\")\n",
    "    logging.info(str(model)) # Log the full model structure\n",
    "    logging.info(f\"Total model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    logging.info(\"--------------------------\\n\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    logging.info(\"== TIMER IS STARTING FOR TRAINING ==\")\n",
    "    start_time = time.perf_counter()\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(\"       STARTING EPOCHS       \")\n",
    "    logging.info(\"===============================\")\n",
    "    logging.info(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "    logging.info(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        for batch_idx, (input_tensor, target_tensor, start_idx, end_idx, target_start, target_end) in enumerate(train_loader):  \n",
    "    \n",
    "            # Move input and target to the device\n",
    "            # x: (B, context_length, num_patches, input_patch_features_dim), y: (B, forecast_horizon, num_patches)\n",
    "            x = input_tensor.to(device)  # Shape: (B, T, P, C, L)\n",
    "            y = target_tensor.to(device)  # Shape: (B, forecast_horizon, P, L)\n",
    "    \n",
    "            # Reshape x for transformer input\n",
    "            B, T, P, C, L = x.shape\n",
    "            x_reshaped_for_transformer_D = x.view(B, T, P, C * L)\n",
    "    \n",
    "            # Run through transformer\n",
    "            y_pred = model(x_reshaped_for_transformer_D) # y_pred is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, y) # DIRECTLY compare y_pred and y\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "        # --- Validation loop ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Unpack the full tuple\n",
    "                x_val, y_val, start_idx, end_idx, target_start, target_end = batch\n",
    "        \n",
    "                # Move to GPU if available\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "    \n",
    "                # Extract dimensions from x_val for reshaping\n",
    "                # x_val before reshaping: (B_val, T_val, P_val, C_val, L_val)\n",
    "                B_val, T_val, P_val, C_val, L_val = x_val.shape\n",
    "                \n",
    "                # Reshape x_val for transformer input\n",
    "                x_val_reshaped_for_transformer_input = x_val.view(B_val, T_val, P_val, C_val * L_val)\n",
    "    \n",
    "                # Model output is (B, forecast_horizon, P, L)\n",
    "                y_val_pred = model(x_val_reshaped_for_transformer_input) \n",
    "    \n",
    "                # Compute validation loss (y_val_pred and y_val should have identical shapes)\n",
    "                val_loss += criterion(y_val_pred, y_val).item() # y_val is (B, forecast_horizon, P, L)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        logging.info(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\") # Keep print for immediate console feedback\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    logging.info(\"===============================================\")\n",
    "    logging.info(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    logging.info(\"===============================================\")\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Elapsed time for TRAINING: {elapsed_time:.2f} seconds\")\n",
    "    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {},
   "source": [
    "TODO OPTION: Try temporal attention only (ex., Informer, Time Series Transformer).\n",
    "\n",
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ecdd478-4bc1-4a3f-b91d-b40f03db7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at fd_nTM_D128_B8_lt40_P210_L256_T7_Fh7_e10_ROW_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path where to save or load the model\n",
    "PATH = f\"{model_version}_model.pth\"\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    # Save the model's state_dict\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(f\"Saved model at {PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a5f4e-ea4d-4cb1-88f4-6e800dc92fe4",
   "metadata": {},
   "source": [
    "# === BELOW - CAN BE USED ANY TIME FROM A .PTH FILE\n",
    "\n",
    "Make sure and run the cells that contain constants or run all, but comment out the \"save\" and the training loop cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278114f-96bb-4bbc-8e7f-f87c52e2a946",
   "metadata": {},
   "source": [
    "# Re-Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca0dc4b5-7cca-4a6d-99fd-6754ca833879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NC_FILE_PROCESSING.metrics_and_plots import *\n",
    "\n",
    "if EVALUATING_ON:\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise ValueError(\"There is a problem with Torch not recognizing the GPUs\")\n",
    "    \n",
    "    # Instantiate the model (must have the same architecture as when it was saved)\n",
    "    # Create an identical instance of the original __init__ parameters\n",
    "    loaded_model = IceForecastTransformer()\n",
    "    \n",
    "    # Load the saved state_dict (weights_only=True helps ensure safety of pickle files)\n",
    "    loaded_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # Move the model to the appropriate device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loaded_model.to(device)\n",
    "    \n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e362e-fc7b-4692-93aa-96dc68546d91",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e630a7cf-9771-4b4d-b266-459305b95876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EVALUATING_ON:\n",
    "    import io\n",
    "    import time # Import the time module for timing\n",
    "    from NC_FILE_PROCESSING.metrics_and_plots import * \n",
    "\n",
    "    start_full_evaluation = time.perf_counter()\n",
    "    \n",
    "    nCells_array = np.load('cellAreas.npy')\n",
    "    \n",
    "    # Create a string buffer to capture output\n",
    "    captured_output = io.StringIO()\n",
    "    \n",
    "    # Redirect stdout to the buffer\n",
    "    sys.stdout = captured_output\n",
    "    \n",
    "    print(\"\\nStarting evaluation and metric calculation...\")\n",
    "    logging.info(\"\\nStarting evaluation and metric calculation...\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: Batch Size: {BATCH_SIZE} Days\")\n",
    "    print(f\"DEBUG: Context Length: {CONTEXT_LENGTH} Days\")\n",
    "    print(f\"DEBUG: Forecast Horizon: {FORECAST_HORIZON} Days\")\n",
    "    print(f\"DEBUG: Number of batches in test_loader (with drop_last=True): {len(test_loader)} Batches\")\n",
    "    print(\"==================\")\n",
    "    print(f\"DEBUG: len(test_set): {len(dataset) - val_end} Days (approx, as test_set is not directly defined here)\")\n",
    "    print(f\"DEBUG: len(dataset) for splitting: {len(dataset)} Days\")\n",
    "    print(f\"DEBUG: train_end: {train_end}\")\n",
    "    print(f\"DEBUG: val_end: {val_end}\")\n",
    "    print(f\"DEBUG: range for test_set: {range(val_end, total_days)}\")\n",
    "    print(\"==================\")\n",
    "    \n",
    "    # --- Data Accumulators ---\n",
    "    all_abs_errors_spatial = [] # To store absolute errors for each cell in each patch (for spatial maps)\n",
    "    all_mse_errors_spatial = [] # To store MSE for each cell in each patch (for spatial maps)\n",
    "    \n",
    "    all_predicted_values_flat = [] # To accumulate all flattened predicted SIC values\n",
    "    all_actual_values_flat = []    # To accumulate all flattened actual SIC values\n",
    "\n",
    "    all_sic_temporal_results_dfs = [] # For SIC temporal degradation\n",
    "    all_sie_temporal_results_dfs = [] # For SIE temporal degradation\n",
    "\n",
    "    print(\"\\n--- Running Test Set Evaluation for Temporal Degradation ---\")\n",
    "    start_time_test_eval = time.perf_counter()\n",
    "\n",
    "    for i, (sample_x, sample_y, start_idx, end_idx, target_start_idx, target_end_idx) in enumerate(test_loader):\n",
    "        # Move data to device\n",
    "        sample_x = sample_x.to(device)\n",
    "        sample_y = sample_y.to(device)\n",
    "    \n",
    "        # Reshape for model input\n",
    "        B_sample, T_context, P_sample, C_sample, L_sample = sample_x.shape\n",
    "        sample_x_reshaped = sample_x.view(B_sample, T_context, P_sample, C_sample * L_sample)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "    \n",
    "        # Ensure output shape matches target shape\n",
    "        if predicted_y_patches.shape != sample_y.shape:\n",
    "            print(f\"Shape mismatch: Predicted {predicted_y_patches.shape}, Actual {sample_y.shape}\")\n",
    "            continue\n",
    "    \n",
    "        # Accumulate errors for spatial error calculation (these are still 4D tensors)\n",
    "        diff = predicted_y_patches - sample_y\n",
    "        all_abs_errors_spatial.append(torch.abs(diff).cpu())\n",
    "        # Ensure the operation is done on the tensor, then move to CPU\n",
    "        all_mse_errors_spatial.append((diff ** 2.0).cpu()) \n",
    "        \n",
    "        # Accumulate flattened values for overall SIC distribution and classification\n",
    "        all_predicted_values_flat.append(predicted_y_patches.cpu().numpy().flatten())\n",
    "        all_actual_values_flat.append(sample_y.cpu().numpy().flatten())\n",
    "    \n",
    "        # --- Logic for temporal degradation analysis (for SIC and SIE) ---\n",
    "        if dataset.times is not None:\n",
    "            batch_size, forecast_horizon = predicted_y_patches.shape[0], predicted_y_patches.shape[1]\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                base_date = dataset.times[target_start_idx[b]]\n",
    "                \n",
    "                for h in range(forecast_horizon):\n",
    "                    step_date = base_date + pd.Timedelta(days=h)\n",
    "                    \n",
    "                    # Flatten the data for this forecast step for SIC temporal analysis\n",
    "                    predicted_sic_step_flat = predicted_y_patches[b, h, :, :].cpu().numpy().flatten()\n",
    "                    actual_sic_step_flat = sample_y[b, h, :, :].cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Calculate SIE for this forecast step (squeeze to remove batch/time dim for calculate_sie)\n",
    "                    # predicted_sic_2d_for_sie has shape (NUM_PATCHES, CELLS_PER_PATCH)\n",
    "                    # actual_sic_2d_for_sie has shape (NUM_PATCHES, CELLS_PER_PATCH)\n",
    "                    predicted_sic_2d_for_sie = predicted_y_patches[b, h, :, :].cpu().numpy()\n",
    "                    actual_sic_2d_for_sie = sample_y[b, h, :, :].cpu().numpy()\n",
    "                    \n",
    "                    # --- Reconstruct full grid from patches for SIE calculation ---\n",
    "                    # Initialize full grid arrays with NaNs, using the total number of cells from the dataset's mesh\n",
    "                    total_grid_cells = len(dataset.latCell) # Assuming dataset.latCell holds the full number of cells\n",
    "                    recovered_predicted_grid = np.full(total_grid_cells, np.nan)\n",
    "                    recovered_actual_grid = np.full(total_grid_cells, np.nan)\n",
    "\n",
    "                    # Populate the full grid using the patch data and mapping\n",
    "                    # dataset.indices_per_patch_id contains masked indices\n",
    "                    # dataset.masked_to_full maps masked indices to full indices\n",
    "                    for patch_idx in range(P_sample): # P_sample is the number of patches in the batch\n",
    "                        masked_cell_indices_in_patch = dataset.indices_per_patch_id[patch_idx]\n",
    "\n",
    "                        # Get the values for this patch from the predicted/actual 2D arrays\n",
    "                        predicted_values_in_patch = predicted_sic_2d_for_sie[patch_idx]\n",
    "                        actual_values_in_patch = actual_sic_2d_for_sie[patch_idx]\n",
    "\n",
    "                        # Map these values back to the full grid using the masked_to_full mapping\n",
    "                        for i, masked_idx in enumerate(masked_cell_indices_in_patch):\n",
    "                            full_idx = dataset.masked_to_full[masked_idx]\n",
    "                            recovered_predicted_grid[full_idx] = predicted_values_in_patch[i]\n",
    "                            recovered_actual_grid[full_idx] = actual_values_in_patch[i]\n",
    "\n",
    "                    # Now pass the reconstructed full grids to the SIE calculation\n",
    "                    predicted_sie_km = calculate_full_sie_in_kilometers(recovered_predicted_grid, nCells_array)\n",
    "                    actual_sie_km = calculate_full_sie_in_kilometers(recovered_actual_grid, nCells_array)\n",
    "\n",
    "                    # Store SIC results for temporal degradation\n",
    "                    all_sic_temporal_results_dfs.append(pd.DataFrame({\n",
    "                        'predicted': predicted_sic_step_flat,\n",
    "                        'actual': actual_sic_step_flat,\n",
    "                        'date': step_date,\n",
    "                        'forecast_step': h + 1\n",
    "                    }))\n",
    "                    \n",
    "                    # Store SIE results for temporal degradation\n",
    "                    all_sie_temporal_results_dfs.append(pd.DataFrame({\n",
    "                        'predicted_sie_km': [predicted_sie_km],\n",
    "                        'actual_sie_km': [actual_sie_km],\n",
    "                        'date': [step_date],\n",
    "                        'forecast_step': [h + 1]\n",
    "                    }))\n",
    "    \n",
    "    # Concatenate all temporal results into single DataFrames for analysis\n",
    "    degradation_sic_df = pd.concat(all_sic_temporal_results_dfs, ignore_index=True) if all_sic_temporal_results_dfs else pd.DataFrame()\n",
    "    degradation_sie_df = pd.concat(all_sie_temporal_results_dfs, ignore_index=True) if all_sie_temporal_results_dfs else pd.DataFrame()\n",
    "\n",
    "    logging.info(\"\\nSaving csv's...\")\n",
    "    # Save degradation data\n",
    "    # The SIC per cell file is GB size; the SIE one is small (SIE as km^2 area averaged over all cells)\n",
    "    if not degradation_sic_df.empty:\n",
    "        degradation_sic_df.to_csv(f'{model_version}_sic_degradation.csv', index=False)\n",
    "        print(f\"Saved SIC performance degradation data as {model_version}_sic_degradation.csv\")\n",
    "    if not degradation_sie_df.empty:\n",
    "        degradation_sie_df.to_csv(f'{model_version}_sie_degradation.csv', index=False)\n",
    "        print(f\"Saved SIE performance degradation data as {model_version}_sie_degradation.csv\")\n",
    "\n",
    "    end_time_test_eval = time.perf_counter()\n",
    "    print(f\"Elapsed time for saving SIC and SIE performance degradation csvs: {end_time_test_eval - start_time_test_eval:.2f} seconds\")\n",
    "    \n",
    "    # --- Conditional Data collection for Training and Validation Sets ---\n",
    "    if PLOT_DATA_SPLIT_DISTRIBUTION:\n",
    "        print(\"--- Collecting ground truth data from training and validation sets ---\")\n",
    "        start_time_data_collection = time.perf_counter()\n",
    "        \n",
    "        all_train_actual_values = []\n",
    "        all_val_actual_values = []\n",
    "    \n",
    "        for i, (sample_x, sample_y, *_) in enumerate(train_loader):\n",
    "            all_train_actual_values.append(sample_y.cpu().numpy().flatten())\n",
    "        \n",
    "        for i, (sample_x, sample_y, *_) in enumerate(val_loader):\n",
    "            all_val_actual_values.append(sample_y.cpu().numpy().flatten())\n",
    "    \n",
    "        final_train_values = np.concatenate(all_train_actual_values)\n",
    "        final_val_values = np.concatenate(all_val_actual_values)\n",
    "        \n",
    "        end_time_data_collection = time.perf_counter()\n",
    "        elapsed_time_data_collection = end_time_data_collection - start_time_data_collection\n",
    "    else:\n",
    "        # Define empty arrays if not plotting distribution to avoid NameError later\n",
    "        final_train_values = np.array([])\n",
    "        final_val_values = np.array([])\n",
    "\n",
    "    logging.info(\"\\nConcatenating values...\")\n",
    "    # --- Final Data Concatenation for overall metrics ---\n",
    "    final_predicted_values = np.concatenate(all_predicted_values_flat) if all_predicted_values_flat else np.array([])\n",
    "    final_actual_values = np.concatenate(all_actual_values_flat) if all_actual_values_flat else np.array([])\n",
    "    \n",
    "    if PLOT_DATA_SPLIT_DISTRIBUTION:\n",
    "        print(f\"Time for training/validation data collection: {elapsed_time_data_collection:.2f} seconds\")\n",
    "    \n",
    "    # --- CALL FUNCTIONS HERE ---\n",
    "    \n",
    "    logging.info(\"\\nCalling SIC plotting functions...\")\n",
    "    # 1. Calculate and Log Overall Spatial Errors (Overall SIC's MAE, MSE, RMSE)\n",
    "    # This function calculates and prints overall spatial error metrics.\n",
    "    start_time_spatial_errors = time.perf_counter()\n",
    "    calculate_and_log_spatial_errors(degradation_sic_df, model_version, title_suffix=\" (Overall)\")\n",
    "    end_time_spatial_errors = time.perf_counter()\n",
    "    print(f\"Elapsed time for Overall Spatial Error Calculation: {end_time_spatial_errors - start_time_spatial_errors:.2f} seconds\")\n",
    "\n",
    "    # 2. Plot Temporal Degradation (SIC Over Each Forecast Day)\n",
    "    # This function plots MAE and RMSE degradation over the forecast horizon for SIC.\n",
    "    start_time_sic_temporal_degradation = time.perf_counter()\n",
    "    if not degradation_sic_df.empty:\n",
    "        plot_SIC_temporal_degradation(degradation_sic_df, model_version, patching_strategy_abbr)\n",
    "    end_time_sic_temporal_degradation = time.perf_counter()\n",
    "    print(f\"Elapsed time for SIC Temporal Degradation Plot: {end_time_sic_temporal_degradation - start_time_sic_temporal_degradation:.2f} seconds\")\n",
    "\n",
    "    # 3. Plot Actual vs. Predicted SIC Distribution (Overall SIC)\n",
    "    # This function plots overlapping histograms of actual vs. predicted SIC values.\n",
    "    start_time_actual_vs_predicted_sic_dist = time.perf_counter()\n",
    "    plot_actual_vs_predicted_sic_distribution(final_actual_values, final_predicted_values, model_version, patching_strategy_abbr, num_bins=50, title_suffix=\" (Overall)\")\n",
    "    end_time_actual_vs_predicted_sic_dist = time.perf_counter()\n",
    "    print(f\"Elapsed time for Overall Actual vs Predicted SIC histogram: {end_time_actual_vs_predicted_sic_dist - start_time_actual_vs_predicted_sic_dist:.2f} seconds\")\n",
    "\n",
    "    logging.info(\"\\nCalling SIE plotting functions...\")\n",
    "    # 4. Log Classification Report (SIE as a binary value of SIC with 15% threshold)\n",
    "    # This function provides a classification report for Sea Ice Extent (SIE).\n",
    "    start_time_classification_report = time.perf_counter()\n",
    "    # Assuming sie_threshold is defined elsewhere, e.g., sie_threshold = 0.15\n",
    "    sie_threshold = 0.15 # Define sie_threshold if not already defined\n",
    "    log_classification_report(final_actual_values, final_predicted_values, threshold=sie_threshold)\n",
    "    end_time_classification_report = time.perf_counter()\n",
    "    print(f\"Elapsed time for Overall Classification Report: {end_time_classification_report - start_time_classification_report:.2f} seconds\")\n",
    "    \n",
    "    # 5. Plot Overall SIE Confusion Matrix (SIE as a binary value of SIC with 15% threshold)\n",
    "    # This function generates a confusion matrix plot for SIE classification.\n",
    "    start_time_confusion_matrix = time.perf_counter()\n",
    "    plot_sie_confusion_matrix(degradation_sic_df, threshold=sie_threshold, model_version=model_version, patching_strategy_abbr=patching_strategy_abbr, forecast_day=None) # None for overall\n",
    "    end_time_confusion_matrix = time.perf_counter()\n",
    "    print(f\"Elapsed time for Overall Confusion Matrix Plot: {end_time_confusion_matrix - start_time_confusion_matrix:.2f} seconds\")\n",
    "    \n",
    "    # 6. Plot Overall ROC Curve (SIE as a binary value of SIC with 15% threshold)\n",
    "    # This function plots the Receiver Operating Characteristic (ROC) curve and calculates AUC.\n",
    "    start_time_roc_curve = time.perf_counter()\n",
    "    plot_roc_curve(degradation_sic_df, model_version=model_version, patching_strategy_abbr=patching_strategy_abbr, threshold=sie_threshold, forecast_day=None) # None for overall\n",
    "    end_time_roc_curve = time.perf_counter()\n",
    "    print(f\"Elapsed time for Overall ROC Curve Plot: {end_time_roc_curve - start_time_roc_curve:.2f} seconds\")\n",
    "\n",
    "    # 7. Plot F1-Score Degradation (SIE as a binary value of SIC with 15% threshold)\n",
    "    # This function plots the F1-score degradation for SIE classification.\n",
    "    start_time_f1_degradation = time.perf_counter()\n",
    "    if not degradation_sic_df.empty: # F1-score uses the same SIC temporal data\n",
    "        plot_SIE_f1_score_degradation(degradation_sic_df, model_version, patching_strategy_abbr, threshold=sie_threshold)\n",
    "    end_time_f1_degradation = time.perf_counter()\n",
    "    print(f\"Elapsed time for F1-Score Degradation Plot: {end_time_f1_degradation - start_time_f1_degradation:.2f} seconds\")\n",
    "\n",
    "    # 8. Plot SIE Degradation (SIE as the area that is ice in km^2)\n",
    "    # This function plots MAE and RMSE degradation over the forecast horizon for SIE in km^2.\n",
    "    start_time_sie_kilometers_degradation = time.perf_counter()\n",
    "    if not degradation_sie_df.empty:\n",
    "        plot_SIE_Kilometers_degradation(degradation_sie_df, model_version, patching_strategy_abbr)\n",
    "    end_time_sie_kilometers_degradation = time.perf_counter()\n",
    "    print(f\"Elapsed time for SIE Kilometers Degradation Plot: {end_time_sie_kilometers_degradation - start_time_sie_kilometers_degradation:.2f} seconds\")\n",
    "\n",
    "    # 9. Conditional Plotting All Three SIC Distributions (Train, Val, Test Sets)\n",
    "    # This block plots the distribution of SIC values across training, validation, and test sets.\n",
    "    if PLOT_DATA_SPLIT_DISTRIBUTION:\n",
    "        logging.info(\"\\nPlotting data distributions (Train, Val, Test)...\")\n",
    "        print(\"\\n--- Plotting data distributions (Train, Val, Test) ---\")\n",
    "        start_time_plot_data_dist = time.perf_counter()\n",
    "        plot_sic_distribution_bars(\n",
    "            train_data=final_train_values,\n",
    "            val_data=final_val_values,\n",
    "            test_data=final_actual_values,\n",
    "            start_date=train_set_start_year,\n",
    "            end_date=test_set_end_year,\n",
    "            num_bins=10\n",
    "        )\n",
    "        end_time_plot_data_dist = time.perf_counter()\n",
    "        print(f\"Elapsed time for plotting data distribution comparison: {end_time_plot_data_dist - start_time_plot_data_dist:.2f} seconds\")\n",
    "\n",
    "        # Calculate Pairwise Jensen-Shannon Distances for Data Splits\n",
    "        # This calculates the Jensen-Shannon Distance between the distributions of the data splits.\n",
    "        print(\"\\n--- Pairwise Jensen-Shannon Distances for Data Splits ---\")\n",
    "        start_time_jsd_pairwise = time.perf_counter()\n",
    "        distributions_for_jsd = {\n",
    "            'train': final_train_values,\n",
    "            'validation': final_val_values,\n",
    "            'test': final_actual_values\n",
    "        }\n",
    "        jsd_bins = np.linspace(0, 1, 10 + 1) # 10 bins for JSD calculation\n",
    "        pairwise_jsd_results = jensen_shannon_distance_pairwise(distributions_for_jsd, jsd_bins)\n",
    "        \n",
    "        for pair, jsd_val in pairwise_jsd_results.items():\n",
    "            print(f\"JSD ({pair}): {jsd_val:.4f}\")\n",
    "        end_time_jsd_pairwise = time.perf_counter()\n",
    "        print(f\"Elapsed time for Jensen Shannon Pairwise Calculation: {end_time_jsd_pairwise - start_time_jsd_pairwise:.2f} seconds\")\n",
    "\n",
    "    if PLOT_DAY_BY_DAY_METRICS:\n",
    "        \n",
    "        logging.info(\"\\nPlotting per-day forecast analysis...\")\n",
    "        # --- Optional: Per-Day Analysis for Classification Metrics and Distributions ---\n",
    "        # This section iterates through each forecast day to provide detailed metrics and plots.\n",
    "        print(\"\\n############################################\")\n",
    "        print(\"\\n#   PER-DAY FORECAST ANALYSIS (Optional)   #\")\n",
    "        print(\"\\n############################################\")\n",
    "        start_time_per_day_analysis = time.perf_counter()\n",
    "        for day in range(1, FORECAST_HORIZON + 1): # Loop through each forecast day\n",
    "            df_day = degradation_sic_df[degradation_sic_df['forecast_step'] == day]\n",
    "            if not df_day.empty:\n",
    "                print(f\"\\n--- Metrics for Forecast Day {day} ---\")\n",
    "                \n",
    "                # Log Classification Report for specific day\n",
    "                log_classification_report(df_day['actual'].values, df_day['predicted'].values, threshold=sie_threshold)\n",
    "                \n",
    "                # Plot SIC distribution for specific day\n",
    "                plot_actual_vs_predicted_sic_distribution(\n",
    "                    df_day['actual'].values, df_day['predicted'].values, model_version, patching_strategy_abbr, num_bins=50, title_suffix=f\" (Day {day})\"\n",
    "                )\n",
    "    \n",
    "                # Plot Confusion Matrix for specific day\n",
    "                plot_sie_confusion_matrix(degradation_sic_df, sie_threshold, model_version, patching_strategy_abbr, forecast_day=day)\n",
    "                \n",
    "                # Plot ROC Curve for specific day\n",
    "                plot_roc_curve(degradation_sic_df, model_version, patching_strategy_abbr, sie_threshold, forecast_day=day)\n",
    "        \n",
    "        end_time_per_day_analysis = time.perf_counter()\n",
    "        print(f\"Elapsed time for Per-Day Forecast Analysis: {end_time_per_day_analysis - start_time_per_day_analysis:.2f} seconds\")\n",
    "\n",
    "    # END OF EVALUATION\n",
    "    end_full_evaluation = time.perf_counter()\n",
    "    print(f\"Elapsed time for FULL EVALUATION: {end_full_evaluation - start_full_evaluation:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete.\")\n",
    "\n",
    "    # Restore stdout\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    # Now, write the captured output to the file\n",
    "    with open(f'{model_version}_Metrics.txt', 'w') as f:\n",
    "        f.write(captured_output.getvalue())\n",
    "    \n",
    "    print(f\"Metrics saved as {model_version}_Metrics.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7955-7de9-4ac1-ae71-ef1f7bedd950",
   "metadata": {},
   "source": [
    "# Make a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e76ed2c4-bee4-4fd0-81b6-79a3e79ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATING_ON:\n",
    "    if MAP_WITH_CARTOPY_ON:\n",
    "        # Load one batch\n",
    "        data_iter = iter(test_loader)\n",
    "        sample_x, sample_y, start_idx, end_idx, target_start, target_end = next(data_iter)\n",
    "        \n",
    "        print(f\"Shape of sample_x {sample_x.shape}\")\n",
    "        print(f\"Shape of sample_y {sample_y.shape}\")   \n",
    "        \n",
    "        print(f\"Fetched sample_x start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "        print(f\"Fetched sample_x end   index {end_idx}:   Time={dataset.times[end_idx]}\")\n",
    "        \n",
    "        print(f\"Fetched sample_y (target) start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "        print(f\"Fetched sample_y (target) end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "        \n",
    "        # Move to device and apply initial reshape as done in training\n",
    "        sample_x = sample_x.to(device)\n",
    "        sample_y = sample_y.to(device) # Keep sample_y for actual comparison\n",
    "        \n",
    "        # Initial reshape of x for the Transformer model\n",
    "        B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "        sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "        \n",
    "        print(f\"Sample x for inference shape (reshaped): {sample_x_reshaped.shape}\")\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "            predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "        \n",
    "        print(f\"Predicted y patches shape: {predicted_y_patches.shape}\")\n",
    "        print(f\"Expected shape: (B, forecast_horizon, NUM_PATCHES, CELLS_PER_PATCH) ex., (16, {loaded_model.forecast_horizon}, 140, 256)\")\n",
    "                         \n",
    "        # Option 1: Select a specific day from the forecast horizon (ex., the first day)\n",
    "        # This is the shape (B, NUM_PATCHES, CELLS_PER_PATCH) for that specific day.\n",
    "        predicted_for_day_0 = predicted_y_patches[:, 0, :, :].cpu()\n",
    "        print(f\"Predicted ice area for Day 0 (specific day) shape: {predicted_for_day_0.shape}\")\n",
    "        \n",
    "        # Ensure sample_y has the same structure\n",
    "        actual_for_day_0 = sample_y[:, 0, :, :].cpu()\n",
    "        print(f\"Actual ice area for Day 0 (specific day) shape: {actual_for_day_0.shape}\")\n",
    "        \n",
    "        # Save predictions so that I can use cartopy by switching kernels for the next jupyter cell\n",
    "        np.save(f'patches/ice_area_patches_predicted_{PATH}_day0.npy', predicted_for_day_0)\n",
    "        np.save(f'patches/ice_area_patches_actual_{PATH}_day0.npy', actual_for_day_0)\n",
    "    \n",
    "        # Option 2: Iterate through all forecast days\n",
    "        all_predicted_ice_areas = []\n",
    "        all_actual_ice_areas = []\n",
    "        \n",
    "        for day_idx in range(loaded_model.forecast_horizon):\n",
    "            predicted_day = predicted_y_patches[:, day_idx, :, :].cpu()\n",
    "            all_predicted_ice_areas.append(predicted_day)\n",
    "        \n",
    "            actual_day = sample_y[:, day_idx, :, :].cpu()\n",
    "            all_actual_ice_areas.append(actual_day)\n",
    "        \n",
    "            print(f\"Processing forecast day {day_idx}: Predicted shape {predicted_day.shape}, Actual shape {actual_day.shape}\")\n",
    "        \n",
    "            # Save each day's prediction/actual data if needed\n",
    "            # np.save(f'patches/ice_area_patches_predicted_day{day_idx}.npy', predicted_day)\n",
    "            # np.save(f'patches/ice_area_patches_actual_day{day_idx}.npy', actual_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Recover nCells from Patches for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAP_WITH_CARTOPY_ON:\n",
    "\n",
    "    ########################################\n",
    "    # SWAP KERNELS IN THE JUPYTER NOTEBOOK #\n",
    "    ########################################\n",
    "    \n",
    "    from MAP_ANIMATION_GENERATION.map_gen_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.nc_utility_functions import *\n",
    "    from NC_FILE_PROCESSING.patchify_utils import *\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    predicted_ice_area_patches = np.load(f'patches/SIC_predicted_{model_version}_day0.npy')\n",
    "    actual_y_ice_area_patches = np.load(f'patches/SIC_actual_{model_version}_day0.npy')\n",
    "    \n",
    "    NUM_PATCHES = len(predicted_ice_area_patches[0])\n",
    "    print(\"NUM_PATCHES is\", NUM_PATCHES)\n",
    "    \n",
    "    latCell, lonCell = load_mesh(perlmutterpathMesh)\n",
    "    TOTAL_GRID_CELLS = len(lonCell) \n",
    "    cell_mask = latCell >= LATITUDE_THRESHOLD\n",
    "    \n",
    "    # Extract Freeboard (index 0) and Ice Area (index 1) for predicted and actual\n",
    "    # Predicted output is (B, 1, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "    # Assuming the model predicts ice area, which is the second feature (index 1)\n",
    "    # if the output of the model aligns with the order of features *within* the original patch_dim.\n",
    "    \n",
    "    # Load the original patch-to-cell mapping\n",
    "    # indices_per_patch_id = [\n",
    "    #     [idx_cell_0_0, ..., idx_cell_0_255],\n",
    "    #     [idx_cell_1_0, ..., idx_cell_1_255],\n",
    "    #     ...\n",
    "    # ]\n",
    "    \n",
    "    full_nCells_patch_ids, indices_per_patch_id, patch_latlons = patchify_by_latlon_spillover(\n",
    "                latCell, lonCell, k=256, max_patches=NUM_PATCHES, LATITUDE_THRESHOLD=LATITUDE_THRESHOLD)\n",
    "    \n",
    "    # Select one sample from the batch for visualization (ex., the first one)\n",
    "    # Output is (NUM_PATCHES, CELLS_PER_PATCH) for this single sample\n",
    "    sample_predicted_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    sample_actual_cells_per_patch = predicted_ice_area_patches[2] # First item in batch\n",
    "    \n",
    "    # Initialize empty arrays for the full grid (nCells)\n",
    "    recovered_predicted_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    recovered_actual_grid = np.full(TOTAL_GRID_CELLS, np.nan)\n",
    "    \n",
    "    # Populate the full grid using the patch data and mapping\n",
    "    for patch_idx in range(NUM_PATCHES):\n",
    "        cell_indices_in_patch = indices_per_patch_id[patch_idx]\n",
    "        \n",
    "        # For predicted values\n",
    "        recovered_predicted_grid[cell_indices_in_patch] = sample_predicted_cells_per_patch[patch_idx]\n",
    "        nan_mask = np.isnan(recovered_predicted_grid)\n",
    "        nan_count = np.sum(nan_mask)\n",
    "    \n",
    "        # For actual values\n",
    "        recovered_actual_grid[cell_indices_in_patch] = sample_actual_cells_per_patch[patch_idx]\n",
    "    \n",
    "    print(f\"Recovered predicted grid shape: {recovered_predicted_grid.shape}\")\n",
    "    print(f\"Recovered actual grid shape: {recovered_actual_grid.shape}\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_predicted_grid, f\"model {model_version} ice area recovered\")\n",
    "    \n",
    "    fig, northMap = generate_axes_north_pole()\n",
    "    generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_actual_grid, f\"model {model_version} ice area actual\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIC SIE Kernel",
   "language": "python",
   "name": "sic_sie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
