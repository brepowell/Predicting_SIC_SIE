{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a28e7d-dc79-40f6-a74a-8a679428fce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b09f00-1689-4691-8150-4c794246e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('System Version:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6360503b-6b8b-469e-87f0-2728b3f06a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.executable) # for troubleshooting kernel issues\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c8c6ca-37fa-4862-ae15-9533e9be2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e01aaf1-72c2-4519-8686-940b4479f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy version', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f86c23-cfd2-48cd-8f60-3a22f2dc095b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d488e-9fc4-471d-890f-66512f9f8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "print('Cudf version', cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb8e5c-b522-4079-a77a-781ef3898b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e968c7-1f0b-4bec-ab94-febc3c468ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import cupy_xarray # This registers the .cupy namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3feac-5e19-413f-8662-5e4f318d3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list | grep \"xarray\\|cupy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4c9f4-1822-477f-be68-9355e0ca1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Matplotlib version', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23254110-90d5-4ea1-9e3d-dfc0c8f4ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3720a6-37d6-40aa-acb0-5aa1cd3cee40",
   "metadata": {},
   "source": [
    "# Hardware Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ead02e-ae14-41bb-a7fd-6242e11003f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count()) # check the number of available CUDA devices\n",
    "# will print 1 on login node; 4 on GPU exclusive node; 1 on shared GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6e907-60d0-4502-9123-e183f25c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.get_device_properties(0)) #provides information about a specific GPU\n",
    "\n",
    "#total_memory=40326MB, multi_processor_count=108, L2_cache_size=40MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc99a47-c7fa-477c-a697-00e3f34ece6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "# Get general CPU information\n",
    "processor_name = platform.processor()\n",
    "print(f\"Processor Name: {processor_name}\")\n",
    "\n",
    "# Get core counts\n",
    "physical_cores = psutil.cpu_count(logical=False)\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Physical Cores: {physical_cores}\")\n",
    "print(f\"Logical Cores: {logical_cores}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_frequency = psutil.cpu_freq()\n",
    "if cpu_frequency:\n",
    "    print(f\"Current CPU Frequency: {cpu_frequency.current:.2f} MHz\")\n",
    "    print(f\"Min CPU Frequency: {cpu_frequency.min:.2f} MHz\")\n",
    "    print(f\"Max CPU Frequency: {cpu_frequency.max:.2f} MHz\")\n",
    "\n",
    "# Get CPU utilization (percentage)\n",
    "# The interval argument specifies the time period over which to measure CPU usage.\n",
    "# Setting percpu=True gives individual core utilization.\n",
    "cpu_percent_total = psutil.cpu_percent(interval=1)\n",
    "print(f\"Total CPU Usage: {cpu_percent_total}%\")\n",
    "\n",
    "# cpu_percent_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "# print(\"CPU Usage per Core:\")\n",
    "# for i, percent in enumerate(cpu_percent_per_core):\n",
    "#     print(f\"  Core {i+1}: {percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd50d6-18a5-4f14-94c6-56d69814e0e6",
   "metadata": {},
   "source": [
    "# Example of one netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4edde5-7f16-4f36-a069-b97fb9844378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"train/v3.LR.DTESTM.pm-cpu-10yr.mpassi.hist.am.timeSeriesStatsDaily.0010-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955c68e-3428-4d41-b556-16226329b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e633a4-1006-47dd-a2bf-52ee17e6c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_counter = ds[\"timeDaily_counter\"]\n",
    "day_counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552710e3-15d2-4a86-b14f-3bc036d7e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[\"xtime_startDaily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90545b27-4a01-4421-91b1-aed28af2282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[\"xtime_startDaily\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832e806-4dbd-4139-b08a-546abcac3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_area = ds[\"timeDaily_avg_iceAreaCell\"]\n",
    "ice_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d6581-da44-4d6f-9582-908d0a86581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c376f4-a54c-489d-bd15-2130fe8eab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.coords)\n",
    "print(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949ea59-c1d7-410f-9bba-b02b4facb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e89844-e98f-4b42-b3cd-e3593ea3f151",
   "metadata": {},
   "source": [
    "# Example of Mesh File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8b6f4-6969-45c9-8859-709234040954",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(\"NC_FILE_PROCESSING/mpassi.IcoswISC30E3r5.20231120.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4df70-fa5a-4c94-8be1-f93caae8209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40476985-dfb0-4da2-b48e-eb2c97f74f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cellsOnCell = mesh[\"cellsOnCell\"].values\n",
    "print(mesh[\"cellsOnCell\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cbd2b-ae4f-41d4-b8c1-213fb92b3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mesh[\"cellsOnCell\"].max().values)\n",
    "print(mesh[\"cellsOnCell\"].min().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21202b02-d87e-4354-aea7-586babbfb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.save('cellsOnCell.npy', cellsOnCell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0aab3b-733a-4f9f-ad63-e0a510b05cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mesh.coords)\n",
    "print(mesh.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816fde5-ec19-4430-9875-f4e8cadc10d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff3ae1-5dc1-4ac7-af6d-eaa97bee17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679cef-dfd2-46e8-9dfe-6c9b519add06",
   "metadata": {},
   "source": [
    "# Pre-processing + Freeboard calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9a215-6126-459b-8df2-3842efb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust if you use different units)\n",
    "D_WATER = 1023  # Density of seawater (kg/m^3)\n",
    "D_ICE = 917     # Density of sea ice (kg/m^3)\n",
    "D_SNOW = 330    # Density of snow (kg/m^3)\n",
    "\n",
    "MIN_AREA = 1e-6\n",
    "\n",
    "def compute_freeboard(area: cp.ndarray, \n",
    "                      ice_volume: cp.ndarray, \n",
    "                      snow_volume: cp.ndarray) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sea ice freeboard from ice and snow volume and area.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    area : cp.ndarray\n",
    "        Sea ice concentration / area (same shape as ice_volume and snow_volume).\n",
    "    ice_volume : cp.ndarray\n",
    "        Sea ice volume per grid cell.\n",
    "    snow_volume : cp.ndarray\n",
    "        Snow volume per grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freeboard : cp.ndarray\n",
    "        Freeboard height for each cell, same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Initialize arrays\n",
    "    height_ice = cp.zeros_like(ice_volume)\n",
    "    height_snow = cp.zeros_like(snow_volume)\n",
    "\n",
    "    # Valid mask: avoid dividing by very small or zero area\n",
    "    valid = area > MIN_AREA\n",
    "\n",
    "    # Safely compute heights where valid\n",
    "    height_ice[valid] = ice_volume[valid] / area[valid]\n",
    "    height_snow[valid] = snow_volume[valid] / area[valid]\n",
    "\n",
    "    # Compute freeboard using the physical formula\n",
    "    freeboard = (\n",
    "        height_ice * (D_WATER - D_ICE) / D_WATER +\n",
    "        height_snow * (D_WATER - D_SNOW) / D_WATER\n",
    "    )\n",
    "\n",
    "    return freeboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a739e27-622f-4a24-b031-270622c5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freeboard(freeboard, min_val=-0.2, max_val=1.2):\n",
    "    return cp.clip((freeboard - min_val) / (max_val - min_val), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5856f-e996-4eee-b5aa-941ccd127b88",
   "metadata": {},
   "source": [
    "# Custom Pytorch Dataset\n",
    "\n",
    "Example from NERSC of using ERA5 Dataset:\n",
    "https://github.com/NERSC/dl-at-scale-training/blob/main/utils/data_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7df3ff-a8ea-46cc-9053-97bfbb41cea8",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "TRY: NUM_WORKERS as 16 to 32 - profile to see if the GPU is still waiting on the CPU.\n",
    "\n",
    "TRY: NUM_WORKERS as 64 - the number of CPU cores available.\n",
    "\n",
    "TRY: NUM_WORKERS experiment with os.cpu_count() - 2\n",
    "\n",
    "TRY: NUM_WORKERS experiment with (logical_cores_per_gpu * num_gpus)\n",
    "\n",
    "num_workers considerations:\n",
    "Too few workers: GPUs might become idle waiting for data.\n",
    "Too many workers: Can lead to increased CPU memory usage and context switching overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646812d0-b13a-4fff-a261-9fb5f6835cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 64\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2488a-5e61-4012-9a47-9f231de6ecd2",
   "metadata": {},
   "source": [
    "# __ init __ - masks and loads the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e4d81-da70-4886-869a-7e69db40fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Callable, Tuple\n",
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "from perlmutterpath import * # Contains the data_dir and mesh_dir variables\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set level to logging.INFO to see the statements\n",
    "logging.basicConfig(filename='DailyNetCDFDataset.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "class DailyNetCDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that concatenates a directory of month-wise NetCDF files\n",
    "    along their 'Time' dimension and yields daily data *plus* its timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Directory containing NetCDF files\n",
    "    transform : Callable | None\n",
    "        Optional transform applied to the data tensor *only*.\n",
    "    decode_time : bool\n",
    "        Let xarray convert CF-style time coordinates to cp.datetime64.\n",
    "    drop_missing : bool\n",
    "        If True, drops any days where one of the requested variables is missing.\n",
    "    latitude_threshold\n",
    "        The minimum latitude to use for Artic data\n",
    "    context_length\n",
    "        The number of days to fetch for input in the prediction step\n",
    "    forecast_horizon\n",
    "        The number of days to predict in the future\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        mesh_dir: str = mesh_dir,\n",
    "        transform: Callable = None,\n",
    "        decode_time: bool = True,\n",
    "        drop_missing: bool = True,\n",
    "        latitude_threshold = 40,\n",
    "        context_length = 7,\n",
    "        forecast_horizon = 1\n",
    "    ):\n",
    "\n",
    "        \"\"\" __init__ needs to \n",
    "\n",
    "        Handle the raw data:\n",
    "        1) Gather the sorted daily data from each netCDF file (1 file = 1 month of daily data)\n",
    "            The netCDF files contain nCells worth of data per day for each feature (ice area, ice volume, etc.)\n",
    "            nCells = 465044 with the IcoswISC30E3r5 mesh\n",
    "        2) Store the datetime information from each nCells array from the daily data\n",
    "        3) Extract raw data\n",
    "        \n",
    "        Perform pre-processing:\n",
    "        4) Apply a mask to nCells to look just at regions in certain latitudes\n",
    "            nCells >= 40 degrees is 53973 cells\n",
    "            nCells >= 50 degrees is 35623 cells\n",
    "        5) Derive Freeboard from ice area, snow volume, and ice volume\n",
    "        6) Custom patchify and store patch_ids so the data loader can use them\n",
    "        7) Concatenate the data across Time\n",
    "        8) Normalize the data (Ice area is already between 0 and 1; Freeboard is not) \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.transform = transform\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # --- 1. Gather files (sorted for deterministic order) ---------\n",
    "        self.data_dir = data_dir\n",
    "        self.mesh_dir = mesh_dir\n",
    "        self.file_paths = sorted(\n",
    "            [\n",
    "                os.path.join(data_dir, f)\n",
    "                for f in os.listdir(data_dir)\n",
    "                if f.endswith(\".nc\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Found {len(self.file_paths)} NetCDF files:\")\n",
    "        # for f in self.file_paths:\n",
    "        #     logging.info(f\"  - {f}\")     # Print all the file names in the folder\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"No *.nc files found in {data_dir!r}\")\n",
    "\n",
    "        # Open all the netCDF files and concatenate them along Time dimension\n",
    "        logging.info(\"Loading datasets with xarray.open_mfdataset...\")\n",
    "        \n",
    "        # --- 2. Store a list of datetimes from each file -> helps with retrieving 1 day's data later\n",
    "        # This happens on the CPU\n",
    "        all_times = []\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "        \n",
    "            # Decode byte strings and fix the format\n",
    "            xtime_strs = ds[\"xtime_startDaily\"].str.decode(\"utf-8\").values\n",
    "            xtime_strs = [s.replace(\"_\", \" \") for s in xtime_strs]  # \"0010-01-01_00:00:00\" → \"0010-01-01 00:00:00\"\n",
    "        \n",
    "            # Convert to datetime.datetime objects\n",
    "            times = [datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\") for s in xtime_strs]\n",
    "            all_times.extend(times)\n",
    "        \n",
    "        # Store in self.times\n",
    "        self.times = all_times\n",
    "        self.times = cudf.to_datetime(all_times)\n",
    "\n",
    "        # Checking the dates\n",
    "        logging.info(f\"Parsed {len(self.times)} total dates\")\n",
    "        logging.info(f\"First few: {str(self.times[:5])}\")\n",
    "\n",
    "        # Stats on how many dates there are\n",
    "        logging.info(f\"Total days collected: {len(self.times)}\")\n",
    "        logging.info(f\"Unique days: {len(self.times.unique())}\") # Use .unique() for cudf Series\n",
    "        logging.info(f\"First 35 days: {self.times[:35]}\")\n",
    "        logging.info(f\"First days 360 to 400 days: {self.times[360:401]}\")\n",
    "\n",
    "        # Load the mesh file and create a mask. Latitudes and Longitudes are in radians.\n",
    "        latCell, lonCell = load_mesh_radians(self.mesh_dir)\n",
    "        latCell = cp.array(latCell)\n",
    "        lonCell = cp.array(lonCell)\n",
    "        latCell = cp.degrees(latCell)\n",
    "        lonCell = cp.degrees(lonCell)\n",
    "        print(type(latCell))\n",
    "        \n",
    "        self.cell_mask = latCell >= latitude_threshold\n",
    "        logging.info(f\"Mask size: {cp.count_nonzero(self.cell_mask)}\")\n",
    "\n",
    "        self.full_to_masked = {\n",
    "            int(full_idx): new_idx\n",
    "            for new_idx, full_idx in enumerate(cp.where(self.cell_mask)[0])\n",
    "        }\n",
    "\n",
    "        # --- 3. Extract raw data \n",
    "        self.freeboard_all = []\n",
    "        self.ice_area_all = []\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # Extract raw data\n",
    "            area = cp.array(ds[\"timeDaily_avg_iceAreaCell\"].values)\n",
    "            ice_volume = cp.array(ds[\"timeDaily_avg_iceVolumeCell\"].values)\n",
    "            snow_volume = cp.array(ds[\"timeDaily_avg_snowVolumeCell\"].values)\n",
    "\n",
    "            # --- 4. Apply a mask to the nCells\n",
    "            area = area[:, self.cell_mask]\n",
    "            ice_volume = ice_volume[:, self.cell_mask]\n",
    "            snow_volume = snow_volume[:, self.cell_mask]\n",
    "\n",
    "            # --- 5. Derive Freeboard from ice area, snow volume and ice volume\n",
    "            freeboard = compute_freeboard(area, ice_volume, snow_volume)\n",
    "\n",
    "            # These will be deleted later to save space\n",
    "            self.freeboard_all.append(freeboard) \n",
    "            self.ice_area_all.append(area)\n",
    "\n",
    "        # --- 6. Custom patchify function       \n",
    "        self.full_nCells_patch_ids, self.indices_per_patch_id = patchify_by_latlon_spillover(\n",
    "            latCell, lonCell, k=256, max_patches=140, lat_threshold=latitude_threshold)\n",
    "\n",
    "        # Convert full-domain patch indices to masked-domain indices\n",
    "        self.indices_per_patch_id = [\n",
    "            [self.full_to_masked[i] for i in patch if i in self.full_to_masked]\n",
    "            for patch in self.indices_per_patch_id\n",
    "        ]\n",
    "\n",
    "        # --- 7. Concatenate the data across Time\n",
    "\n",
    "        # Concatenate across time\n",
    "        self.freeboard = cp.concatenate(self.freeboard_all, axis=0)  # (T, nCells)\n",
    "        self.ice_area = cp.concatenate(self.ice_area_all, axis=0)    # (T, nCells)\n",
    "\n",
    "        # Discard the lists that are not needed anymore -- save space\n",
    "        del self.freeboard_all, self.ice_area_all\n",
    "\n",
    "        logging.info(f\"Freeboard {self.freeboard.shape}\")\n",
    "        logging.info(f\"Ice Area  {self.ice_area.shape}\")\n",
    "\n",
    "        # --- 8. Normalize the data (Area is already between 0 and 1; Freeboard is not)\n",
    "        self.freeboard_min = self.freeboard[0].min()\n",
    "        self.freeboard_max = self.freeboard[0].max()\n",
    "        \n",
    "        logging.info(f\"Freeboard min: {self.freeboard_min}\" )\n",
    "        logging.info(f\"Freeboard max: {self.freeboard_max}\")\n",
    "\n",
    "        self.freeboard_all = normalize_freeboard(\n",
    "            freeboard, min_val=self.freeboard_min, max_val=self.freeboard_max)\n",
    "\n",
    "        logging.info(\"=== Normalized Freeboard ===\")\n",
    "        logging.info(\"End of __init__\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of possible starting indices (idx) for a valid sequence.\n",
    "        A valid sequence needs `self.context_length` days for input and `self.forecast_horizon` days for target.\n",
    "        \n",
    "        ex) If the total number of days is 365, the context_length is 7 and the forecast_horizon is 3, then\n",
    "        last valid starting index = total days - (context length + forecast horizon) + 1\n",
    "        365 - (7 + 3) + 1 = 365 - 10 + 1 = 356 valid starting indices\n",
    "        \"\"\"\n",
    "        required_length = self.context_length + self.forecast_horizon\n",
    "        if len(self.freeboard) < required_length:\n",
    "            return 0 # Not enough raw data to form even one sample\n",
    "\n",
    "        # The total number of valid starting indices\n",
    "        return len(self.freeboard) - required_length + 1\n",
    "\n",
    "    def get_patch_tensor(self, day_idx: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Retrieves the feature data for a specific day, organized into patches.\n",
    "\n",
    "        This method extracts 'freeboard' and 'ice_area' data for a given day\n",
    "        and then reshapes it according to the pre-defined patches. Each patch\n",
    "        will contain its own set of feature values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        day_idx : int\n",
    "            The integer index of the day to retrieve data for, relative to the\n",
    "            concatenated dataset's time dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing the feature data organized by patches for the\n",
    "            specified day.\n",
    "            Shape: (num_patches, num_features, patch_size)\n",
    "            Where:\n",
    "            - num_patches: Total number of patches (ex., 140).\n",
    "            - num_features: The number of features per cell (currently 2: freeboard, ice_area).\n",
    "            - patch_size: The number of cells within each patch.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        freeboard_day = self.freeboard[day_idx]  # (nCells,)\n",
    "        ice_area_day = self.ice_area[day_idx]    # (nCells,)\n",
    "        features = cp.stack([freeboard_day, ice_area_day], axis=0)  # (2, nCells)\n",
    "        patch_tensors = []\n",
    "\n",
    "        for patch_indices in self.indices_per_patch_id:\n",
    "            patch = features[:, patch_indices]  # (2, patch_size)\n",
    "            patch_tensors.append(torch.tensor(patch, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(patch_tensors)  # (context_length, num_patches, num_features, patch_size)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, np.datetime64]:\n",
    "\n",
    "        \"\"\"__ getitem __ needs to \n",
    "        \n",
    "        1. Given an input of a certain date id, get the input and the target tensors\n",
    "        2. Return all the patches for the input and the target\n",
    "           Features are: [freeboard, ice_area] over masked cells. \n",
    "           \n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        logging.info(\"Calling __getitem__\")\n",
    "\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.context_length\n",
    "        target_start = end_idx + 1              # added this - TODO - CHECK FOR ERRORS\n",
    "        target_end = end_idx + self.forecast_horizon\n",
    "\n",
    "        if target_end > len(self.freeboard):\n",
    "            raise IndexError(\"Requested time window exceeds dataset\")\n",
    "        \n",
    "        # Build input tensor\n",
    "        input_seq = [self.get_patch_tensor(i) for i in range(start_idx, end_idx)]\n",
    "        input_tensor = torch.stack(input_seq)\n",
    "    \n",
    "        # Build target tensor: shape (forecast_horizon, num_patches)\n",
    "        target_seq = self.ice_area[end_idx:target_end]  # (forecast_horizon, nCells)\n",
    "        target_patches = []\n",
    "        for day in target_seq:\n",
    "            patch_day = [\n",
    "                torch.tensor(day[patch_indices]) for patch_indices in self.indices_per_patch_id\n",
    "            ]\n",
    "            patch_day_tensor = torch.stack(patch_day)  # (num_patches,)\n",
    "            target_patches.append(patch_day_tensor)\n",
    "        \n",
    "        target_tensor = torch.stack(target_patches)  # (forecast_horizon, num_patches)\n",
    "        \n",
    "        logging.info(f\"Input  tensor shape {input_tensor.shape}\")\n",
    "        logging.info(f\"Target tensor shape {target_tensor.shape}\")\n",
    "\n",
    "        logging.info(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "        logging.info(\"target_tensor should be of shape (forecast_horizon, num_patches, patch_size)\")\n",
    "\n",
    "        logging.info(f\"Fetched start index {start_idx}: Time={self.times[start_idx]}\")\n",
    "        logging.info(f\"Fetched end   index {end_idx}: Time={self.times[end_idx]}\")\n",
    "        \n",
    "        logging.info(f\"Fetched target start index {target_end}: Time={self.times[target_end]}\")\n",
    "        logging.info(f\"Fetched target end   index {target_end}: Time={self.times[target_end]}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "        return input_tensor, target_tensor, start_idx, end_idx, target_start, target_end # TODO, CHECK FOR ERRORS\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Format the string representation of the data \"\"\"\n",
    "        return (\n",
    "            f\"<DailyNetCDFDataset: {len(self)} days, \"\n",
    "            f\"{len(self.freeboard[0])} cells/day, \"\n",
    "            f\"{len(self.file_paths)} files loaded>\"\n",
    "        )\n",
    "\n",
    "    def time_to_dataframe(self) -> pd.DataFrame:\n",
    "            \"\"\"Return a DataFrame of time features you can merge with predictions.\"\"\"\n",
    "            t = pd.to_datetime(self.times)            # pandas Timestamp index\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"time\": t,\n",
    "                    \"year\": t.year,\n",
    "                    \"month\": t.month,\n",
    "                    \"day\": t.day,\n",
    "                    \"doy\": t.dayofyear,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972aabe-4e3e-4062-ab72-15c8318018b5",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b03083-9694-4237-b348-092ef3506ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print(\"===== Making the Dataset Class ===== \")\n",
    "\n",
    "# OPTION 1: LOADING FROM ONE BIG FOLDER:\n",
    "dataset = DailyNetCDFDataset(data_dir)\n",
    "\n",
    "total_days = len(dataset)\n",
    "train_end = int(total_days * 0.7)\n",
    "val_end = int(total_days * 0.85)\n",
    "\n",
    "train_set = Subset(dataset, range(0, train_end))\n",
    "val_set   = Subset(dataset, range(train_end, val_end))\n",
    "test_set  = Subset(dataset, range(val_end, total_days))\n",
    "\n",
    "print(\"Training data length:   \", len(train_set))\n",
    "print(\"Validation data length: \", len(val_set))\n",
    "print(\"Testing data length:    \", len(test_set))\n",
    "\n",
    "total_days = len(train_set) + len(val_set) + len(test_set)\n",
    "print(\"Total days = \", total_days)\n",
    "\n",
    "# OPTION 2: LOADING FROM SEPARATE FOLDERS:\n",
    "# train_dataset = DailyNetCDFDataset(data_dir=\"/train\", mesh_dir=mesh_dir)\n",
    "# val_dataset   = DailyNetCDFDataset(data_dir=\"/valid\", mesh_dir=mesh_dir)\n",
    "# test_dataset  = DailyNetCDFDataset(data_dir=\"/test\",  mesh_dir=mesh_dir)\n",
    "\n",
    "print(\"===== Printing Dataset ===== \")\n",
    "print(dataset)                 # calls __repr__ → see how many files & days loaded\n",
    "\n",
    "input_tensor, target_tensor, start_idx, end_idx, target_start, target_end = dataset[0]        # sample is tensor, ts is cp.datetime64\n",
    "\n",
    "print(f\"Fetched start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched end   index {end_idx}: Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched target start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "print(f\"Fetched target end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "print(\"===== Starting DataLoader ====\")\n",
    "# wrap in a DataLoader\n",
    "# 1. Use pinned memory for faster asynch transfer to GPUs)\n",
    "# 2. Use a prefetch factor so that the GPU is fed w/o a ton of CPU memory use\n",
    "# 3. Use shuffle=False to preserve time order (especially for forecasting)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "print(\"input_tensor should be of shape (context_length, num_patches, num_features, patch_size)\")\n",
    "print(\"target_tensor should be of shape (forecast_horizon, num_patches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3d47c-9e26-41da-9b6b-7afe0faf54ab",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Constants / Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6b153-3646-4b50-b1e7-2807eac982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 7         # T: Number of historical time steps used for input\n",
    "FORECAST_HORIZON = 1       # Number of future time steps to predict (ex. 1 day for next time step)\n",
    "NUM_PATCHES = 140          # P: Number of spatial patches\n",
    "NUM_FEATURES = 2           # C: Number of features per cell (ex., Freeboard, Ice Area)\n",
    "CELLS_PER_PATCH = 256      # L: Number of cells within each patch\n",
    "D_MODEL = 128              # d_model: Dimension of the transformer's internal representations (embedding dimension)\n",
    "N_HEAD = 8                 # nhead: Number of attention heads\n",
    "NUM_TRANSFORMER_LAYERS = 4 # num_layers: Number of TransformerEncoderLayers\n",
    "\n",
    "# The input dimension for the patch embedding linear layer.\n",
    "# Each patch at a given time step has NUM_FEATURES * CELLS_PER_PATCH features.\n",
    "# This is the 'D' dimension used in the Transformer's input tensor (B, T, P, D).\n",
    "PATCH_EMBEDDING_INPUT_DIM = NUM_FEATURES * CELLS_PER_PATCH # 2 * 256 = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da72b-3a07-4183-a3a2-49fa953cdd1f",
   "metadata": {},
   "source": [
    "# Transformer Class\n",
    "<!-- outputs = model(features)\n",
    "model.train()\n",
    "model.eval() -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9baa0-427e-49d9-a50a-97c3d60a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IceForecastTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Transformer-based model for forecasting ice conditions based on sequences of\n",
    "    historical patch data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_patch_features_dim : int\n",
    "        The dimensionality of the feature vector for each individual patch (ex. 2 features).\n",
    "        This is the input dimension for the patch embedding layer.\n",
    "    num_patches : int\n",
    "        The total number of geographical patches that the `nCells` data was divided into.\n",
    "        (ex., 256 patches).\n",
    "    context_length : int, optional\n",
    "        The number of historical days (time steps) to use as input for the transformer.\n",
    "        Defaults to 7.\n",
    "    forecast_horizon : int, optional\n",
    "        The number of future days to predict for each patch.\n",
    "        Defaults to 1.\n",
    "    d_model : int, optional\n",
    "        The dimension of the model's hidden states (embedding dimension).\n",
    "        This is the size of the vectors that flow through the Transformer encoder.\n",
    "        Defaults to 128.\n",
    "    nhead : int, optional\n",
    "        The number of attention heads in the multi-head attention mechanism within\n",
    "        each Transformer encoder layer. Defaults to 8.\n",
    "    num_layers : int, optional\n",
    "        The number of Transformer encoder layers in the model. Defaults to 4.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : nn.Linear\n",
    "        Linear layer to project input patch features into the `d_model` hidden space.\n",
    "    encoder : nn.TransformerEncoder\n",
    "        The Transformer encoder module composed of `num_layers` encoder layers.\n",
    "    mlp_head : nn.Sequential\n",
    "        A multi-layer perceptron head for outputting predictions for each patch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_patch_features_dim: int, # D: The flat feature dimension of a single patch (ex., 512)\n",
    "                 num_patches: int,              # P: Number of spatial patches\n",
    "                 context_length: int,           # T: Number of historical time steps\n",
    "                 forecast_horizon: int,         # Number of future time steps to predict (usually 1)\n",
    "                 d_model: int = D_MODEL,        # d_model: Transformer's embedding dimension\n",
    "                 nhead: int = N_HEAD,           # nhead: Number of attention heads\n",
    "                 num_layers: int = NUM_TRANSFORMER_LAYERS # num_layers: Number of TransformerEncoderLayers\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The transformer should\n",
    "        1. Accept a sequence of days (ex. 7 days of patches). \n",
    "           The context_length parameter says how many days to use for input.\n",
    "        2. Encode each patch with the transformer\n",
    "        3. Output the patches for regression (ex. predict the 8th day)\n",
    "           The forecast_horizon parameter says how many days to use for the output prediction\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.input_patch_features_dim = input_patch_features_dim\n",
    "   \n",
    "        print(\"Calling IceForecastTransformer __init__\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Patch embedding layer: projects the raw patch features (512)\n",
    "        # into d_model (128) hidden space dimension\n",
    "        self.patch_embed = nn.Linear(input_patch_features_dim, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output tensors are (batch, sequence, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP head:\n",
    "        # Make a prediction for every cell per patch\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, CELLS_PER_PATCH) # TODO: CHECK - Should this be multiplied by forecast_horizon???\n",
    "        )\n",
    "\n",
    "        # Positional Encoding (from your previous code, assuming it's implemented)\n",
    "        # self.pos_encoder = PositionalEncoding(d_model)\n",
    "        # TODO: IMPLEMENT A simple positional embedding or a standard sine/cosine one.\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"End of __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B = Batch size\n",
    "        T = Time (context_length)\n",
    "        P = Patch count\n",
    "        D = Patch Dimension (cells per patch * feature count)\n",
    "        x: Tensor of shape (B, T, P, D)\n",
    "        Output: Tensor of shape (batch_size, forecast_horizon, num_patches)\n",
    "        Output: (B, forecast_horizon, P)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"Calling forward\")\n",
    "        \n",
    "        # Initial input x shape from DataLoader / pre-processing:\n",
    "        # (B, T, P, D) i.e., (Batch_Size, Context_Length, Num_Patches, Input_Patch_Features_Dim)\n",
    "        # Example: (16, 7, 140, 512)\n",
    "\n",
    "        logging.info(\"Expected x shape: (B, T, P, D) ex., (16, 7, 140, 512)\")\n",
    "        logging.info(\"Actual   x shape: \", x.shape)\n",
    "        \n",
    "        B, T, P, D = x.shape\n",
    "\n",
    "        # Flatten time and patches for the Transformer Encoder:\n",
    "        # Each (Time, Patch) combination becomes a single token in the sequence.\n",
    "        # Output shape: (B, T * P, D)\n",
    "        # Example: (16, 7 * 140 = 980, 512)\n",
    "        \n",
    "        # Flatten time and patches for the Transformer Encoder: (B, T * P, D)\n",
    "        # This treats each patch at each time step as a distinct token\n",
    "        x = x.view(B, T * P, D)\n",
    "\n",
    "        # Project patch features to the transformer's d_model dimension\n",
    "        x = self.patch_embed(x)  # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "        logging.info(\"Expected patch embedding dimensions: (B, T * P, d_model) ex., (16, 980, 128)\")\n",
    "        logging.info(\"Actual   patch embedding dimensions: \", x.shape)\n",
    "\n",
    "        # TODO: Add positional encoding HERE\n",
    "        # x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        x = self.encoder(x)      # Output: (B, T * P, d_model) ex., (16, 980, 128)\n",
    "\n",
    "        # Reshape back to separate time and patches: (B, T, P, d_model) ex., (16, 7, 140, 128)\n",
    "        x = x.view(B, T, P, self.d_model) \n",
    "\n",
    "        # Mean pooling over the time (context_length) dimension for each patch.\n",
    "        # This aggregates information from all historical time steps for each patch's final prediction.        \n",
    "        x = x.mean(dim=1)  # Output: (B, P, d_model) ex., (16, 140, 128)\n",
    "\n",
    "        # TODO: SOMEHOW SAVE ATTENTION TO MAP LATER\n",
    "\n",
    "        # Apply MLP head to predict values for each cell in each patch\n",
    "        # The MLP head outputs CELLS_PER_PATCH values for each of the P patches\n",
    "        x = self.mlp_head(x)  # Output: (B, P, CELLS_PER_PATCH) ex., (16, 140, 256)\n",
    "\n",
    "        # Add forecast_horizon dimension\n",
    "        # The target 'y' is (B, forecast_horizon, P, CELLS_PER_PATCH)\n",
    "        # This makes the output shape match the target 'y' or the forecast_horizon\n",
    "        x = x.unsqueeze(self.forecast_horizon) # Output: (B, 1, P, CELLS_PER_PATCH) ex., (16, 1, 140, 256)\n",
    "\n",
    "        logging.info(\"Expected output dimensions: (B, 1, P, CELLS_PER_PATCH) ex., (16, 1, 140, 256)\")\n",
    "        logging.info(\"Actual   output dimensions: \", x.shape)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200db32-3fbb-4f8f-b841-bbcdc9117c11",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383efb0-3e8d-468d-bf66-0bb91f943ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import Tensor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Set level to logging.INFO to see the statements\n",
    "# logging.basicConfig(filename='IceForecastTransformer.log', filemode='w', level=logging.INFO)\n",
    "\n",
    "# model = IceForecastTransformer(\n",
    "#     input_patch_features_dim=PATCH_EMBEDDING_INPUT_DIM,\n",
    "#     num_patches=NUM_PATCHES,\n",
    "#     context_length=CONTEXT_LENGTH,\n",
    "#     forecast_horizon=FORECAST_HORIZON\n",
    "# ).to(device)\n",
    "\n",
    "# print(\"\\n--- Model Architecture ---\")\n",
    "# print(model)\n",
    "# print(\"--------------------------\\n\")\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 100\n",
    "\n",
    "# start_time = time.time()\n",
    "# logging.info(\" ===============================\")\n",
    "# logging.info(\" =      STARTING EPOCHS        =\")\n",
    "# logging.info(\" ===============================\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch_idx, (x, y) in enumerate(train_loader):  \n",
    "        \n",
    "#         # x: (B, context_length, num_patches, input_patch_features_dim), y: (B, forecast_horizon, num_patches)\n",
    "#         x = x.to(device) # Move to GPU if available\n",
    "#         y = y.to(device) # y is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "\n",
    "#         logging.info(\"Expected x shape is (B, T, P, C, L) ex., (16, 7, 140, 2, 256)\")\n",
    "#         logging.info(\"Actual   x shape is \", x.shape)\n",
    "\n",
    "#         logging.info(\"Expected y shape is (B, forecast_horizon, P, L) ex., (16, 1, 140, 256)\")\n",
    "#         logging.info(\"Actual   y shape is \", y.shape)\n",
    "\n",
    "#         # Reshape x for transformer input\n",
    "#         B, T, P, C, L = x.shape\n",
    "#         x_reshaped_for_transformer_D = x.view(B, T, P, C * L)\n",
    "\n",
    "#         logging.info(\"Expected reshaped x is (B, T, P, D_input)\")\n",
    "#         logging.info(\"Actual   reshaped x is \", x_reshaped_for_transformer_D.shape)  # should now be (B, T, P, 512)\n",
    "    \n",
    "#         # Run through transformer\n",
    "#         y_pred = model(x_reshaped_for_transformer_D) # y_pred is (B, forecast_horizon, num_patches) ex., (16, 1, 140)\n",
    "\n",
    "#         logging.info(\"Expected y_pred shape is (B, forecast_horizon , P, L)\")\n",
    "#         logging.info(\"Actual   y_pred shape is \", y_pred.shape)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(y_pred, y) # DIRECTLY compare y_pred and y\n",
    "    \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "#     # --- Validation loop ---\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for x_val, y_val in val_loader:\n",
    "#             x_val = x_val.to(device)\n",
    "#             y_val = y_val.to(device)\n",
    "\n",
    "#             # Extract dimensions from x_val for reshaping\n",
    "#             # x_val before reshaping: (B_val, T_val, P_val, C_val, L_val)\n",
    "#             B_val, T_val, P_val, C_val, L_val = x_val.shape\n",
    "            \n",
    "#             # Reshape x_val for transformer input\n",
    "#             x_val_reshaped_for_transformer_input = x_val.view(B_val, T_val, P_val, C_val * L_val)\n",
    "\n",
    "#             # Model output is (B, forecast_horizon, P, L)\n",
    "#             y_val_pred = model(x_val_reshaped_for_transformer_input) \n",
    "\n",
    "#             # Compute validation loss (y_val_pred and y_val should have identical shapes)\n",
    "#             val_loss += criterion(y_val_pred, y_val).item() # y_val is (B, forecast_horizon, P, L)\n",
    "    \n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"===============================================\")\n",
    "# print(f\"Elapsed time for TRAINING: {end_time - start_time:.2f} seconds\")\n",
    "# print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56564-d7f8-4060-bfa9-8edb3fce9d83",
   "metadata": {},
   "source": [
    "TODO: Add Positional Encoding to represent time steps.\n",
    "\n",
    "TODO OPTION: Try temporal attention only (ex., Informer, Time Series Transformer).\n",
    "\n",
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b69c0-01b8-491a-b71b-79ac6a7798d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where to save the model\n",
    "# PATH = \"sea_ice_concentration_model_2.pth\"\n",
    "\n",
    "# # Save the model's state_dict\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278114f-96bb-4bbc-8e7f-f87c52e2a946",
   "metadata": {},
   "source": [
    "# Re-Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dc4b5-7cca-4a6d-99fd-6754ca833879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the path where to load the model\n",
    "PATH = \"sea_ice_concentration_model.pth\"\n",
    "\n",
    "# Instantiate the model (must have the same architecture as when it was saved)\n",
    "# Create an identical instance of the original __init__ parameters\n",
    "# Make sure global constants (like D_MODEL, N_HEAD, etc.) are consistent.\n",
    "loaded_model = IceForecastTransformer(\n",
    "    input_patch_features_dim=PATCH_EMBEDDING_INPUT_DIM,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=NUM_TRANSFORMER_LAYERS\n",
    ")\n",
    "\n",
    "# Load the saved state_dict (weights_only=True helps ensure safety of pickle files)\n",
    "loaded_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7955-7de9-4ac1-ae71-ef1f7bedd950",
   "metadata": {},
   "source": [
    "# Make a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ed2c4-bee4-4fd0-81b6-79a3e79ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the logging for this part\n",
    "# https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
    "logging.disable(level=logging.INFO)\n",
    "\n",
    "# Load one batch for demonstration\n",
    "data_iter = iter(test_loader)\n",
    "sample_x, sample_y, start_idx, end_idx, target_start, target_end = next(data_iter)\n",
    "\n",
    "print(f\"Fetched sample_x start index {start_idx}: Time={dataset.times[start_idx]}\")\n",
    "print(f\"Fetched sample_x end   index {end_idx}:   Time={dataset.times[end_idx]}\")\n",
    "\n",
    "print(f\"Fetched sample_y (target) start index {target_end}: Time={dataset.times[target_end]}\")\n",
    "print(f\"Fetched sample_y (target) end   index {target_end}: Time={dataset.times[target_end]}\")\n",
    "\n",
    "# Move to device and apply initial reshape as done in training\n",
    "sample_x = sample_x.to(device)\n",
    "sample_y = sample_y.to(device) # Keep sample_y for actual comparison\n",
    "\n",
    "# Initial reshape of x for the Transformer model\n",
    "B_sample, T_sample, P_sample, C_sample, L_sample = sample_x.shape\n",
    "sample_x_reshaped = sample_x.view(B_sample, T_sample, P_sample, C_sample * L_sample)\n",
    "\n",
    "print(f\"Sample x for inference shape (reshaped): {sample_x_reshaped.shape}\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad(): # Essential for inference to disable gradient calculations\n",
    "    predicted_y_patches = loaded_model(sample_x_reshaped)\n",
    "\n",
    "print(f\"Predicted y patches shape: {predicted_y_patches.shape}\")\n",
    "print(\"Expected shape: (B, 1, NUM_PATCHES, CELLS_PER_PATCH) e.g., (16, 1, 140, 256)\")\n",
    "\n",
    "# Squeeze the forecast_horizon dimension (since it's 1)\n",
    "predicted_ice_area_patches = predicted_y_patches.squeeze(1).cpu() # Shape: (B, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "actual_y_ice_area_patches = sample_y.squeeze(1).cpu() # Shape: (B, NUM_PATCHES, CELLS_PER_PATCH).\n",
    "\n",
    "cp.save('ice_area_patches_predicted.npy', predicted_ice_area_patches)\n",
    "cp.save('ice_area_patches_actual.npy', actual_y_ice_area_patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dcfa7-f75c-4e21-9d29-c2fb3f7615ed",
   "metadata": {},
   "source": [
    "# Recover nCells from Patches for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de318f-3166-464d-b8e0-c79abe06766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# SWAP KERNELS IN THE JUPYTER NOTEBOOK #\n",
    "########################################\n",
    "\n",
    "from MAP_ANIMATION_GENERATION.map_gen_utility_functions import *\n",
    "from NC_FILE_PROCESSING.nc_utility_functions import *\n",
    "from NC_FILE_PROCESSING.patchify_utils import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "LAT_THRESHOLD = 40\n",
    "\n",
    "predicted_ice_area_patches = cp.load(\"ice_area_patches_predicted.npy\")\n",
    "actual_y_ice_area_patches = cp.load(\"ice_area_patches_actual.npy\")\n",
    "\n",
    "NUM_PATCHES = len(predicted_ice_area_patches[0])\n",
    "print(\"NUM_PATCHES is\", NUM_PATCHES)\n",
    "\n",
    "latCell, lonCell = load_mesh(perlmutterpathMesh)\n",
    "TOTAL_GRID_CELLS = len(lonCell) \n",
    "\n",
    "# Extract Freeboard (index 0) and Ice Area (index 1) for predicted and actual\n",
    "# Predicted output is (B, 1, NUM_PATCHES, CELLS_PER_PATCH)\n",
    "# Assuming the model predicts ice area, which is the second feature (index 1)\n",
    "# if the output of the model aligns with the order of features *within* the original patch_dim.\n",
    "\n",
    "# Load the original patch-to-cell mapping\n",
    "# indices_per_patch_id = [\n",
    "#     [idx_cell_0_0, ..., idx_cell_0_255],\n",
    "#     [idx_cell_1_0, ..., idx_cell_1_255],\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "full_nCells_patch_ids, indices_per_patch_id = patchify_by_latlon_spillover(\n",
    "            latCell, lonCell, k=256, max_patches=140, lat_threshold=LAT_THRESHOLD)\n",
    "\n",
    "# Select one sample from the batch for visualization (e.g., the first one)\n",
    "# Output is (NUM_PATCHES, CELLS_PER_PATCH) for this single sample\n",
    "sample_predicted_cells_per_patch = predicted_ice_area_patches[0] # First item in batch\n",
    "sample_actual_cells_per_patch = predicted_ice_area_patches[0] # First item in batch\n",
    "\n",
    "# Initialize empty arrays for the full grid (nCells)\n",
    "recovered_predicted_grid = cp.full(TOTAL_GRID_CELLS, -1, dtype=int)\n",
    "recovered_actual_grid = cp.full(TOTAL_GRID_CELLS, -1, dtype=int)\n",
    "\n",
    "# Populate the full grid using the patch data and mapping\n",
    "for patch_idx in range(NUM_PATCHES):\n",
    "    cell_indices_in_patch = indices_per_patch_id[patch_idx]\n",
    "    \n",
    "    # For predicted values\n",
    "    recovered_predicted_grid[cell_indices_in_patch] = sample_predicted_cells_per_patch[patch_idx]\n",
    "\n",
    "    # For actual values\n",
    "    recovered_actual_grid[cell_indices_in_patch] = sample_actual_cells_per_patch[patch_idx]\n",
    "\n",
    "print(f\"Recovered predicted grid shape: {recovered_predicted_grid.shape}\")\n",
    "print(f\"Recovered actual grid shape: {recovered_actual_grid.shape}\")\n",
    "\n",
    "fig, northMap = generate_axes_north_pole()\n",
    "generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_predicted_grid, \"ice area recovered\")\n",
    "\n",
    "fig, northMap = generate_axes_north_pole()\n",
    "generate_map_north_pole(fig, northMap, latCell, lonCell, recovered_actual_grid, \"ice area actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabb906-c95f-445f-a9b6-2e02d2f180e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIC Kernel 2",
   "language": "python",
   "name": "sic_sie_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
