INFO:root:<xarray.Dataset> Size: 922MB
Dimensions:         (nCells_full: 465044, time: 2100, nCells_masked: 53973)
Coordinates:
  * nCells_full     (nCells_full) int64 4MB 0 1 2 3 ... 465041 465042 465043
  * nCells_masked   (nCells_masked) int64 432kB 0 1 2 3 ... 53970 53971 53972
  * time            (time) datetime64[ns] 17kB 1850-01-01T00:30:00 ... 2024-1...
Data variables:
    cell_mask       (nCells_full) bool 465kB dask.array<chunksize=(232522,), meta=np.ndarray>
    freeboard       (time, nCells_masked) float32 453MB dask.array<chunksize=(132, 3374), meta=np.ndarray>
    full_to_masked  <U1337572 5MB ...
    ice_area        (time, nCells_masked) float32 453MB dask.array<chunksize=(132, 3374), meta=np.ndarray>
    masked_to_full  <U1337572 5MB ...
    num_raw_files   int64 8B ...
    times           (time) datetime64[ns] 17kB dask.array<chunksize=(2100,), meta=np.ndarray>
INFO:root:times.shape = (2100,)
INFO:root:ice_area.shape = (2100, 53973)
INFO:root:freeboard.shape = (2100, 53973)
INFO:root:cell_mask.shape = (465044,)
INFO:root:evaluated full_to_masked shape = ()
INFO:root:evaluated masked_to_full shape = ()
INFO:root:num_raw_files = 2100
INFO:root:Loaded pre-processed data from Zarr in 1.80 seconds.
INFO:root:=== Patchifying using latitude_spillover_redo algorithm ===
INFO:root:Longitude and latitude array shape: (210, 2) should be (n_patches, 2)
INFO:root:Minimum latitude:  40.05312114509891
INFO:root:Maximum latitude:  87.37120626698702
INFO:root:Minimum longitude: 0.0654903314441616
INFO:root:Maximum longitude: 354.58651214630476
INFO:root:Should be between lat_threshold and 90 degrees for latitude
INFO:root:Should be between 0 and 360 degrees for longitude
INFO:root:Patchifying completed in 0.39 seconds.
INFO:root:Total time steps collected (days or months): 2100
INFO:root:Unique times: 2100
INFO:root:First 35 time values: ['1850-01-01T00:30:00.000000000' '1850-02-01T00:30:00.000000000'
 '1850-03-01T00:30:00.000000000' '1850-04-01T00:30:00.000000000'
 '1850-05-01T00:30:00.000000000' '1850-06-01T00:30:00.000000000'
 '1850-07-01T00:30:00.000000000' '1850-08-01T00:30:00.000000000'
 '1850-09-01T00:30:00.000000000' '1850-10-01T00:30:00.000000000'
 '1850-11-01T00:30:00.000000000' '1850-12-01T00:30:00.000000000'
 '1851-01-01T00:30:00.000000000' '1851-02-01T00:30:00.000000000'
 '1851-03-01T00:30:00.000000000' '1851-04-01T00:30:00.000000000'
 '1851-05-01T00:30:00.000000000' '1851-06-01T00:30:00.000000000'
 '1851-07-01T00:30:00.000000000' '1851-08-01T00:30:00.000000000'
 '1851-09-01T00:30:00.000000000' '1851-10-01T00:30:00.000000000'
 '1851-11-01T00:30:00.000000000' '1851-12-01T00:30:00.000000000'
 '1852-01-01T00:30:00.000000000' '1852-02-01T00:30:00.000000000'
 '1852-03-01T00:30:00.000000000' '1852-04-01T00:30:00.000000000'
 '1852-05-01T00:30:00.000000000' '1852-06-01T00:30:00.000000000'
 '1852-07-01T00:30:00.000000000' '1852-08-01T00:30:00.000000000'
 '1852-09-01T00:30:00.000000000' '1852-10-01T00:30:00.000000000'
 '1852-11-01T00:30:00.000000000']
INFO:root:Last 35 time values: ['2022-02-01T00:30:00.000000000' '2022-03-01T00:30:00.000000000'
 '2022-04-01T00:30:00.000000000' '2022-05-01T00:30:00.000000000'
 '2022-06-01T00:30:00.000000000' '2022-07-01T00:30:00.000000000'
 '2022-08-01T00:30:00.000000000' '2022-09-01T00:30:00.000000000'
 '2022-10-01T00:30:00.000000000' '2022-11-01T00:30:00.000000000'
 '2022-12-01T00:30:00.000000000' '2023-01-01T00:30:00.000000000'
 '2023-02-01T00:30:00.000000000' '2023-03-01T00:30:00.000000000'
 '2023-04-01T00:30:00.000000000' '2023-05-01T00:30:00.000000000'
 '2023-06-01T00:30:00.000000000' '2023-07-01T00:30:00.000000000'
 '2023-08-01T00:30:00.000000000' '2023-09-01T00:30:00.000000000'
 '2023-10-01T00:30:00.000000000' '2023-11-01T00:30:00.000000000'
 '2023-12-01T00:30:00.000000000' '2024-01-01T00:30:00.000000000'
 '2024-02-01T00:30:00.000000000' '2024-03-01T00:30:00.000000000'
 '2024-04-01T00:30:00.000000000' '2024-05-01T00:30:00.000000000'
 '2024-06-01T00:30:00.000000000' '2024-07-01T00:30:00.000000000'
 '2024-08-01T00:30:00.000000000' '2024-09-01T00:30:00.000000000'
 '2024-10-01T00:30:00.000000000' '2024-11-01T00:30:00.000000000'
 '2024-12-01T00:30:00.000000000']
INFO:root:Shape of combined ice_area array: (2100, 53973)
INFO:root:Shape of combined freeboard array: (2100, 53973)
INFO:root:Elapsed time for MonthlyOrDailyNetCDFDataset __init__: 2.1969346429686993 seconds
INFO:root:Training set start date: 1850-01-01T00:30:00.000000000
INFO:root:Training set end date (cosmetic): 2012-12-01T00:30:00.000000000)
INFO:root:(actual last viable target day): 2012-12-01T00:30:00.000000000
INFO:root:Validation set start date: 2013-01-01T00:30:00.000000000
INFO:root:Validation set end date (cosmetic): 2018-12-01T00:30:00.000000000)
INFO:root:(actual last viable target day): 2018-12-01T00:30:00.000000000
INFO:root:Testing set start date: 2019-01-01T00:30:00.000000000
INFO:root:Testing set end date (cosmetic): 2024-12-01T00:30:00.000000000)
INFO:root:(actual last viable target day): 2024-12-01T00:30:00.000000000
INFO:root:
--- Model Architecture ---
INFO:root:DataParallel(
  (module): IceForecastTransformer(
    (patch_embed): Linear(in_features=512, out_features=128, bias=True)
    (pos_encoder): CombinedPositionalEncoder(
      (temporal_encoder): TemporalPositionalEncoder(
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (spatial_encoder): SpatialPositionalEncoder(
        (patch_embeddings): Embedding(210, 64)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (mlp_head): Sequential(
      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (1): Linear(in_features=128, out_features=1536, bias=True)
      (2): Sigmoid()
    )
  )
)
INFO:root:Total model parameters: 1070592
INFO:root:--------------------------

INFO:root:== TIMER IS STARTING FOR TRAINING ==
INFO:root:===============================
INFO:root:       STARTING EPOCHS       
INFO:root:===============================
INFO:root:Number of epochs: 40
INFO:root:Learning Rate: 0.0001
INFO:root:Epoch 1/40 - Train Loss: 0.1579
INFO:root:Epoch 1/40 - Validation Loss: 0.0968
INFO:root:Epoch 2/40 - Train Loss: 0.0843
INFO:root:Epoch 2/40 - Validation Loss: 0.0751
INFO:root:Epoch 3/40 - Train Loss: 0.0712
INFO:root:Epoch 3/40 - Validation Loss: 0.0642
INFO:root:Epoch 4/40 - Train Loss: 0.0657
INFO:root:Epoch 4/40 - Validation Loss: 0.0597
INFO:root:Epoch 5/40 - Train Loss: 0.0638
INFO:root:Epoch 5/40 - Validation Loss: 0.0588
INFO:root:Epoch 6/40 - Train Loss: 0.0634
INFO:root:Epoch 6/40 - Validation Loss: 0.0748
INFO:root:Epoch 7/40 - Train Loss: 0.0637
INFO:root:Epoch 7/40 - Validation Loss: 0.0593
INFO:root:Epoch 8/40 - Train Loss: 0.0600
INFO:root:Epoch 8/40 - Validation Loss: 0.0550
INFO:root:Epoch 9/40 - Train Loss: 0.0591
INFO:root:Epoch 9/40 - Validation Loss: 0.0607
INFO:root:Epoch 10/40 - Train Loss: 0.0592
INFO:root:Epoch 10/40 - Validation Loss: 0.0553
INFO:root:Epoch 11/40 - Train Loss: 0.0579
INFO:root:Epoch 11/40 - Validation Loss: 0.0534
INFO:root:Epoch 12/40 - Train Loss: 0.0569
INFO:root:Epoch 12/40 - Validation Loss: 0.0565
INFO:root:Epoch 13/40 - Train Loss: 0.0559
INFO:root:Epoch 13/40 - Validation Loss: 0.0517
INFO:root:Epoch 14/40 - Train Loss: 0.0548
INFO:root:Epoch 14/40 - Validation Loss: 0.0512
INFO:root:Epoch 15/40 - Train Loss: 0.0536
INFO:root:Epoch 15/40 - Validation Loss: 0.0502
INFO:root:Epoch 16/40 - Train Loss: 0.0526
INFO:root:Epoch 16/40 - Validation Loss: 0.0498
INFO:root:Epoch 17/40 - Train Loss: 0.0516
INFO:root:Epoch 17/40 - Validation Loss: 0.0501
INFO:root:Epoch 18/40 - Train Loss: 0.0501
INFO:root:Epoch 18/40 - Validation Loss: 0.0466
INFO:root:Epoch 19/40 - Train Loss: 0.0491
INFO:root:Epoch 19/40 - Validation Loss: 0.0453
INFO:root:Epoch 20/40 - Train Loss: 0.0471
INFO:root:Epoch 20/40 - Validation Loss: 0.0442
INFO:root:Epoch 21/40 - Train Loss: 0.0461
INFO:root:Epoch 21/40 - Validation Loss: 0.0424
INFO:root:Epoch 22/40 - Train Loss: 0.0444
INFO:root:Epoch 22/40 - Validation Loss: 0.0439
INFO:root:Epoch 23/40 - Train Loss: 0.0431
INFO:root:Epoch 23/40 - Validation Loss: 0.0394
INFO:root:Epoch 24/40 - Train Loss: 0.0415
INFO:root:Epoch 24/40 - Validation Loss: 0.0406
INFO:root:Epoch 25/40 - Train Loss: 0.0403
INFO:root:Epoch 25/40 - Validation Loss: 0.0376
INFO:root:Epoch 26/40 - Train Loss: 0.0390
INFO:root:Epoch 26/40 - Validation Loss: 0.0380
INFO:root:Epoch 27/40 - Train Loss: 0.0380
INFO:root:Epoch 27/40 - Validation Loss: 0.0359
INFO:root:Epoch 28/40 - Train Loss: 0.0371
INFO:root:Epoch 28/40 - Validation Loss: 0.0355
INFO:root:Epoch 29/40 - Train Loss: 0.0366
INFO:root:Epoch 29/40 - Validation Loss: 0.0339
INFO:root:Epoch 30/40 - Train Loss: 0.0355
INFO:root:Epoch 30/40 - Validation Loss: 0.0342
INFO:root:Epoch 31/40 - Train Loss: 0.0340
INFO:root:Epoch 31/40 - Validation Loss: 0.0319
INFO:root:Epoch 32/40 - Train Loss: 0.0325
INFO:root:Epoch 32/40 - Validation Loss: 0.0308
INFO:root:Epoch 33/40 - Train Loss: 0.0311
INFO:root:Epoch 33/40 - Validation Loss: 0.0290
INFO:root:Epoch 34/40 - Train Loss: 0.0302
INFO:root:Epoch 34/40 - Validation Loss: 0.0309
INFO:root:Epoch 35/40 - Train Loss: 0.0296
INFO:root:Epoch 35/40 - Validation Loss: 0.0285
INFO:root:Epoch 36/40 - Train Loss: 0.0286
INFO:root:Epoch 36/40 - Validation Loss: 0.0297
INFO:root:Epoch 37/40 - Train Loss: 0.0278
INFO:root:Epoch 37/40 - Validation Loss: 0.0268
INFO:root:Epoch 38/40 - Train Loss: 0.0268
INFO:root:Epoch 38/40 - Validation Loss: 0.0272
INFO:root:Epoch 39/40 - Train Loss: 0.0267
INFO:root:Epoch 39/40 - Validation Loss: 0.0270
INFO:root:Epoch 40/40 - Train Loss: 0.0271
INFO:root:Epoch 40/40 - Validation Loss: 0.0274
INFO:root:===============================================
INFO:root:Elapsed time for TRAINING: 801.65 seconds
INFO:root:===============================================
INFO:root:
Starting evaluation and metric calculation...
